import arxiv
import json
import os
import time
from datetime import datetime, timedelta, timezone

# é¡¹ç›®æ ¹ç›®å½•ï¼ˆå½“å‰è„šæœ¬ä½äº scripts/ ä¸‹ï¼‰
SCRIPT_DIR = os.path.dirname(__file__)
ROOT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, ".."))

# ArXiv çš„ä¸»è¦ä¸€çº§åˆ†ç±»åˆ—è¡¨
# æ³¨æ„ï¼šç‰©ç†å­¦æ¯”è¾ƒç‰¹æ®Šï¼ŒArXiv å†å²ä¸Šæœ‰å¾ˆå¤šç‹¬ç«‹çš„ç‰©ç†å­˜æ¡£ï¼Œä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬åˆ—å‡ºä¸»è¦çš„
CATEGORIES_TO_FETCH = [
    "cs", "math", "stat", "q-bio", "q-fin", "eess", "econ",
    "physics", "cond-mat", "hep-ph", "hep-th", "gr-qc", "astro-ph",
]

def log(message: str) -> None:
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] {message}", flush=True)


def group_start(title: str) -> None:
    print(f"::group::{title}", flush=True)


def group_end() -> None:
    print("::endgroup::", flush=True)


def fetch_all_domains_metadata_robust(
    days: int = 1,
    output_file: str | None = None,
) -> None:
    # 1. è®¡ç®—æ—¶é—´çª—å£
    end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=days)
    
    start_str = start_date.strftime("%Y%m%d0000")
    end_str = end_date.strftime("%Y%m%d2359")
    
    group_start("Step 1 - fetch arXiv")
    log(f"ğŸŒ [Global Ingest] Window: {start_str} TO {end_str}")
    
    # ç»“æœé›†ä½¿ç”¨å­—å…¸å»é‡ (å› ä¸ºæœ‰äº›è®ºæ–‡è·¨é¢†åŸŸï¼Œæ¯”å¦‚åŒæ—¶åœ¨ cs å’Œ stat)
    unique_papers = {}
    
    client = arxiv.Client(
        page_size=200,    # é™çº§ï¼šä» 1000 é™åˆ° 200ï¼Œé¿å…å•æ¬¡å“åº”è¿‡å¤§å¯¼è‡´ 500
        delay_seconds=3.0,
        num_retries=5
    )

    # 2. éå†åˆ†ç±»è¿›è¡ŒæŠ“å–
    for category in CATEGORIES_TO_FETCH:
        group_start(f"Fetch category: {category}")
        log(f"ğŸš€ Fetching category: {category} ...")
        
        # æ„é€ æŸ¥è¯¢ï¼šcat:cs* AND submittedDate[...]
        # ä½¿ç”¨é€šé…ç¬¦ category* ä»¥è¦†ç›–å­é¢†åŸŸ (å¦‚ cs.AI, cs.LG)
        query = f"cat:{category}* AND submittedDate:[{start_str} TO {end_str}]"
        
        search = arxiv.Search(
            query=query,
            max_results=None,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        
        count = 0
        try:
            for r in client.results(search):
                pid = r.get_short_id()
                
                # å¦‚æœè¿™ç¯‡è®ºæ–‡å·²ç»å­˜åœ¨ï¼ˆè¢«å…¶ä»–åˆ†ç±»æŠ“è¿‡äº†ï¼‰ï¼Œè·³è¿‡
                if pid in unique_papers:
                    continue
                    
                # ä½¿ç”¨ PDF é“¾æ¥è€Œä¸æ˜¯æ‘˜è¦é¡µé“¾æ¥ï¼Œæ–¹ä¾¿åç»­ç›´æ¥ä¸‹è½½æˆ–ä¼ ç»™ä¸‹æ¸¸å¤„ç†
                pdf_link = getattr(r, "pdf_url", None) or r.entry_id
                paper_dict = {
                    "id": pid,
                    "source": "arxiv",
                    "title": r.title.replace("\n", " "),
                    "abstract": r.summary.replace("\n", " "),
                    "authors": [a.name for a in r.authors],
                    "primary_category": r.primary_category,
                    "categories": r.categories,
                    "published": str(r.published),
                    "link": pdf_link,
                }
                unique_papers[pid] = paper_dict
                count += 1
                
                if count % 100 == 0:
                    log(f"   Category {category}: {count} papers fetched...")
            
            log(f"   âœ… Finished {category}: Got {count} new papers.")
            
        except Exception as e:
            # å•ä¸ªåˆ†ç±»å¤±è´¥ä¸å½±å“å¤§å±€ï¼Œæ‰“å°é”™è¯¯ç»§ç»­ä¸‹ä¸€ä¸ª
            log(f"   âŒ Error fetching category {category}: {e}")
            time.sleep(5) # å‡ºé”™åå¤šæ­‡ä¸€ä¼š
        finally:
            group_end()

    # 3. ä¿å­˜æ±‡æ€»ç»“æœ
    total_count = len(unique_papers)
    log(f"âœ… All Done. Total unique papers fetched: {total_count}")
    
    if total_count > 0:
        # è‹¥æœªæ˜¾å¼æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œåˆ™æŒ‰æ—¥æœŸå‘½ååˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ archive/YYYYMMDD/raw ç›®å½•ï¼š
        # <ROOT_DIR>/archive/YYYYMMDD/raw/arxiv_papers_YYYYMMDD.json
        if not output_file:
            today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
            archive_dir = os.path.join(ROOT_DIR, "archive", today_str)
            raw_dir = os.path.join(archive_dir, "raw")
            output_file = os.path.join(
                raw_dir,
                f"arxiv_papers_{today_str}.json",
            )

        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(list(unique_papers.values()), f, ensure_ascii=False, indent=2)
        log(f"ğŸ’¾ File saved to: {output_file}")
    else:
        log("âš ï¸ No papers found. Check your date range or network.")
    group_end()

if __name__ == "__main__":
    # å»ºè®®å…ˆç”¨ days=1 æµ‹è¯•ä¸€ä¸‹ï¼Œæ²¡é—®é¢˜å†è·‘æ›´é•¿æ—¶é—´çª—å£
    fetch_all_domains_metadata_robust(days=3)
