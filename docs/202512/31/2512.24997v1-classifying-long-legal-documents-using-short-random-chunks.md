
# Classifying long legal documents using short random chunks

# 使用短随机片段分类长篇法律文档



**Authors**: Luis Adrián Cabrera-Diego
**Date**: 2025-12-31

**Tags**: <span class="tag-label tag-green">keywords: symbolic regression</span> <span class="tag-label tag-green">keywords: transformer</span>

---

## Abstract
Classifying legal documents is a challenge, besides their specialized vocabulary, sometimes they can be very long. This means that feeding full documents to a Transformers-based models for classification might be impossible, expensive or slow. Thus, we present a legal document classifier based on DeBERTa V3 and a LSTM, that uses as input a collection of 48 randomly-selected short chunks (max 128 tokens). Besides, we present its deployment pipeline using Temporal, a durable execution solution, which allow us to have a reliable and robust processing workflow. The best model had a weighted F-score of 0.898, while the pipeline running on CPU had a processing median time of 498 seconds per 100 files.



## 摘要
法律文档分类是一项挑战，除了其包含专业词汇外，文档有时可能非常长。这意味着将完整文档输入基于Transformer的模型进行分类可能无法实现、成本高昂或速度缓慢。因此，我们提出了一种基于DeBERTa V3和LSTM的法律文档分类器，该分类器使用48个随机选取的短片段（最多128个词元）作为输入。此外，我们利用Temporal（一种持久化执行解决方案）展示了该模型的部署流水线，这使我们拥有了一个可靠且稳健的处理工作流。最佳模型的加权F分数为0.898，而在CPU上运行的流水线处理每100个文件的中位时间为498秒。


---

## 论文详细总结（自动生成）

以下是对论文《Classifying long legal documents using short random chunks》（使用短随机片段分类长篇法律文档）的详细结构化总结：

### 1. 核心问题与整体含义
*   **研究背景**：法律AI面临着处理专业词汇和超长文档的双重挑战。传统的基于Transformer的模型（如BERT）受限于输入长度，无法直接处理长文档，而大型语言模型（LLM）虽然能处理长文本，但存在隐私泄露风险、成本高昂且需要昂贵硬件支持的问题。
*   **核心问题**：如何在保证隐私、低硬件成本（仅使用CPU或小型GPU）和模型可维护性的前提下，高效且准确地对长篇法律文档进行分类？
*   **研究假设**：论文基于这样一个假设：仅使用文档中随机选取的少量、短小的文本片段，就足以实现对长法律文档的有效分类。

### 2. 论文提出的方法论
*   **核心思想**：不输入完整文档，而是通过随机采样机制，仅抽取极少数量的短文本块作为模型输入，结合上下文池化和LSTM进行分类。
*   **模型架构**：**DeBERTa V3 + LSTM**。
    1.  **输入处理**：将文档按段落分割并Tokenize，每个块最大长度限制为128 tokens，步长为16。
    2.  **随机采样**：在每次训练Epoch开始时，从所有块中随机抽取固定数量的块（测试了20、48、62三种大小），按文档顺序排列。
    3.  **特征提取**：将块输入多语言DeBERTa V3模型，提取每个块的`[CLS]` token嵌入。
    4.  **上下文池化**：对`[CLS]`嵌入经过Dense层和GELU激活函数，得到上下文池。
    5.  **序列整合**：将所有上下文池输入LSTM层（隐藏层大小128）。
    6.  **辅助特征**：可选地将文档长度特征（字符数、段落数、预估页数的自然对数）拼接至LSTM的最终状态。
    7.  **分类输出**：经过Dense层和Softmax激活输出类别概率。
*   **训练策略**：使用Lookahead优化器的AdamW，学习率`2e-5`，采用线性warm-up策略，并设置了早停机制。

### 3. 实验设计
*   **数据集**：使用Jus Mundi公司的专有多语种法律语料库（涉及仲裁领域，涵盖25种语言），包含约12,400份文档，分为18个类别。数据集按80%/10%/10%划分为训练集、验证集和测试集。
*   **评估指标**：由于数据集类别不平衡，主要采用**加权F-score**（Weighted F-score）。
*   **实验设置**：
    *   **消融实验**：对比了不同的采样大小（20, 48, 62 chunks）以及是否使用辅助文档长度特征（如`nc`, `np`, `app`）。
    *   **稳定性测试**：考虑到采样过程的随机性，测试集结果基于30次独立运行计算分布情况（最小值、四分位数、中位数、最大值）。
*   **对比方法**：论文未直接在统一数据集上对比Longformer或CogLXT等外部SOTA模型，而是主要在内部架构上进行配置对比，并在文本中引用了前人工作作为理论支撑。

### 4. 资源与算力
*   **训练算力**：明确使用了 **NVIDIA A100 (80GB)** GPU 进行模型训练。
*   **推理/部署算力**：在部署阶段，重点测试了仅使用 **CPU 服务器** 的性能（未具体说明CPU型号，但强调了低成本环境）。
*   **训练时长**：论文未明确说明单次训练的总小时数，仅列出了最大Epoch数为35。

### 5. 实验数量与充分性
*   **实验数量**：进行了多组配置实验，涵盖了3种采样尺寸和多种辅助特征组合。为了解决随机性带来的偏差，对测试集进行了30次重复测试来统计性能分布。
*   **充分性与客观性**：
    *   **较为客观**：多次运行取中位数的做法有效地评估了模型在随机采样机制下的鲁棒性，避免了偶然的高分。
    *   **存在局限**：实验仅基于一个私有数据集，缺乏与当前长文档分类的主流模型（如Longformer、LLM基线）在同一数据集上的直接量化对比，因此难以绝对证明其方法在业界的领先地位，更多是证明其在特定约束下（CPU、隐私）的可行性。

### 6. 论文的主要结论与发现
*   **有效性**：证明了可以使用非常少且随机的短片段准确分类长法律文档，最佳模型的中位加权F-score达到了 **0.898**。
*   **采样大小**：48个片段的性能与62个片段相近，且远优于20个片段，证明了适度增加采样量有益，但并非越多越好（过多样本增加开销且性能提升有限）。
*   **辅助特征**：文档长度特征（如字符数、段落数）对整体分类性能的提升并不显著，未能有效区分特定的易混淆类别（如“Notice of intent”和“Notice of arbitration”）。
*   **部署效率**：在使用Temporal构建的CPU流水线上，处理100个文件的中位时间为498秒（约4.98秒/文件），虽然慢于A100 GPU，但在无GPU环境下具有实用价值。

### 7. 优点
*   **成本与隐私友好**：方法允许在内部CPU服务器上运行，避免了将敏感法律数据发送给第三方LLM服务商，解决了隐私担忧，并大幅降低了硬件成本。
*   **模型简洁高效**：相比复杂的层级网络或基于提示的LLM链，DeBERTa V3 + LSTM架构简单，易于维护和重训练，技术债低。
*   **鲁棒性强**：通过每个Epoch重新随机采样 chunks 的训练方式，增强了模型对不同文本部分的泛化能力，不依赖文档的特定结构。
*   **工程实践**：引入Temporal作为编排工具，展示了如何构建容错、可扩展且易于开发的工业级处理流水线。

### 8. 不足与局限
*   **特定类别表现差**：对于样本量少（如“Settlement agreement”）或与其他类别相似度高（如“Pleadings”、“Notice of intent”）的类别，F-score较低（0.6-0.75区间），且辅助特征未能改善这一状况。
*   **数据集专有**：实验基于Jus Mundi的内部数据，限制了结果的可复现性和在其他法律领域的普适性验证。
*   **推理速度瓶颈**：相比A100 GPU处理100个文件仅需68秒，CPU方案耗时接近500秒，虽然可用但对实时性要求极高的场景仍有挑战。
*   **工程限制**：Temporal的gRPC载荷大小限制（2MB/4MB）迫使作者将文档分批处理（每批10个），且当前逻辑是逐文档处理，未充分利用批处理加速。