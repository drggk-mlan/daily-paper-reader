
# Modeling Language as a Sequence of Thoughts

# 将语言建模为思维序列



**Authors**: Nasim Borazjanizadeh, James McClelland
**Date**: 2025-12-31

**Tags**: <span class="tag-label tag-green">keywords: symbolic regression</span> <span class="tag-label tag-green">keywords: transformer</span>

---

## Abstract
Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level "thought" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.



## 摘要
Transformer语言模型通过将语言建模为词元序列，能够生成极其自然的文本。然而，由于主要依赖表层共现统计，它们未能形成实体和事件的全局一致的潜在表示，这种缺失导致了关系方向上的脆弱性（例如，逆转诅咒）、上下文错误和数据效率低下。另一方面，认知科学表明，人类理解涉及将输入的语言流转化为紧凑的、类似事件的表示，这些表示在记忆中持久存在，而逐字形式则转瞬即逝。受此观点启发，我们提出了Thought Gestalt (TG) 模型，这是一种循环Transformer，它在两个抽象层面上对语言进行建模——词元和句子级别的“思维”状态。TG 一次生成一个句子的词元，同时对先前的句子表示记忆进行交叉注意力操作。在 TG 中，词元和句子表示使用同一组模型参数生成，并使用单一目标（即下一个词元的交叉熵）进行训练：通过保留写入内存的句子表示的计算图，来自未来词元损失的梯度通过交叉注意力反向流动，从而优化生成先前句子向量的参数。在扩展实验中，与其他基线相比，TG 始终比匹配的 GPT-2 运行表现出更高的效率，扩展拟合结果表明，GPT-2 需要多约 5-8% 的数据和约 33-42% 的参数才能匹配 TG 的损失。TG 还在父子逆转诅咒探测任务上减少了关系方向泛化的错误。


---

## 论文详细总结（自动生成）

以下是对论文《Modeling Language as a Sequence of Thoughts》（将语言建模为思维序列）的详细结构化总结：

### 1. 论文的核心问题与整体含义

*   **研究背景与动机**：
    *   当前主流的 Transformer 语言模型（如 GPT）主要基于“词元”级别的统计共现来建模。这种依赖表层统计的方法导致模型难以形成全局一致的实体和事件潜在表示。
    *   这种局限性引发了一系列问题：**关系方向脆弱性（Reversal Curse，逆转诅咒）**（如知道“A是B”但推不出“B是A”）、**上下文错误**以及**数据效率低下**（相比人类，模型需要万亿级的训练数据）。
*   **认知科学视角**：
    *   论文借鉴了认知科学理论，指出人类在理解语言时，会将输入的语言流转换为紧凑的、类似事件的表示（即“思维”），并在记忆中持久保存，而逐字逐句的原文形式则很快被遗忘。
*   **整体含义**：
    *   论文主张语言不仅仅是词元的序列，更是潜在“思维”或“事件”的序列。旨在构建一种能像人类一样构建“情境模型”的架构，以解决现有模型在逻辑推理和长程依赖上的缺陷。

### 2. 论文提出的方法论

*   **核心思想：Thought Gestalt (TG) 模型**
    *   提出了一种循环 Transformer 架构，在两个抽象层面上同时建模语言：**词元级**和**句子级**（作为“思维”状态的代理）。
    *   模型一次生成一个句子，同时维护并交叉引用先前的句子表示记忆。
*   **关键技术与架构细节**：
    *   **混合注意力机制**：模型堆栈由交替的**自注意力**（处理当前句子内部的词元）和**交叉注意力**（处理记忆中的先前句子向量）块组成。
    *   **句子表示提取**：在每个句子结束时（`<EOS>` 标记处），从中间层（第 7 层）提取隐藏状态，通过一个线性层投影生成一个向量 $m_t$，作为该句子的“格式塔”表示。
    *   **可微分循环记忆**：
        *   维护一个固定容量 $M$（如 40 个句子）的记忆库，存储之前的句子向量。
        *   **关键设计**：在将句子向量写入记忆时，**保留其计算图**。这意味着未来句子预测的损失可以通过交叉注意力反向传播，从而优化生成早期句子向量的参数。这是 TG 区别于以往记忆网络（如 Transformer-XL 截断梯度）的核心。
    *   **上下文种子**：为了解决新句子首词缺乏上下文的问题，将新句子的 `<BOS>` 嵌入替换为上一句的表示向量 $m_{t-1}$。
*   **训练策略**：
    *   **单一目标**：仅使用标准的“下一个词元预测”交叉熵损失，不引入辅助的句子级损失（如 Next Sentence Prediction）。
    *   **句子流课程**：为了控制反向传播的深度，训练初期使用较短的句子流切片（如 30 句），随着训练进行逐渐增加流长度，以稳定优化。

### 3. 实验设计

*   **数据集与场景**：
    *   主要数据集：**WikiText-103**。
    *   场景：语言建模预训练。
    *   专门设计了用于测试关系方向泛化的“父子逆转诅咒”探针任务。
*   **基准与评估指标**：
    *   **基准**：使用经验缩放定律框架，测试 Loss 和 Perplexity（困惑度）随数据量和参数量的变化。
    *   **逆转诅咒评估**：通过 Negative Log-Likelihood (NLL) 评估模型在正常（正向）和反向（逆向）关系查询下的表现。
*   **对比方法**：
    *   **GPT-2 (Standard)**：匹配参数量的标准 Transformer。
    *   **GPT-2 with Sentence Boundary Bias**：在词元流中显式保留 `<BOS>` 和 `<EOS>` 标记的 GPT-2，用于测试结构偏差的作用。
    *   **Fixed Token-Span Recurrence**：将 TG 中的句子分割替换为固定长度的词元块，用于测试语义边界的重要性。
    *   **GPT-2 + Gist Masking**：利用注意力掩码限制词元仅能访问摘要词元的 Gisting 方法，用于对比外部记忆与上下文压缩的区别。

### 4. 资源与算力

*   **算力规模**：
    *   论文明确指出研究处于**低算力/低计算 regime**（Low-compute regime），训练规模远低于工业级大模型。
    *   训练数据量范围：1200万 至 5000万 词元。
    *   模型规模范围：约 34万 至 8500万 非嵌入参数。
*   **硬件细节**：
    *   消融实验中的吞吐量指标是在单块 **NVIDIA A40 GPU** 上测量的，未使用模型并行或数据并行。
*   **时间成本**：
    *   论文中未明确提及具体的训练 wall-clock 时长（天数或小时），仅关注计算量和收敛性能。

### 5. 实验数量与充分性

*   **实验数量**：
    *   **缩放实验**：进行了数据缩放（3-4个量级）和参数缩放（4个量级）的拟合分析。
    *   **基线对比**：对比了 3 类主要变体方法，每种方法在不同数据规模下进行了测试。
    *   **专门任务测试**：在父子关系任务上进行了详细的正反向对比测试。
    *   **消融实验**：对关键组件（如梯度截断、记忆位置、层选择、课程学习等）进行了 9 组以上的消融研究。
*   **充分性与客观性**：
    *   **充分**：对于其设定的数据规模和参数规模，实验覆盖面非常广，包含了缩放定律验证、机制消融和多项任务评估。
    *   **客观公平**：所有对比模型均在相同的数据划分、优化器和评估协议下进行，评估指标排除了高频特殊标记（如 EOS）的影响，确保了比较的公平性。

### 6. 论文的主要结论与发现

*   **效率提升**：在相同 Loss 下，TG 相比 GPT-2 具有显著的数据和参数效率优势。GPT-2 需要多约 **5–8%** 的训练数据或多约 **33–42%** 的参数才能达到 TG 的性能。
*   **逆转诅咒缓解**：在反向关系查询中，TG 提高正确答案概率的速度远快于 GPT-2，且随着数据增加，其对错误答案的偏差明显减少，表现出向大模型（如 Mistral 7B）行为收敛的趋势。
*   **梯度流至关重要**：消融实验证实，如果在写入记忆时切断计算图，性能会显著下降，证明 TG 的收益不仅仅来自引入循环状态，更来自于端到端的记忆优化。
*   **语义边界的有效性**：使用基于语义的句子边界进行压缩，性能优于任意的固定词元块压缩。

### 7. 优点

*   **认知合理性**：模型设计紧密契合认知心理学中关于人类将语言分割为事件/思维单元的理论。
*   **训练简洁性**：不需要额外的辅助损失（如对比学习或下一句预测），完全依靠标准的下一个词元预测损失端到端训练。
*   **长程依赖优化**：通过保留计算图，使得未来信息可以直接指导过去记忆向量的形成，有效解决了标准 Transformer 早期层难以利用长程上下文的问题。
*   **结构创新**：将注意力机制解耦为句子内的 $O(T^2)$ 自注意力和句子间的 $O(T \cdot M)$ 交叉注意力，在保持建模能力的同时优化了计算结构。

### 8. 不足与局限

*   **规模限制**：目前仅在 WikiText-103 的子集（最高 5000 万词元）上验证，远未达到现代大语言模型的训练规模，其在大规模数据上的缩放特性尚待验证。
*   **句子边界的代理局限**：使用句子边界作为“思维”边界的代理并不总是完美的，某些跨句子的思维可能被切断，而某些长句可能包含多个思维。
*   **计算图存储开销**：保留记忆向量的计算图会导致反向传播深度增加，虽然论文通过课程学习控制了流长度，但在极长序列训练中仍可能面临显存或训练稳定性的挑战。
*   **未展示通用推理能力**：虽然缓解了逆转诅咒，但论文未在更广泛的推理或数学基准上进行测试，其在复杂逻辑推理上的能力提升仍需进一步证明。