
# Many Minds from One Model: Bayesian Transformers for Population Intelligence

# 单一模型众智：面向群体智能的贝叶斯Transformer



**Authors**: Diji Yang, Yi Zhang
**Date**: 2025-12-31

**Tags**: <span class="tag-label tag-green">keywords: symbolic regression</span> <span class="tag-label tag-green">keywords: transformer</span>

---

## Abstract
Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.
  B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.



## 摘要
尽管现代Transformer规模庞大且取得了成功，但它们几乎普遍被训练为单一心智系统：优化过程产生一组确定性参数，代表对数据的单一功能假设。受智能源于众智这一思想的启发，我们提出了群体贝叶斯Transformer（B-Trans），它将标准大语言模型转化为贝叶斯Transformer模型，支持从一组预训练权重中采样出多样且连贯的模型实例。B-Trans引入了一种受贝叶斯启发的后验代理，通过将归一化层中的偏置类偏移量视为具有高斯变分近似的随机变量，在不花费训练完整贝叶斯神经网络成本的情况下，诱导出模型行为的分布。从该代理进行采样会产生一组具有多样化行为的模型实例，同时保持通用能力。为了保持每次生成内的连贯性，我们在序列级别冻结采样的噪声，强制跨词元的时间一致性。B-Trans允许进行群体级决策，其中聚合采样个体的预测显著增强了探索能力。在零样本生成、基于可验证奖励的强化学习（RLVR）以及无显式标签强化学习方面的实验表明，B-Trans有效利用了群体智慧，在实现优于确定性基线的任务性能的同时，产生了卓越的语义多样性。


---

## 论文详细总结（自动生成）

# 论文总结：Many Minds from One Model: Bayesian Transformers for Population Intelligence

### 1. 核心问题与整体含义
*   **研究背景与动机**：
    *   现代大语言模型（LLM）通常被训练为单一的确定性参数集（点估计），这导致模型在面对复杂推理时表现出“结构同质化”。
    *   在对齐训练（如 SFT 和 RLHF）的压力下，模型容易坍缩到狭窄的推理模式，缺乏探索替代路径的能力。
    *   现有的多样性方法（如提高温度进行采样）仅作用于动作空间（输出端），往往产生的是统计噪声而非语义上的实质性进步，且容易导致生成内容逻辑不连贯。
*   **整体含义**：
    *   论文主张将单一的确定性模型转化为一个能够生成多样化“心智”（模型实例）的群体。
    *   核心思想是利用“群体智能”，在保持单模型效率的同时，通过参数空间的不确定性引入有意义的语义多样性，从而解决模型探索能力受限和推理僵化的问题。

### 2. 论文提出的方法论
*   **核心思想**：
    *   提出 **B-Trans (Population Bayesian Transformers)**，这是一种将标准 LLM 转化为贝叶斯 Transformer 的方法。
    *   不训练完整的贝叶斯神经网络，而是通过在归一化层引入受贝叶斯启发的后验代理，以极低的计算成本从单一预训练权重中采样出多样且连贯的模型实例。
*   **关键技术细节**：
    *   **随机偏置注入**：将归一化层（如 RMSNorm）中的偏置类偏移量视为随机变量。具体实现是在归一化操作后添加一个服从高斯分布的潜在变量 $z$：
      $$ y = \text{Norm}(x) \cdot w + (b + z), \quad z \sim N(\mu, \sigma^2) $$
    *   **假设采样**：为了保证生成的逻辑连贯性，B-Trans 采用序列级采样而非传统的逐词采样。即在生成一个序列之前采样一次噪声 $z$，并在该序列的所有自回归生成步骤中冻结该噪声。
    *   **时序一致性**：通过缓存噪声向量，确保整个序列由同一个“心智”生成，避免了因中间参数变化导致的逻辑断裂。
    *   **即插即用设计**：通过 `BayesianBiasWrapper` 封装现有的归一化层，无需修改底层训练管线，且兼容 LoRA 等高效微调技术。

### 3. 实验设计
*   **零样本生成与多样性**：
    *   **数据集**：MMLU-Pro（推理能力）、INFINITY-CHAT（开放域创意写作）。
    *   **评估指标**：Pass@K（检查 K 个样本中是否有正确答案）、平均嵌入余弦距离（语义多样性）、PCA 可视化。
    *   **对比方法**：基础模型（Qwen3, Llama-3.1）配合高温采样。
*   **强化学习与可验证奖励 (RLVR)**：
    *   **场景**：稀疏奖励环境下的数学推理。
    *   **数据集**：GSM8K, MATH-500, Minerva Math。
    *   **基准**：使用 GRPO (Group Relative Policy Optimization) 进行训练，对比标准解码与 B-Trans 解码。
*   **无标签测试时强化学习 (TTRL)**：
    *   **场景**：无监督的自我提升，仅依靠多数投票作为奖励信号。
    *   **数据集**：AIME24。
    *   **对比**：高温采样基线 vs. B-Trans。
*   **消融实验**：
    *   对比了“序列级噪声”与“逐词噪声”对逻辑一致性的影响。
    *   验证了在不同训练框架（SimpleRL-Zoo vs. VeRL）下的兼容性。

### 4. 资源与算力
*   **算力使用情况**：
    *   论文中**未明确提及**使用的具体 GPU 型号、GPU 数量或总训练时长。
    *   为了节省计算资源，实验部分主要采用了 LoRA（低秩适应）进行微调，可训练参数量少于 1%。
    *   RLVR 实验使用了从数据集中分层采样出的 400 个示例进行训练。

### 5. 实验数量与充分性
*   **实验数量**：
    *   涵盖了 2 个零样本任务（MMLU-Pro, INFINITY-CHAT）。
    *   涵盖了 4 个 RLVR 数学推理数据集。
    *   涵盖了 1 个 TTRL 数据集（AIME24）。
    *   包含了时序一致性消融实验和框架兼容性测试。
*   **充分性与客观性评价**：
    *   **较为充分**：作为概念验证，实验涵盖了生成质量、语义多样性、稀疏奖励探索和无监督学习等多个维度，能够全方位展示 B-Trans 的潜力。
    *   **客观公平**：在对比实验中，除了引入 B-Trans 的噪声机制外，解码策略、训练协议（如 GRPO 设置）均保持与基线一致，确保了比较的公平性。
    *   **局限性**：作者也承认这是一个“最小化的概念验证”，未进行大规模的基准测试或超参数全面扫描。

### 6. 论文的主要结论与发现
*   **功能多样性**：B-Trans 生成的实例具有功能异质性。在 MMLU-Pro 上，随着样本数 K 的增加，B-Trans 的 Pass@K 性能提升显著优于高温采样基线，证明其产生的多样性是有效的推理路径而非随机噪声。
*   **语义多样性**：在 INFINITY-CHAT 上，B-Trans 产生的输出嵌入距离更大，且 PCA 可视化显示其能跨越模型原本的“语义领地”，甚至覆盖到不同模型家族的语义区域。
*   **探索能力增强**：在 RLVR 和 TTRL 任务中，B-Trans 通过参数空间的多样性显著提升了探索效率，在 GSM8K、MATH-500 和 AIME24 等高难度数据集上取得了优于确定性基线的成绩。
*   **群体智慧**：在无标签情况下，利用群体内部的多样性进行投票，能够产生更优的学习信号，实现自我纠错。

### 7. 优点
*   **计算高效**：相比深度集成或全贝叶斯神经网络，B-Trans 仅增加微小的计算开销和内存占用（仅缓存偏置噪声向量），即可实现类似的“群体”效果。
*   **逻辑连贯**：创新的序列级噪声缓存机制确保了自回归生成过程中的时序一致性，避免了像 Dropout 那样的逻辑断裂。
*   **通用性强**：该方法与模型架构（Qwen, Llama 均有效）和训练基础设施（TRL, VeRL 均兼容）解耦，具有即插即用的特性。
*   **深层探索**：通过在参数空间而非动作空间引入扰动，实现了比单纯调整温度更深层、更系统的推理路径探索。

### 8. 不足与局限
*   **简化的近似**：目前的实现仅对归一化层的偏置项进行了随机化处理，而注意力层和前馈层的权重保持确定。这仅是完整贝叶斯神经网络的一个简化代理，可能限制了后验分布的表达力。
*   **先验假设简单**：使用简单的各向同性高斯分布作为先验，假设所有层和维度的不确定性相同。这是一个较强的简化假设，无法捕捉模型不同部分对噪声敏感度的差异。
*   **实验范围有限**：研究主要作为概念验证，未在更大规模的模型或更多样化的任务（如代码生成、长文本生成）上进行广泛的超参数调优和基准测试。
*   **未学习先验**：当前的超参数（如 $\sigma$）是固定的，而非根据数据学习得到的，这可能限制了模型在不同任务上的自适应能力。