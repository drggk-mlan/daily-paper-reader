
# Self-Supervised Amortized Neural Operators for Optimal Control: Scaling Laws and Applications

# 用于最优控制的自监督摊销神经算子：缩放定律与应用



**Authors**: Wuzhe Xu, Jiequn Han, Rongjie Lai
**Date**: 2025-12-31

**Tags**: <span class="tag-label tag-green">keywords: symbolic regression</span> <span class="tag-label tag-green">keywords: transformer</span>

---

## Abstract
Optimal control provides a principled framework for transforming dynamical system models into intelligent decision-making, yet classical computational approaches are often too expensive for real-time deployment in dynamic or uncertain environments. In this work, we propose a method based on self-supervised neural operators for open-loop optimal control problems. It offers a new paradigm by directly approximating the mapping from system conditions to optimal control strategies, enabling instantaneous inference across diverse scenarios once trained. We further extend this framework to more complex settings, including dynamic or partially observed environments, by integrating the learned solution operator with Model Predictive Control (MPC). This yields a solution-operator learning method for closed-loop control, in which the learned operator supplies rapid predictions that replace the potentially time-consuming optimization step in conventional MPC. This acceleration comes with a quantifiable price to pay. Theoretically, we derive scaling laws that relate generalization error and sample/model complexity to the intrinsic dimension of the problem and the regularity of the optimal control function. Numerically, case studies show efficient, accurate real-time performance in low-intrinsic-dimension regimes, while accuracy degrades as problem complexity increases. Together, these results provide a balanced perspective: neural operators are a powerful novel tool for high-performance control when hidden low-dimensional structure can be exploited, yet they remain fundamentally constrained by the intrinsic dimensional complexity in more challenging settings.



## 摘要
最优控制为将动力学系统模型转化为智能决策提供了一个原则性框架，但在动态或不确定的环境中，经典计算方法的成本通常过高，无法满足实时部署的需求。在本工作中，我们针对开环最优控制问题提出了一种基于自监督神经算子的方法。该方法通过直接近似从系统条件到最优控制策略的映射提供了一种新范式，一旦训练完成，便能在不同场景下实现瞬时推断。我们通过将学习到的解算子与模型预测控制（MPC）相结合，进一步将该框架扩展到更复杂的设定中，包括动态或部分可观测环境。这产生了一种用于闭环控制的解算子学习方法，其中学习到的算子提供快速预测，取代了传统 MPC 中潜在耗时的优化步骤。这种加速伴随着可量化的代价。在理论上，我们推导了缩放定律，将泛化误差和样本/模型复杂度与问题的本征维度及最优控制函数的正则性联系起来。数值实验表明，该方法在低本征维度下表现出高效、准确的实时性能，而随着问题复杂度的增加，精度会有所下降。综合这些结果，我们提供了一个平衡的视角：当可以利用隐藏的低维结构时，神经算子是高性能控制的有力新工具，但在更具挑战性的设定中，它们仍然受到本征维度复杂性的根本限制。


---

## 论文详细总结（自动生成）

基于提供的论文内容，以下是关于《Self-Supervised Amortized Neural Operators for Optimal Control: Scaling Laws and Applications》的详细结构化总结：

### 1. 论文的核心问题与整体含义

*   **核心问题**：最优控制在机器人、航空航天、金融等领域至关重要，但传统的数值计算方法（如基于轨迹优化、PMP、HJB方程的方法）在动态或不确定环境中计算成本过高，难以满足实时部署的需求。此外，现有的深度学习方法通常需要昂贵的真值数据集进行监督学习，或者需要针对每个具体问题实例重新训练。
*   **整体含义与动机**：本研究提出了一种基于自监督神经算子的新范式，旨在解决上述计算瓶颈。核心思想是“摊销优化”，即学习一个从环境条件（如障碍物分布）和初始状态直接映射到最优控制策略的算子。一旦训练完成，该算子可以在不同场景下实现瞬时推理。研究不仅关注方法的效率，还深入探讨了其理论基础（缩放定律）和实际局限性（本征维度的诅咒），为神经算子在控制领域的应用提供了平衡的视角。

### 2. 论文提出的方法论

*   **核心思想**：将最优控制问题视为算子学习问题。不依赖标签数据，而是利用已知的动力学模型和代价函数，通过自监督的方式训练神经网络来逼近最优解算子 $G^*$。
*   **关键技术与细节**：
    *   **算子映射**：定义算子 $G: (B, x_0) \mapsto u^*(t)$，其中 $B$ 是环境函数（如障碍物），$x_0$ 是初始状态。
    *   **自监督训练损失**：通过最小化模型预测控制轨迹下的代价函数来训练。给定神经算子 $G_\theta$，将其输出的控制输入到由 Neural ODE 建模的动力学系统中生成轨迹，然后计算该轨迹的总代价。
        *   目标：$\min_\theta \mathbb{E}_{B, x_0} [J_B(x(t), G_\theta(B, x_0))]$
        *   约束：$\dot{x}(t) = f(t, x(t), G_\theta(B, x_0))(t)$
    *   **网络架构**：
        *   **理论分析**：使用深度 ReLU 网络。
        *   **实际实现**：针对高斯混合障碍物，使用共享 MLP 提取特征并平均池化以保持排列不变性；针对迷宫导航，结合了傅里叶特征和自注意力机制。
    *   **闭环扩展 (MPC)**：将学习到的算子嵌入到模型预测控制（MPC）框架中。在每个时间步，利用当前可见的局部环境信息，通过算子预测短时域内的最优控制，从而处理动态或部分可观测的环境。此外，引入了自由终端时间 $T$ 作为决策变量以优化任务完成时间。

### 3. 实验设计

*   **数据集 / 场景**：
    1.  **高斯 bump 障碍**：包括 2D 和 3D 空间中的静止及移动障碍物场景。
    2.  **迷宫导航**：使用随机深度优先搜索（DFS）生成的不同复杂度（5x5, 7x7, 9x9）的迷宫。
    3.  **独轮车模型**：具有非线性动力学（速度、航向角、加速度）的 2D 物理模型。
    4.  **闭环控制任务**：独轮车在具有部分可观测动态障碍（模拟行人）的迷宫中执行投递任务。
    5.  **缩放定律验证**：LQR 问题（不同 DOF）、高斯 bump 问题（不同 bump 数量）、迷宫问题（不同尺寸）。
*   **Benchmark / 对比方法**：
    *   **NLP (非线性规划) 求解器**（如 `OptTraj`）：用于独轮车问题的对比基准，作为传统数值方法的代表。
    *   **瞬态冻结预测**：针对移动障碍物场景，对比神经算子（利用未来信息）与仅基于当前障碍物位置进行预测的传统方法。
    *   **解析解**：用于 LQR 问题的误差基准。
    *   **数值求解器**：用于生成高斯 bump 问题的参考解。
*   **评估指标**：轨迹质量（是否碰撞）、相对均方误差（RMSE）、代价差异（能量/障碍物惩罚）、计算时间。

### 4. 资源与算力

*   **明确提及的算力信息**：
    *   在独轮车与 NLP 的计算时间对比中，论文明确提到使用了 **Apple M3 Pro 芯片**。数据显示 NLP 平均耗时 4.42 秒，而神经算子（NO）平均推理时间仅为 0.00184 秒。
*   **未明确说明的信息**：
    *   论文中未明确说明训练阶段使用的具体 GPU 型号（如 NVIDIA A100, V100 等）、GPU 数量或总训练时长。仅提及了软件框架和超参数设置（如 Epochs 4000, Batch size 等）。

### 5. 实验数量与充分性

*   **实验数量**：
    *   论文进行了大量的数值实验，涵盖了从简单的线性控制（LQR）到复杂的非线性动力学（独轮车）和高维空间（3D 障碍）。
    *   具体包括：2D/3D 高斯 bump 的参数扫描（不同高度、位置、数量）、迷宫复杂度分级测试（3 种尺寸）、独轮车的 100 个测试任务对比、以及针对缩放定律的系统性验证。
*   **充分性与客观性**：
    *   **充分**：实验设计紧扣论文提出的理论（缩放定律）。特别是通过改变本征维度（如增加障碍物数量 $N_b$ 或迷宫大小）来验证误差随样本量衰减变慢的预测，非常有说服力。
    *   **客观**：不仅展示了方法在简单场景下的优越性（速度极快），也诚实地展示了其在高复杂度场景（9x9 迷宫）下的性能下降，这与理论分析的局限性相吻合，体现了实验的客观性。

### 6. 论文的主要结论与发现

1.  **高效性**：在低本征维度问题中，训练好的神经算子能够实现毫秒级推理，速度比传统 NLP 求解器快几个数量级，同时保持相近的代价性能。
2.  **理论缩放定律**：建立了泛化误差界限，证明神经算子的学习可以克服“环境维度”的诅咒，但仍然受限于“本征维度”的诅咒。样本复杂度随本征维度的增加呈指数级恶化。
3.  **性能权衡**：当问题复杂度（本征维度）较低时，神经算子表现出极高的效率和精度；但随着复杂度增加（如 9x9 迷宫或大量障碍物），由于需要覆盖的空间变大，性能显著下降。
4.  **闭环能力**：通过与 MPC 结合，该方法能有效处理部分可观测环境，实现实时的闭环控制。

### 7. 优点

*   **自监督与免标签**：不需要昂贵的优化求解器生成真值标签进行训练，仅依赖动力学模型和代价函数，降低了数据获取成本。
*   **理论深度**：不仅仅提出了应用方法，还推导了严格的缩放定律，从理论上量化了“要付出的代价”，即对本征维度的依赖性，这在同类工作中较为少见。
*   **泛化性与实时性**：一旦训练完成，可对新环境实现零样本推理，且推理速度极快，适合实时应用。
*   **架构灵活性**：针对不同类型的输入（高斯混合、迷宫结构）设计了合理的编码器（如排列不变性网络、傅里叶特征+注意力机制）。

### 8. 不足与局限

*   **本征维度的限制**：这是论文强调的核心局限。神经算子在高本征维度或结构复杂的任务中表现不佳，无法像经典方法那样保证在高维无结构空间中的有效性。
*   **优化误差未量化**：理论分析主要关注统计误差和逼近误差，而假设找到的是经验风险的全局最小化器。实际训练中使用的非凸优化（如 Adam）带来的优化误差未在理论中严格刻画。
*   **约束处理**：目前的公式主要针对无约束最优控制问题。虽然文中提到可以通过惩罚项扩展到带约束情况，但未在实验中深入探讨硬约束的处理。
*   **训练成本**：虽然推理极快，但训练过程需要大量的样本和计算资源（4000 Epochs），这本身也是一种计算开销的摊销。
*   **实验覆盖**：虽然涵盖了多种场景，但主要集中于确定性系统。对于随机最优控制或强噪声环境下的鲁棒性，论文仅提及作为未来工作，未进行实验验证。