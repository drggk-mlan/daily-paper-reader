Title: Learning to Ideate for Machine Learning Engineering Agents

URL Source: https://arxiv.org/pdf/2601.17596v1

Published Time: Tue, 27 Jan 2026 01:42:15 GMT

Number of Pages: 12

Markdown Content:
# Learning to Ideate for Machine Learning Engineering Agents 

Yunxiang Zhang 1,2 * Kang Zhou 1† Zhichao Xu 1 Kiran Ramnath 1

Yun Zhou 1 Sangmin Woo 1 Haibo Ding 1† Lin Lee Cheong 11AWS AI Labs 2University of Michigan 

{yxzyx,zhoukang,xzhichao,raxkiran,yunzzhou, sangminw,hbding,lcheong}@amazon.com 

Abstract 

Existing machine learning engineering (MLE) agents struggle to iteratively optimize their im-plemented algorithms for effectiveness. To ad-dress this, we introduce MLE-I DEATOR , a dual-agent framework that separates ideation from implementation. In our system, an im-plementation agent can request strategic help from a dedicated I DEATOR . We show this approach is effective in two ways. First, in a training-free setup, our framework signifi-cantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demon-strate that the I DEATOR can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B IDEATOR achieves an 11.5% relative improve-ment compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery. 

1 Introduction 

Artificial intelligence (AI) agents capable of build-ing machine learning models offer great potential to accelerate scientific discovery (Swanson et al., 2024; Lu et al., 2024; Chan et al., 2025). While AI agents powered by large language models (LLMs) have made significant progress in relevant fields such as software development (Wang et al., 2025b; Jimenez et al., 2024; Yang et al., 2024), their typ-ical designs are insufficient for autonomous ma-chine learning engineering (MLE). They are often limited to implementing a single valid solution, ceasing exploration prematurely rather than con-tinuously optimizing algorithms to enhance per-formance. In MLE tasks, a merely valid solution is insufficient, as the task demands strategic itera-tion—testing alternative models and hyperparam-

> *Work done during an internship at Amazon.
> †Corresponding authors.

Figure 1: Overview of MLE-I DEATOR . The I MPLE - 

> MENTER

uses a <seek_help> action to solicit strategic guidance from a dedicated I DEATOR . This separation improves performance over single-agent baselines and enables the I DEATOR to be optimized via reinforcement learning with execution-based rewards. 

eters, refining data and features to achieve state-of-the-art results (Chan et al., 2025; Qiang et al., 2025; Zhang et al., 2025b). While recent work has sought to develop agents capable of strategic iteration, typically by train-ing small language models (SLMs) as MLE agents via reinforcement learning (RL) (Liu et al., 2025b; Yang et al., 2025b), these approaches typically rely on a single-agent paradigm that conflates strategic reasoning (idea proposal) with coding proficiency (implementation). This conflation imposes a criti-cal trade-off: small models lack the coding capacity to implement complex solutions, yet training them to master both skills requires inefficient multi-turn rollouts with sparse rewards. To address this struc-tural limitation, we ask: Can decoupling ideation from implementation enable small models to effi-ciently learn strategies that guide stronger models? 

We answer this question by designing a novel dual-agent framework MLE-I DEATOR (Figure 1) that explicitly separates ideation from implemen-tation. When the Implementer’s progress plateaus, 

> arXiv:2601.17596v1 [cs.CL] 24 Jan 2026

it can invoke a new <seek_help> action to solicit high-level strategies from the I DEATOR . This de-sign facilitates superalignment (Burns et al., 2024), where a lightweight I DEATOR model serves as a strategic guider for a more capable Implementer, directing algorithm refinements without needing to retrain the Implementer itself. A well-trained IDEATOR can thus be reused as a plug-in to aug-ment the performance of various implementer mod-els. We first demonstrate on the MLE-Bench (Chan et al., 2025) that training-free ideation improves performance over implementation-only agent base-lines. To further enhance ideation capability, we introduce an RL pipeline to train the I DEATOR .Each proposed idea is executed by a frozen Im-plementer in a single step and rewarded if it im-proves performance. The I DEATOR is then updated using the GRPO (Shao et al., 2024) algorithm to maximize this execution-based reward, thereby ef-ficiently avoiding costly multi-step rollouts. Our experiments show that the Qwen3-8B model, after RL training on just 10 MLE tasks, not only outper-forms its untrained counterpart but also surpasses the powerful Claude Sonnet 3.5 as an I DEATOR .These results highlight a promising path toward teaching LLM agents to generate high-impact ideas for scientific discovery. Our contributions are threefold: • We introduce MLE-I DEATOR , a novel dual-agent framework that separates high-level ideation from low-level implementation for MLE tasks. • We design an efficient RL pipeline to train an IDEATOR with execution-based rewards, min-imizing reliance on costly multi-step rollouts. • We demonstrate strong performance gains on MLE-Bench, showing that an RL-trained small Ideator can outperform more powerful LLMs in guiding the Implementer. 

2 Related Work 

Single-Agent Frameworks for MLE. Coding agents such as CodeAct (Wang et al., 2025b), MLAB (Huang et al., 2024), and RepoMas-ter (Wang et al., 2025a) execute MLE tasks using basic tools like Python and Bash but lack dedicated ideation, often stopping at the first valid solution. Parallel exploration frameworks like AIDE (Jiang et al., 2025b), AIRA (Toledo et al., 2025), and ML-Master (Liu et al., 2025a) extend this via tree search across multiple solution branches. However, these methods rely on expensive code-space search and static prompting, limiting exploration to imple-mentation rather than high-level ideation. In con-trast, we decouple ideation from coding, enabling efficient idea-space exploration without costly tree-search orchestration. 

Multi-Agent Frameworks for MLE. Multi-agent frameworks like MLZero (Fang et al., 2025), InternAgent (Zhang et al., 2025a), Agent Lab-oratory (Schmidgall et al., 2025), and R&D-Agent (Yang et al., 2025c) decompose tasks us-ing specialized roles (e.g., planner, experimenter). Yet, their coordination relies on complex, rigid, and prompt-driven workflows. Our work differs in two ways. First, we employ a dynamic workflow where the Implementer solicits on-demand strategic guidance via a <seek_help> action, ensuring col-laboration is context-driven rather than predefined. Second, unlike prompt-only systems, we apply re-inforcement learning directly to the ideation model, optimizing it to generate effective algorithmic sug-gestions based on performance outcomes. 

RL-Trained MLE Agents. Reinforcement learn-ing has recently been applied to MLE agents (Liu et al., 2025b; Yang et al., 2025b), but these works focus on training single-agent implementers for code generation, neglecting the strategic challenge of ideation. By contrast, we train the I DEATOR 

itself with RL to directly optimize idea quality via performance feedback. This lightweight mecha-nism enables the I DEATOR to learn high-level re-finements. 

3 Methods 

3.1 MLE-I DEATOR 

To decouple ideation from implementation in ma-chine learning engineering tasks, we propose a dual-agent framework in Figure 1 that consists of two primary components: a Implementer agent responsible for implementation and an I DEATOR 

agent for providing high-level suggestions. We instantiate the Implementer with the CodeAct agent framework (Wang et al., 2025b), which provides an action space for executing Python and Bash commands ( <execute_ipython> and 

<execute_bash> ). We introduce a new action, 

<seek_help> , which the Implementer can invoke as needed when it experiences plateaued perfor-mance, or identifies a potential optimization oppor-tunity. The query is formatted as follows: 

Implementer Agent Query via <seek_help> 

> <seek_help> PROBLEM_STATEMENT: <one sentence describing the current blocking issue> ATTEMPTS_SO_FAR: <a short bullet list of what you already tried> GOAL: <one sentence on what success looks like for this step> </seek_help>

The I DEATOR receives this query along with the full trajectory of the Implementer’s work (including code, logs, performance) and provides a structured suggestion: 

IDEATOR Agent Response for <seek_help> 

> ANALYSIS_ON_CURRENT_PROGRESS: <Briefly state whether to keep refining the present approach or revert to a prior solution and pursue a new path.> ACTION: <One imperative command or code block the agent must execute next.> RATIONALE: <Concise justification for why this action is optimal right now.>

This structure makes the suggestions easily com-prehendable and actionable for the Implementer. The Implementer receives the I DEATOR ’s output as an observation and integrates the suggested ac-tion into its workflow in the subsequent step. This protocol enables targeted, context-aware collabora-tion. 

3.2 MLE-I DEATOR -RL 

To enable the I DEATOR to learn from experience, we introduce a reinforcement learning framework to train the I DEATOR to propose more effective ideas. 

State and Action Space. At each timestep t

where the Implementer issues a <seek_help> ac-tion, the I DEATOR receives a comprehensive snap-shot of the Implementer agent’s progress as the current state representation st = ( D, τ 1: t, p t, C t),consisting of the task description D, the cur-rent trajectory τ1: t = (a1, o 1, ..., a t), which includes all previous actions and observations, the current ML solution’s performance score 

pt, and the full ML solution code Ct. The IDEATOR ’s action is to generate a natural lan-guage suggestion α with a structured format: α =(ANALYSIS , ACTION , RATIONALE ).

Reward Function. To directly optimize for effec-tive ideas, we define a discrete, three-level reward function Rt(α) that evaluates the outcome of ap-plying an idea α:

Rt(α) = 



+1 if pt+1 (α) > p t

0 if pt+1 (α) ≤ pt or execution fails 

−1 if format errors, (1) where pt is the solution performance before the idea, and pt+1 (α) is the new performance after the Implementer executes revised code based on the idea α in a single step. The reward function incentivizes the I DEATOR to propose ideas that improve performance and penalizes those that are ineffective or invalid. 

RL Training Pipeline. To avoid regenerating agent trajectories online at each training step, we first run MLE-I DEATOR offline to collect a pool of states whose trajectories end with the <seek_help> 

action. These states are then used to construct prompts for the I DEATOR (see Appendix D). We employ GRPO (Shao et al., 2024) as our RL al-gorithm for its efficiency and stability. The train-ing objective is to maximize the expected reward for the ideas generated by the I DEATOR (Ap-pendix A.1). Each training step begins with an 

ideation (rollout) phase , where we sample G can-didate ideas {α1, ..., α G} from the I DEATOR ’s cur-rent policy. Next, during the execution (reward calculation) phase , a frozen Implementer agent attempts to implement each valid idea in a single step. 1 If execution succeeds, we evaluate the new ML solution to compute a performance-based re-ward defined in Equation 1. Finally, in the back-propagation phase , we update the I DEATOR ’s policy parameters using the GRPO loss function (Appendix A.2). This formulation enables di-rect optimization of idea effectiveness through performance-based feedback. 

4 Experiments 

4.1 Experimental Setup Dataset and Baselines. Our experiments are con-ducted on the MLE-Bench dataset (Chan et al., 2025). For our training tasks, we select 10 di-verse Kaggle tasks (Appendix B.1) that span multi-ple data modalities and whose solutions can typi-

> 1

We restrict execution to a single step for simplicity and efficiency, though more turns could reduce execution failure through agent self-debugging. System IDEATOR Avg@3 Best@3                                

> Implementation-only Agent Baselines
> CodeAct + GPT-4o -47.9 51.7 AIDE + GPT-4o -49.6 50.7 AIDE + Sonnet 3.5 -50.7 53.8
> Implementer = Claude Sonnet 3.5
> CodeAct -50.6 52.8 MLE-I DEATOR Sonnet 3.5 58.5 60.9 Qwen3-8B 53.2 56.6 MLE-I DEATOR -RL Qwen3-8B-RL 58.4 63.1
> Implementer = Qwen3-8B
> CodeAct -25.4 25.9 MLE-I DEATOR Sonnet 3.5 28.0 28.3 Qwen3-8B 25.2 25.6 MLE-I DEATOR -RL Qwen3-8B-RL 29.8 30.1

Table 1: Main results on 51 held-out MLE-Bench tasks. 

cally be executed within 20 minutes, a criterion that enables faster reward calculation. To col-lect training data from these 10 tasks, we run the MLE-I DEATOR framework 90 times for each us-ing Claude Sonnet 3.5 as the backbone model and randomly sample 1000 states to serve as training prompts (100 per task) and 100 states for valida-tion (10 per task). 2 Our evaluation is performed on a held-out set of 51 tasks (21 low-complexity and 30 medium-complexity), excluding the 15 high-complexity tasks where agents consistently struggle to produce a valid submission within the time limit. We test several configurations using Qwen3-8B (Yang et al., 2025a) and Claude Son-net 3.5 as the backbone LLMs for the I DEATOR 

and Implementer roles. We compare three con-figurations: a CodeAct baseline where the Im-plementer agent solves the task alone; an MLE-IDEATOR (Prompting) setup where the Imple-menter is paired with a prompted I DEATOR and can use the <seek_help> action; and our MLE-IDEATOR -RL (RL-Trained) approach, which re-places the prompted Qwen3-8B I DEATOR with our RL-trained version. During evaluation, all agents are allotted a maximum of 50 steps and a one-hour runtime per task. We show the hyperparameters of RL training in Appendix B.2. 

Evaluation Metric. We evaluate agent perfor-mance using Avg@3 and Best@3 normalized               

> 2Each run may trigger multiple <seek_help> calls, pro-ducing over 1,000 states ending with that action. We sample the required training and validation sets from this pool.
> IDEATOR Avg@3 Best@3
> Implementer = Claude Sonnet 3.5
> -69.7 72.0 NULL IDEA 68.7 72.8 VAGUE IDEA 75.0 76.3 Claude Sonnet 3.5 80.1 83.8

Table 2: Impact of idea quality on MLE-I DEATOR per-formance under 22 low-complexity MLE-Bench tasks. NULL IDEA and V AGUE IDEA denote non-LLM abla-tions where the I DEATOR is replaced with a fixed tem-plate response. N ULL IDEA always returns “ I have no suggestions for improving the solution. Please proceed using your best judgment. ” and V AGUE IDEA always returns “ Keep improving the performance of your solu-tion. ”

scores (Jiang et al., 2025a), averaged across all tasks. For each task, an agent is run three independent times. Since the native evalua-tion metrics (e.g., accuracy, cross-entropy loss) vary in scale from one Kaggle task to an-other, we normalize the raw score of each run against human performance on the Kaggle leader-board to a common 0-100 scale: max(0 , 100 × 

> agent_score −worst_human_score best_human_score −worst_human_score

). The Best@3 is the maximum of these three normalized scores, while the Avg@3 is their average, excluding any runs that failed to produce a valid submission. An agent receives a score of zero for a task only if all three runs fail to produce any valid solutions. 

4.2 Results and Analysis Prompted and Reinforced Ideation Boost Performance. Table 1 shows that prompted ideation consistently boosts performance over implementation-only agent baselines. With Claude Sonnet 3.5 as the Implementer, pairing it with a prompted I DEATOR yields strong improvements: Avg@3 rises from 50.6 (CodeAct baseline) to 58.5 with a Sonnet I DEATOR and 53.2 with a Qwen3-8B I DEATOR . Reinforcement learning fur-ther enhances effectiveness, as the Qwen3-8B-RL IDEATOR achieves a Best@3 of 63.1, surpassing even the powerful Sonnet I DEATOR . Even with the weaker Qwen3-8B Implementer, ideation proves valuable: pairing with Sonnet 3.5 lifts performance from 25.4 to 28.0, and the RL-trained Qwen3-8B IDEATOR achieves a stronger result at 29.8. These results show that prompted ideation consistently improves implementers, and RL enables smaller 

IDEATORS to surpass stronger prompted ones. Idea Type Qwen3-8B Qwen3-8B-RL ∆                  

> Data Preparation 13.4 16.5 +3.1 Feature Engineering 13.4 20.9 +7.5 Model Architecture 28.5 25.3 -3.2 Model Training 32.4 27.5 -4.9 Hyperparameter Tuning 7.3 6.6 -0.7 Others 5.0 3.3 -1.7

Table 3: Comparison of idea type distributions be-tween Qwen3-8B and Qwen3-8B-RL. Values indicate the proportion (%) of ideas in each category, with 

∆ showing changes after RL training (cells shaded green for increases and red for decreases ). 

Idea Quality Matters. Is the performance boost driven by the quality of ideas, or simply by adding the <seek_help> action and receiving some form of feedback, even if it is uninformative? Table 2 ablates this factor by replacing the LLM-based IDEATOR with fixed template outputs. Using a NULL IDEA yields performance comparable to hav-ing no I DEATOR , since the Implementer receives no meaningful guidance beyond its own trajec-tory. A V AGUE IDEA , which always provides a generic encouragement, produces a slight improve-ment, likely because it prompts the Implementer to continue refining rather than stopping prema-turely. However, this effect is small compared to the substantial gains achieved with specific, con-textual ideas from the Claude Sonnet 3.5 I DEATOR .These results confirm that high-quality, actionable guidance, rather than reflection or additional in-teraction, drives the performance improvement of 

MLE-I DEATOR .

RL Aligns Idea Generation with Empirically Effective Strategies. Not all ideas contribute equally to performance, so we analyze which types are most effective. We define an idea as effective 

if it improves the ML solution’s performance. To this end, we prompt a strong LLM (Claude Sonnet 4) to classify each idea into six categories: Data Preparation, Feature Engineering, Model Architec-ture, Model Training, Hyperparameter Tuning, and Others (full prompt in Appendix E). As shown in Figure 4 (Appendix C), Feature Engineering and 

Data Preparation ideas are typically more effec-tive, whereas Model Training and Hyperparam-eter Tuning are less reliable. Building on this ob-servation, Table 3 shows that RL training shifts the idea distribution toward empirically effective cate-gories and reduces the frequency of less effective ones, thereby improving overall idea quality. We 0 10 20 30 40 50 

> Number of Steps
> 0
> 10
> 20
> 30
> 40
> 50
> 60
> Best Achieved Score
> Implementer = Claude Sonnet 3.5
> Ideator
> None
> Qwen38B
> Qwen38BRL
> Claude Sonnet 3.5
> SeekHelp Freq.
> 0
> 10
> 20
> 30
> 40
> 50
> 60
> SeekHelp Frequency (%)

Figure 2: For each I DEATOR model, we show the agent’s best achieved normalized score so far in a trajectory aver-aged over all tasks w.r.t the number of steps in trajectory. We also plot the frequency of <seek_help> actions at each steps, aggregated over all I DEATORS .

show concrete examples of the I DEATOR ’s success-ful and unsuccessful suggestions in Appendix F. 

Ideation Drives Mid-Trajectory Refinement. 

Figure 2 correlates agent performance with 

<seek_help> behavior. Initially (<12 steps), the Implementer focuses on establishing a baseline with minimal help-seeking. Once an initial sub-mission is made, <seek_help> frequency spikes. This I DEATOR -guided exploration drives imme-diate performance gains, allowing the system to escape local optima where single-agent baselines stagnate. Performance generally plateaus around step 20 due to diminishing returns on refinements and the one-hour execution limit. Future integra-tion with inference-time scaling (Snell et al., 2024; Zhu et al., 2025; Shen et al., 2025) could unlock further gains via Best-of-N idea sampling. 

5 Conclusion 

We introduce MLE-I DEATOR , a dual-agent frame-work that decouples strategic ideation from im-plementation for machine learning engineering tasks. Our experiments on MLE-Bench demon-strate that this approach significantly outperforms implementation-only baselines. Furthermore, by training the Ideator with reinforcement learning us-ing execution-based rewards, our Qwen3-8B model achieves an 11.5% relative improvement over its prompted counterpart and surpasses the proprietary Claude Sonnet 3.5, successfully aligning ideation with high-impact strategies. Our work provides a promising training recipe for LLM agents that can automate AI research. Limitations 

Additional Inference Cost. The MLE-I DEATOR 

framework incurs a higher inference cost than the implementation-only agent baseline, i.e., Code-Act (Wang et al., 2025b). When using Claude Sonnet 3.5 for both the Implementer and I DEATOR 

roles, a single run of our framework costs 1.4 USD on average, compared to 0.9 USD for the baseline. This increased cost stems from two primary factors: • Additional Steps: The dual-agent system av-erages more steps per run (17.4 vs. 15.0) due to the introduction of the <seek_help> action required to query the I DEATOR .• Expanded Context: To provide the I DEATOR 

with sufficient history, its prompt includes the entire agent trajectory (truncated to 32,000 to-kens). This significantly increases the number of input tokens per call. We expect that this cost can be mitigated. Future work could focus on developing summarization or pruning techniques (Wang et al., 2025a; Mei et al., 2025) to reduce the context length of the trajec-tory without sacrificing the quality of the generated ideas. Additionally, the inference latency introduced by ideation is minor relative to the end-to-end ML pipeline, as each <seek_help> action is just a lightweight LLM call compared to the far more time-consuming code execution, model training, and evaluation steps of each ML task. 

Resource-Intensive RL Training. The reinforce-ment learning process is resource-intensive, as cal-culating the reward for each proposed idea requires executing a full ML solution, which often involves training a neural network on a GPU. In our training setup, the reward calculation is distributed across 16 nodes, each with 8 A10G (24GB) GPUs, where each GPU is dedicated to running the solution for a single candidate idea (Appendix B.2). Our use of 128 A10 GPUs was a choice to accelerate reward computation with parallel ML solution execution, but not a strict requirement. The same procedure can be run with fewer GPUs by executing candidate solutions sequentially, trading off training time for compute resources. A primary bottleneck to scalability lies in the requirement of a full execution to evaluate each generated idea. To mitigate this limitation, future work could focus on developing a proxy reward model. Such a model, by predicting an idea’s effec-tiveness without costly execution (Park et al., 2025; Anugraha et al., 2025; Wen et al., 2025), would significantly enhance the scalability of the training process. 

References 

David Anugraha, Genta Indra Winata, Chenyue Li, Patrick Amadeus Irawan, and En-Shiun Annie Lee. 2025. Proxylm: Predicting language model perfor-mance on multilingual tasks via proxy models. In 

Findings of the Association for Computational Lin-guistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025 , pages 1981–2011. As-sociation for Computational Linguistics. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeffrey Wu. 2024. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first International Conference on Machine Learning, ICML 2024, Vi-enna, Austria, July 21-27, 2024 . OpenReview.net. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Alek-sander Madry, and Lilian Weng. 2025. Mle-bench: Evaluating machine learning agents on machine learning engineering. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net. Haoyang Fang, Boran Han, Nick Erickson, Xiyuan Zhang, Su Zhou, Anirudh Dagar, Jiani Zhang, Ali Caner Türkmen, Cuixiong Hu, Huzefa Rang-wala, Ying Nian Wu, Yuyang Wang, and George Karypis. 2025. Mlzero: A multi-agent system for end-to-end machine learning automation. CoRR ,abs/2505.13941. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2024. Mlagentbench: Evaluating language agents on machine learning experimentation. In Forty-first In-ternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 . OpenRe-view.net. Minqi Jiang, Andrei Lupu, and Yoram Bachrach. 2025a. Bootstrapping task spaces for self-improvement. 

arXiv preprint arXiv:2509.04575 .Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. 2025b. Aide: Ai-driven exploration in the space of code. ArXiv , abs/2502.13138. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. 2024. Swe-bench: Can language mod-els resolve real-world github issues? In The Twelfth International Conference on Learning Representa-tions, ICLR 2024, Vienna, Austria, May 7-11, 2024 .OpenReview.net. Zexi Liu, Yuzhu Cai, Xinyu Zhu, Yujie Zheng, Runkun Chen, Ying Wen, Yanfeng Wang, Weinan E, and Siheng Chen. 2025a. Ml-master: Towards ai-for-ai via integration of exploration and reasoning. CoRR ,abs/2506.16499. Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bolun Zhang, Lei Bai, and Siheng Chen. 2025b. Ml-agent: Reinforcing LLM agents for autonomous ma-chine learning engineering. CoRR , abs/2505.23723. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N. Fo-erster, Jeff Clune, and David Ha. 2024. The AI scien-tist: Towards fully automated open-ended scientific discovery. CoRR , abs/2408.06292. Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Bao-long Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, and Shenghua Liu. 2025. A sur-vey of context engineering for large language models. 

CoRR , abs/2507.13334. Jungsoo Park, Ethan Mendes, Gabriel Stanovsky, and Alan Ritter. 2025. Look before you leap: Estimating llm benchmark scores from descriptions. Preprint ,arXiv:2509.20645. Rushi Qiang, Yuchen Zhuang, Yinghao Li, Dingu Sagar V. K, Rongzhi Zhang, Changhao Li, Ian Shu-Hei Wong, Sherry Yang, Percy Liang, Chao Zhang, and Bo Dai. 2025. Mle-dojo: Interactive environ-ments for empowering LLM agents in machine learn-ing engineering. CoRR , abs/2505.07782. Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. 2025. Agent laboratory: Using LLM agents as research assistants. CoRR ,abs/2501.04227. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR , abs/2402.03300. Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, and Aviral Ku-mar. 2025. Thinking vs. doing: Agents that reason by scaling test-time interaction. CoRR , abs/2506.07976. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. Hybridflow: A flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025 , pages 1279–1297. ACM. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Ku-mar. 2024. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. 

CoRR , abs/2408.03314. Kyle Swanson, Wesley Wu, Nash L. Bulaong, John E. Pak, and James Zou. 2024. The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. bioRxiv .Edan Toledo, Karen Hambardzumyan, Martin Josi-foski, Rishi Hazra, Nicolas Mario Baldwin, Alexis Audran-Reiss, Michael Kuchnik, Despoina Magka, Minqi Jiang, Alisia Maria Lupidi, Andrei Lupu, Roberta Raileanu, Kelvin Niu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Michael Shvarts-man, Shagun Sodhani, Alexander H. Miller, Ab-hishek Charnalia, and 6 others. 2025. AI research agents for machine learning: Search, exploration, and generalization in mle-bench. CoRR , abs/2507.02554. Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Yuntao Du, and Pin Lyu. 2025a. Repomaster: Autonomous exploration and understanding of github repositories for complex task solving. CoRR , abs/2505.21577. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, and 2 others. 2025b. Openhands: An open platform for AI software developers as gener-alist agents. In The Thirteenth International Con-ference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net. Jiaxin Wen, Chenglei Si, Yueh-han Chen, He He, and Shi Feng. 2025. Predicting empirical AI re-search outcomes with language models. CoRR ,abs/2506.00794. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Day-iheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 40 others. 2025a. Qwen3 technical report. CoRR ,abs/2505.09388. John Yang, Carlos E. Jimenez, Alexander Wettig, Kil-ian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer in-terfaces enable automated software engineering. In 

Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Pro-cessing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024 .Sherry Yang, Joy He-Yueya, and Percy Liang. 2025b. Reinforcement learning for machine learning engi-neering agents. arXiv preprint arXiv:2509.01684 .Xu Yang, Xiao Yang, Shikai Fang, Bowen Xian, Yuante Li, Jian Wang, Minrui Xu, Haoran Pan, Xinpeng Hong, Weiqing Liu, Yelong Shen, Weizhu Chen, and Jiang Bian. 2025c. R&d-agent: Automating data-driven AI solution building through llm-powered automated research, development, and evolution. 

CoRR , abs/2505.14738. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. DAPO: an open-source LLM reinforcement learning system at scale. CoRR , abs/2503.14476. Bo Zhang, Shi Feng, Xiangchao Yan, Jiakang Yuan, Zhiyin Yu, Xiaohan He, Songtao Huang, Shaowei Hou, Zheng Nie, Zhilong Wang, Jinyao Liu, Run-min Ma, Tianshuo Peng, Peng Ye, Dongzhan Zhou, Shufei Zhang, Xiaosong Wang, Yilan Zhang, Meng Li, and 5 others. 2025a. Novelseek: When agent becomes the scientist - building closed-loop system from hypothesis to verification. ArXiv ,abs/2505.16938. Yunxiang Zhang, Muhammad Khalifa, Shitanshu Bhushan, Grant D. Murphy, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. 2025b. MLRC-Bench: Can language agents solve machine learning research challenges? 

CoRR , abs/2504.09702. King Zhu, Hanhao Li, Siwei Wu, Tianshun Xing, De-hua Ma, Xiangru Tang, Minghao Liu, Jian Yang, Jia-heng Liu, Yuchen Eleanor Jiang, Changwang Zhang, Chenghua Lin, Jun Wang, Ge Zhang, and Wangchun-shu Zhou. 2025. Scaling test-time compute for LLM agents. CoRR , abs/2506.12928. A Ideation RL Training Formulation 

A.1 Step-Level RL Objective 

We train the I DEATOR ’s policy, πθ(a|s), which is parameterized by a small language model with pa-rameters θ. Following recent work, we adopt a step-level RL paradigm for efficient training. The objective is to find the optimal parameters θ∗ that maximize the expected reward from a single-step rollout: 

θ∗ = arg max  

> θ

Est∼dπe ,α ∼πθ (·| st)[Rt(α)] 

where dπe represents the state distribution of the expert coding agent. This stepwise objective is efficient as it allows us to reuse an offline buffer of trajectories and avoids costly multi-step rollouts. 

A.2 GRPO Loss Function 

We formulate our GRPO training loss func-tion (Shao et al., 2024) as follows, while removing the KL divergence term from the loss following recent practice (Yu et al., 2025). 

JGRPO (θ) = E st∼de,c  

> {αi}Gi=1 ∼πθold (·| st)

"

1

G

> G

X

> i=1

1

|αi|

> |αi|

X

> j=1

min ( ri,j (θ)Ai,j , clip (ri,j (θ), 1 − ϵ, 1 + ϵ)Ai,j )

#

,

with ri,j (θ) = πθ(αi,j |st, α i,<j )

πθold (αi,j |st, α i,<j )

and Ai,j = Rt(αi) − mean ({Rt(αi)}Gi=1 )

std ({Rt(αi)}Gi=1 ) .

B Training Details 

B.1 Training Task Selection 

For training, we selected 10 tasks spanning diverse modalities—text, image, audio, video, and tabu-lar—whose solutions can typically be executed in under 20 minutes. They are: • osic-pulmonary-fibrosis-progression • multi-modal-gesture-recognition • chaii-hindi-and-tamil-question-answering • mlsp-2013-birds • google-quest-challenge • tgs-salt-identification-challenge • tweet-sentiment-extraction • spaceship-titanic • jigsaw-unintended-bias-in-toxicity-classification • AI4Code n=91 

> n=96
> n=137
> n=29
> n=186
> n=26
> Idea Type
> Data Preparation
> Feature Engineering
> Model Architecture
> Hyperparameter Tuning
> Model Training
> Others

Figure 3: Distribution of idea types, aggregated across Qwen3-8B, Qwen3-8B-RL and Claude Sonnet 3.5 as IDEATORS , paired with Claude Sonnet 3.5 as the imple-menter. Data Preparation    

> Feature Engineering
> Model Architecture
> Model Training
> Hyperparameter Tuning
> Others
> All
> Idea Type
> 20%
> 30%
> 40%
> 50%
> 60%
> 70%
> 80%
> Proportion of Effective Ideas
> 57.1%
> 64.6%
> 54.0% 51.1% 48.3% 46.2%
> 54.7%

Figure 4: Proportion of effective ideas in each idea type, aggregated across Qwen3-8B, Qwen3-8B-RL and Claude Sonnet 3.5 as I DEATORS , paired with Claude Sonnet 3.5 as the Implementer. We define an effective idea as the performance of the refined ML solution ac-cording to the idea is better than before. 

B.2 Training Hyperparameters 

We use VeRL (Sheng et al., 2025) as our RL train-ing framework. We train the Qwen3-8B model with LoRA (Hu et al., 2022) using a rank of 32 for one epoch on a single node (8x A100 40GB GPUs) with a batch size of 16 and a rollout size ( G) of 8 candidate ideas per prompt. We save checkpoints every 10 steps, selecting the best one based on vali-dation reward. The reward calculation is distributed across 16 nodes, each with 8 A10G (24GB) GPUs, where each GPU is dedicated to running the solu-tion for a single candidate idea. The total training time is approximately 52 hours. C Distribution and Effectiveness of Idea Types 

Figure 3 shows the overall distribution of ideas. The most frequent suggestions concerned Model Training , followed by Model Architecture and 

Feature Engineering . However, frequency alone does not imply effectiveness. We define an idea as 

effective if it improved the ML solution’s perfor-mance. As shown in Figure 4, Feature Engineer-ing had the highest success rate (64.6%), followed by Data Preparation (57.1%). In contrast, Model Training , despite being the most frequent cate-gory, was only the fourth most effective (51.1%), while Hyperparameter Tuning was least effec-tive (48.3%). This highlights a central insight: the most impactful interventions often concern data and features rather than the model itself .

D Prompt for the I DEATOR 

The specific instructions provided to the I DEATOR 

agent are detailed below. This prompt is designed to guide the model in generating high-quality, ac-tionable ideas, while also specifying the required output format. 

Idea Generation Prompt 

You are a machine learning expert. Another AI agent is struggling to improve the performance of its machine learning solution. Your task is to analyze the agent’s progress and provide the most effective algorithmic idea that can significantly improve the performance. You will be provided with the agent’s history, including its previous attempts and the full trajectory of its actions. {query} TRAJECTORY: {step_trace} ### Instruction Evaluate the current trajectory, pick the **single highest-impact next action**, and then output **exactly three items** in this format: ANALYSIS_ON_CURRENT_PROGRESS: <Briefly state whether to keep refining the present approach or revert to a prior solution and pursue a new path.> ACTION: <One imperative command or code block the agent must execute next.> RATIONALE: <Concise justification for why this action is optimal 

right now.> Do **not** list alternatives, background, or extra commentary. Output nothing beyond those three items. 

E Prompt for Idea Type Classification 

We utilize the following prompt to instruct Claude Sonnet 4 in categorizing the generated ideas. The prompt includes detailed definitions for each idea type and provides the model with rubrics to distin-guish between categories. 

Idea Type Classification Prompt 

You are a machine learning expert. Another AI agent is working on a task and has encountered a problem. Your goal is to analyze a proposed idea for improving a machine learning model and classify it into one of the specific categories defined below. {trace} ### Proposed Idea {idea} Your Task: - Review the trajectory to understand the agent’s attempts so far to solve the task. - Consider the problem description and the proposed idea. - Determine which category (from a predefined list below) best describes the core focus of the idea. - Provide a brief rationale for your classification. * Data Preparation: Ideas for handling miss-ing values, correcting errors, removing outliers, or augmenting existing data. * Feature Engineering: Ideas for creating new predictive features, transforming existing features (e.g., scaling, encoding), or selecting the most important ones. * Model Architecture: Ideas for switching to a completely different type of model (e.g., from XGBoost to a Neural Network) or changing the fundamental structure of the current model (e.g., adding/removing layers). * Hyperparameter Tuning: Ideas for systematically searching for the best model settings (e.g., learning rate, tree depth) to improve performance. * Model Training: Ideas for changing the training process (e.g., using a different loss function, implementing cross-validation) or using new metrics to evaluate the model. * Others: Use this category only if the idea clearly does not fit into any of the categories above. Output Format: output **exactly two items** in this format: TYPE: <one of "Data Preparation", "Feature Engineering", "Model Architecture", "Hyperparame-ter Tuning", "Model Training", or "Others"> RATIONALE: <briefly explain why the idea belongs to the type above> Output nothing beyond those two items. 

F Case Studies on Ideation 

F.1 An Effective Feature Engineering Idea 

In the random-acts-of-pizza task where numerical features showed very low importance ( ∼1–1.5%), the Ideator suggested creating interaction-based derived features such as karma_ratio ,

activity_ratio , engagement_score ,

text_length , and time_of_day . These fea-tures added contextual structure that the raw values lacked, and the Implementer’s accuracy improved from 0.60 → 0.64. 

Implementer Query 

PROBLEM_STATEMENT: 

Initial model shows relatively low feature importance for numerical features, need to improve model performance 

ATTEMPTS_SO_FAR: 

• Created baseline with TF-IDF text fea-tures and basic numerical features • Used RandomForestClassifier with de-fault parameters • Feature importance shows numerical features each contributing only ∼1-1.5% 

GOAL: 

Identify ways to improve model perfor-mance 

Ideator Response 

ANALYSIS_ON_CURRENT_PROGRESS: 

Keep refining the present approach, as the foundation with text and metadata features is sound but needs optimization. 

ACTION: 

> # Create derived features that capture interaction effects: karma_ratio = upvotes_minus_downvotes / upvotes_plus_downvotes activity_ratio = comments_in_raop / total_comments engagement_score = number_of_comments * karma_ratio
> text_length = len(request_text) time_of_day = extract hour from unix_timestamp

RATIONALE: 

Raw numerical features show low impor-tance because they lack context - derived features that capture relationships between metrics will better represent user credibility and request quality patterns. 

F.2 An Ineffective Hyperparameter Tuning Idea 

In the whale-categorization-playground task, the Implementer had already reached 72.31% MAP@5 with a ViT model and combined Focal+ArcFace loss. The Ideator proposed further tuning ArcFace parameters (scale, margin, alpha, gamma), but this yielded no benefit and even reduced MAP@5 (0.30 

→ 0.21). This reflects a common pattern: when the model is already near its capacity and only minor hyperparameters remain, ideation provides limited gains. 

Implementer Query 

PROBLEM_STATEMENT: 

Need to further improve MAP@5 score with current ViT + Combined Loss ap-proach that achieved 72.31% accuracy 

ATTEMPTS_SO_FAR: 

• Used EfficientNet-B0/B3 with aug-mentations ( ∼10% accuracy) • Implemented Focal Loss and ArcFace separately • Combined approach with ViT + Cus-tom Loss ( ∼17% accuracy initially) • Adjusted learning rates and increased epochs (72.31% accuracy) • Generated valid submission file 

GOAL: 

Further optimize model performance by ei-ther enhancing the current approach or ex-ploring complementary techniques while maintaining the valid submission format 

Ideator Response 

ANALYSIS_ON_CURRENT_PROGRESS: 

The current ViT model with combined Focal and ArcFace loss has achieved the same accuracy as the initial ViT approach, suggesting the loss function may not be optimally balanced. Further tuning of the loss parameters or exploring advanced regularization techniques could yield improvements. 

ACTION: 

> criterion = CombinedLoss(num_classes=len( train_dataset.id_to_idx), embedding_size=768, scale=60.0, margin=0.3, alpha=0.5, gamma=4)

RATIONALE: 

Increasing the ArcFace scale factor to 60.0 improves the margin between classes, while adjusting alpha and gamma to 0.5 and 4 re-spectively enhances focus on hard examples. This directly addresses class imbalance and improves feature discrimination, which is critical for the task.