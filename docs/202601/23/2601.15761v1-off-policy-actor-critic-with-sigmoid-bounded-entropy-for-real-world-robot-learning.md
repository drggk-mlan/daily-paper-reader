# Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning
# 采用 Sigmoid 边界熵的离线策略演员-评论家算法用于真实世界机器人学习

**Authors**: Xiefeng Wu, Mingyu Hu, Shu Zhang
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15761v1
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:RL</span>
**Score**: 7.0
**Evidence**: off-policy actor-critic reinforcement learning for real-world robotics

---

## Abstract
Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

## 摘要
由于样本效率低下

---

## 速览摘要（自动生成）

**问题**：针对现实机器人强化学习样本效率低、依赖大规模数据且在稀疏奖励下难以收敛的问题。

**方法**：提出 SigEnt-SAC，引入 Sigmoid 边界熵项，防止 Q 函数震荡及向分布外动作过度优化，仅