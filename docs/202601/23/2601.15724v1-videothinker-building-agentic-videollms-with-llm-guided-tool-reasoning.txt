Title: VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning

URL Source: https://arxiv.org/pdf/2601.15724v1

Published Time: Fri, 23 Jan 2026 01:29:43 GMT

Number of Pages: 21

Markdown Content:
# VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning 

Chenglin Li 1,5 , Qianglong Chen 1, Feng Han 2,5 , Yikun Wang 2,5 ,Xingxi Yin 1,Yan Gong 1, Ruilin Li 3,5 ,, Yin Zhang 1,5 , Jiaqi Wang 4,5 1Zhejiang University, 2Fudan University, 3Wuhan University, 

> 4

Shanghai AI Lab, 5Shanghai Innovation Institute 

Abstract 

Long-form video understanding remains a fundamental challenge for current Video Large Language Models (Vide-oLLMs). Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal local-ization and leads to substantial information loss in long videos. Agentic tools, such as temporal retrieval, spatial zoom, and temporal zoom, offer a natural way to over-come these limitations by enabling adaptive exploration of key moments. However, constructing agentic video-understanding data requires models that already possess strong long-video comprehension, creating a circular de-pendency. We address this challenge with VideoThinker, an agentic VideoLLM trained entirely on synthetic tool-interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic LLM to generate multi-step tool-use sequences in the caption space. These trajectories are subsequently grounded back to video by replacing captions with actual frames, yielding a large-scale interleaved video–tool reasoning dataset without re-quiring any long-form understanding from the underlying VideoLLM. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adap-tive temporal exploration, and multi-step tool use. Remark-ably, VideoThinker significantly outperforms both caption-only LLM agents and strong VideoLLM baselines across long-video benchmarks, demonstrating the effectiveness of tool-augmented synthetic data and adaptive retrieval–zoom reasoning for long-form video understanding. 

1. Introduction 

Video understanding is a fundamental task that requires reasoning over both spatial and temporal dimensions of videos [3, 14, 17, 22, 41]. Recent advances in Video Large Language Models (VideoLLMs) have achieved remarkable progress in this area [1, 13, 16, 19, 45, 48]. Despite these advances, existing models still struggle with long-form videos [8, 30, 35], where relevant visual evidence is sparse, distributed across time. Static reasoning over uni-formly sampled frames often causes major information loss and poor temporal localization, while also being computa-tionally costly for long videos with hundreds of frames. A promising avenue for improving long-video under-standing lies in the concept of interleaved multimodal rea-soning, which involves combining visual perception with language reasoning in a dynamic and adaptive manner [12, 33, 50]. Inspired by successful techniques in the image domain, such as those seen in OpenAI’s o3 model [23], concurrent work attempts to extend such interleaved rea-soning to videos, proposing the notion of thinking with videos [9, 21, 46]. However, these approaches face key limitations: (1) they often rely on proprietary models such as Gemini-Pro [5] to construct agentic video-understanding data, which requires extensive prompt engineering and multi-stage filtering; Moreover, although Gemini-Pro can produce interleaved video-reasoning traces, it still lacks the 

emergent video-interleaved reasoning capability observed in o3’s image reasoning [23]; and (2) they primarily per-form single temporal zoom operations without a temporal retrieval mechanism, making them inefficient at identify-ing key temporal intervals. In contrast, open-source LLMs such as Qwen3 [38] and DeepSeek [20] exhibit strong tool-augmented reasoning capabilities as language agents for long-form video understanding. They employ various tools, with VideoLLMs generating intermediate frame captions as tools to support video reasoning [6, 31, 34, 43]. However, in such frameworks, the VideoLLM serves merely as a passive captioning module, while the LLM itself cannot directly perceive visual information. Motivated by these insights, we propose VideoThinker ,an agentic VideoLLM trained entirely on synthetic tool-interaction trajectories. The core idea is to first convert videos into rich textual captions and then leverage a power-ful agentic LLM to generate multi-step tool-use sequences within this caption space. To enable effective long-video reasoning, we design two complementary agentic tools: (1) 

Temporal Retrieval , which identifies candidate temporal intervals that may contain relevant information by leverag-                     

> arXiv:2601.15724v1 [cs.CV] 22 Jan 2026 2
> VideoLLM
> 0s 30min
> (0 s, 8s)
> -> (there is a man
> walk on the road .......)
> Interval -> caption s
> Caption (0s, 8s)
> -> (there is a man....)...)
> Interval -> captions
> LLM Agent
> (Tool reasoning)
> Answer
> Strength:
> (1) Strong at agentic
> tool reasoning for long video
> weakness:
> (1) LLM lacks direct visual
> perception
> (2) VideoLLM only as captioning
> tool, resulting in inefficiency.
> Strength:
> (1) Strong at interleaved video -
> interleaved reasoning for long
> video
> (2) VideoLLM can effectively
> reason an d perceive the video
> frames only with itself
> Interval -> Frames
> VideoThinkers
> (Tool reasoning)
> Answer
> ......
> 0s 30min
> ...
> Strength:
> (1) Capable of video reasoning .
> weakness:
> (1)Struggle with long video
> with hundcreds of frames;
> (2)Limited in agentic reasoning
> Uniformly Sampl e
> VideoLLM Answer
> ......
> 0s 30min
> ...
> ...

(1) VideoLLM 

(2) LLMs Agent 

(3) VideoThinker :

> Zoom(0s, 8s)

Figure 1. Comparison of VideoThinker with VideoLLMs and LLM agents. VideoThinker excels at interleaved video reasoning on long videos, using agentic tools to iteratively perceive and reason over video frames step by step. 

ing audio transcripts (as subtitles), scene descriptions, and subtitle-based summaries; and (2) Temporal Zoom , which inspects intervals at a finer granularity through more de-tailed subtitles or frames. By leveraging these tools to-gether with the LLM’s tool-augmented reasoning capability, we construct multi-turn tool-interaction trajectories. When the LLM invokes the temporal-zoom tool, the VideoLLM generates intermediate textual captions, which are subse-quently replaced with actual video frames to form fully video-interleaved reasoning data. These trajectories are then used to fine-tune the VideoLLM, enabling it to ac-tively retrieve and perceive key frames during reasoning, effectively bridging the gap from text- and image-based rea-soning to true video reasoning. Furthermore, VideoThinker incorporates a confidence-gated tool controller, achieving strong performance gains: +6.8% on MLVU and +10.6% on LVBench over vanilla VideoLLMs, and +3.9% and +3.5% over caption-only LLM agents equipped with our tools. Our main contributions are summarized as follows: • We construct a high-quality agentic video–tool reasoning dataset guided by LLM-based tool reasoning. This syn-thetic data captures diverse temporal reasoning and tool-use behaviors essential for long video understanding. • We equip VideoThinker with dynamic reasoning, adap-tive temporal exploration, and multi-step tool use via 

Temporal Retrieval and Temporal Zoom for efficient long-video understanding. • Extensive experiments demonstrate that VideoThinker 

outperforms strong VideoLLM and LLM-agent baselines across long-video benchmarks, validating the effective-ness of tool-augmented synthetic data and adaptive re-trieval–zoom reasoning. 

2. Related Work 

2.1. Agentic LLMs for Video Understanding 

Recent advances in large language models (LLMs), par-ticularly their reasoning and planning capabilities, have spurred the development of agents [10, 40]. By leverag-ing external tools [24, 44], LLMs can acquire informa-tion, plan, and execute actions in complex environments, bridging the gap between general-purpose language mod-els and real-world applications. Extending this paradigm to video understanding, recent works integrate LLM rea-soning with agentic tools to tackle complex video tasks without costly training. However, applying such agents to video understanding poses challenges, as many exist-ing tools do not generalize well across temporal sequences. Several works have proposed video-specific adaptations of the LLM-agent paradigm [6, 15, 31, 43]. LifeLong-Memory [32] constructs a text-based episodic memory from video narrations, enabling LLMs to reason and re-trieve relevant information for downstream tasks. Dorae-monGPT [39] introduces a MCTS-based prompting strat-egy, combining tool use with structured memory for video reasoning. VideoAgent [31] frames video understanding as a sequential decision-making process, where the agent decides whether to gather more information or produce an answer, mimicking human video interpretation strategies. VideoTree [34] enhances efficiency and interpretability by adaptively extracting key visual information in a coarse-to-Tool reasoning prompt for Videothinker Tuning 

VideoThinker  

> Confidence <
> 1.Reason
> 2.Action
> 3. Obs: t ext

1) Data Synthesis with Agentic LLMs 

Trainng dataset LLM 

Tools  

> Question: In the video, a woman wearing glasses and a brown top folded the lower right
> corner of the book. What vegetables were on the book?
> A. pumpkins B. beans C. corn ....
> Reasoning with Tools
> Turn1:
> <think>: Identify where woman folds the book ... </think> <tool_call>
> clip_retrieval<tool_call>
> <tool_result>clip_retrieval[(0, 10), (130, 150)]</tool_result>
> Turn2:
> <think>:The tool returned two time intervals: (0, 10) and (130, 150). We need to ...
> <think>
> <tool_call> caption_zoom, interval=[0,10] </tool_call>
> <tool_result>The video shows a book with various images of corn,
> including......</tool_result> (VideoLLM as caption tools)
> Turn ...:
> <think>The zoomed-in analysis of the interval (0, 10) reveals that book contains images
> of corn... Final Answer:C.</think>
> < Tool reasoning prompt> (video-interleaved CoTs)
> Question: In the video, a woman wearing glasses and a brown top folded the lower
> right corner of the book. What vegetables were on the book?
> A. pumpkins B. beans C. corn ....
> Reasoning with Tools
> Turn1:
> Turn2:
> <think>:The tool returned two time intervals: (0, 10) and (130, 150). We need to
> analyze both intervals ... </think>
> <tool_call> frame_zoom, interval=[0,10] <t/ool_call>
> <tool_result><video_frames></tool_result>
> Turn ...:
> <think>The zoomed-in analysis of the interval (0, 10) reveals that book contains
> images of corn... Final Answer:C.</think>

Test dataset 

Direct reasoning prompt: 

Question: {question}, output a single 

letter. Best option: 

Answer with confidence  

> 1.Reason
> 2.Action
> 3. Obs: t ext or video frames

VideoThinker  

> Tool Pool
> Clip
> Retrieval
> (For all)
> Subtitle Retrieval
> (For all)
> Subtitle
> Summarization (For all)
> Caption
> Zoom
> (For LLM )
> Subtitle
> Zoom
> (For all)
> Frame
> Zoom
> (For VideoLLM)

2) Multimodal Tool-Reasoning Training 

3) Adaptive Video Reasoning Figure 2. VideoThinker integrates retrieval and zoom tools for multi-turn reasoning. LLMs use caption zoom to generate reasoning data from videos, which is later replaced by frame zoom and video frames to build agentic video understanding CoTs. 

fine manner and organizing it in a human-readable tree for LLM reasoning. Related approaches, such as DVD [47] and VideoExplorer [42], employ agentic search strategies over segmented video clips to tackle long-form VideoQA. Together, these works demonstrate the potential of agentic LLMs for structured, tool-augmented reasoning in complex video understanding tasks. A key limitation of these ap-proaches is that they rely on a strong LLM as the core rea-soning engine, while VideoLLMs serve only as auxiliary tools (e.g., generating captions). In contrast, our method elevates the VideoLLM itself to the reasoning core, effec-tively simplifying the agent architecture. By combining the capabilities of a VideoLLM with the agentic planning paradigm, our framework achieves better performance and reduces reliance on LLMs, improves efficiency, and stream-lines the integration of multimodal reasoning for long-form video understanding. 

2.2. Multimodal reasoning with MLLMs 

Multimodal reasoning differs from pure text reasoning in that it enables models to directly manipulate visual content, such as zooming into specific image regions [12, 27, 33, 50]. Leveraging this capability, our work focuses on de-ciding which video clips to zoom in on, enabling efficient processing of long videos. Recent concurrent works, such as Open-o3-Video [21] and Video-MTR [37], also adopt dynamic processing strategies using chain-of-thought rea-soning (CoT) over video content. These approaches gener-ate multimodal tool data for training advanced VideoLLMs with Gemini-2.5-pro. On one hand, Gemini-pro itself has not demonstrated “thinking with videos” capabilities via multimodal tools (contrary to O3, which shows “thinking with images” on image tasks). On the other hand, syn-thesizing high-quality multi-turn video reasoning data typ-ically requires data validation and extensive data engineer-ing, making direct generation costly. Differently, we con-struct the tool-augmented reasoning ability from the lan-guage agents and use video frames captions as textual prox-ies for multimodal tool outputs. This approach allows ef-ficient data synthesis while leveraging open-source models such as Qwen3 to produce high-quality video-interleaved CoT. In this way, our framework efficiently equips Vide-oLLMs with the capability to “think with videos” through tool-augmented reasoning, without heavy computational or annotation costs. 

2.3. Long-form VideoQA and Video-LLMs 

Long-form VideoQA requires reasoning over extended tem-poral contexts and capturing causal dependencies across events [8, 30, 35, 36]. Recent Video-LLMs extend temporal encoders for joint spatio-temporal reasoning [1, 16], but still face memory and computation bottlenecks when process-ing long videos. To alleviate this, several methods convert videos into textual captions or keyframe summarization, allowing LLMs to perform text-based reasoning [31, 34]. While effective for scalability, these approaches lose fine-grained temporal details. Motivated by these limitations, we propose VideoThinker, which focuses on key segments and performs frame-level video-interleaved reasoning, en-abling efficient long-form video understanding. 

3. Method 

In this section, we present VideoThinker, an agentic Vide-oLLM for long-form video understanding through dy-namic, tool-augmented reasoning. To tackle temporal spar-sity and information loss, we design two complementary tools—Temporal Retrieval and Temporal Zoom—that en-able adaptive exploration of key moments. We then con-struct a synthetic training dataset based on tool-interaction trajectories from LLM, allowing the VideoLLM to learn multi-step reasoning and video-frame perception. Describe any visual evidence in the video that directly  

> helps answer the question. If there is no relevant
> evidence, provide an objective summary of the video
> content.
> Question: <question>
> Figure 3. The prompt designed to enable VideoLLM to serve as a caption generation tool.

3.1. Tools Design 

We design two complementary agentic tools for efficient retrieval and reasoning over long-form videos. It includes functional groups: Temporal Retrieval tools (Clip Retrieval, Subtitle Retrieval, Subtitle Summary) for multimodal con-tent access, and Temporal Zoom tools (Frame Zoom, Sub-title Zoom) for fine-grained inspection of some intervals. This modular design enables adaptive focus on key seg-ments while preserving global context. Under an agen-tic search paradigm, the agent decomposes queries, com-poses tool calls, and iteratively gathers evidence to refine understanding and localize relevant information within long videos. 

3.1.1. Temporal Retrieval ClipRetrieval This tool provides coarse-grained access to semantically relevant temporal regions in long videos. Given a video path and a text query, the video is first segmented into 10-second clips, each encoded using LanguageBind-Video [52] to obtain clip-level embeddings. The tool then retrieves the top-ranked clips with the high-est semantic similarity to the input query and returns their corresponding temporal intervals. By iteratively invoking 

ClipRetrieval with refined queries informed by inter-mediate reasoning results, the agent can progressively zoom in on the most relevant video segments, efficiently narrow-ing the search space for subsequent analysis. 

SubtitleRetrieval This tool enables fine-grained text-level retrieval over automatically transcribed subtitles. Given a 

video path and a text query, Whisper [25] is employed to transcribe the video audio stream, after which the tool retrieves subtitle segments most semantically relevant to the query, along with their timestamps. Through iterative refinement, SubtitleRetrieval allows the agent to align textual evidence with visual cues, thereby precisely localizing key temporal intervals. 

SubtitleSummary This tool generates concise, query-focused summaries to support global video understanding. Built upon Qwen3-30B [38], it processes the complete sub-title transcript to extract information most relevant to the input query. The resulting summaries enable the agent to efficiently grasp the overall narrative flow and contextual dependencies of long-form videos, serving as high-level se-mantic guidance for downstream reasoning. 

3.1.2. Temporal Zoom FrameZoom This tool supports fine-grained visual inspec-tion by extracting raw frames within a specified temporal interval. Given start and end timestamps, it retrieves the corresponding frames for detailed analysis. For example, if a video contains 32 frames in total but only 2 fall within the interval [0 , 10] seconds, invoking FrameZoom(0,10) re-samples this interval to return 8 frames, thereby increasing visual density and improving perceptual detail for localized reasoning. 

SubtitleZoom This tool extracts subtitle segments corre-sponding to a specified temporal interval. By providing start and end timestamps, the agent obtains localized sub-title text aligned with the chosen segment, enabling fine-grained multimodal reasoning within temporally bounded contexts. 

CaptionZoom This tool acts as a semantic bridge between visual frames and text-based reasoning. It first invokes 

FrameZoom to extract frames from a given interval, then employs a VideoLLM to generate natural-language cap-tions summarizing key visual events, objects, and interac-tions. During tool-reasoning data synthesis, the LLM agent queries CaptionZoom for localized visual semantics us-ing textual descriptions rather than raw frames, effectively simulating perceptual grounding through tool-based inter-action. In the final training stage, these generated captions are replaced by visual frame tokens, enabling the Vide-oLLM to internalize structured reasoning patterns grounded directly in visual representations. 

3.2. Data Synthesis with Agentic LLMs 

Given a training dataset D = {(vi, x i, y i)}Mi=1 , where vi de-notes a video, xi is the corresponding question, and yi is the ground-truth answer, we construct structured tool system prompts to elicit tool-based reasoning from the LLM shown in Appendix A.1. For each query xi, a video caption ci of 

vi is generated using the VideoLLM. The query xi, caption 

ci, and tool system prompt template p are then combined to form the model input. The LLM P processes this input to produce both the reasoning trajectory ˆri and a predicted an-swer ˆyi. Each trajectory ˆri represents a multi-turn reasoning Algorithm 1 Tool-Reasoning Data Synthesis 

Require: Training sample (vi, x i), max steps T , LLM P ,VideoLLM, tool set M, action space A = M ∪ {Answer } 

> 1:

Generate video caption: ci ← VideoLLM (vi) 

> 2:

Construct initial input prompt: pi ← p(xi, c i) 

> 3:

Initialize reasoning history: H0 ← { pi, A}  

> 4:

for t = 1 to T do  

> 5:

Generate reasoning step: ˆrt ← P. reason (Ht−1) 

> 6:

Select action: (αt, ˆρt) ← P. call (ˆ rt, Ht−1) 

> 7:

if αt = Answer then  

> 8:

break  

> 9:

end if  

> 10:

Invoke tool: ˆωt ← αt(ˆ ρt) 

> 11:

Update history: Ht ← H t−1 ∪ { (ˆ rt, α t, ˆωt)} 

> 12:

if t = T then  

> 13:

Produce final answer: ˆyi ← P. answer (Ht) 

> 14:

end if  

> 15:

end for  

> 16:

return ˆyi, H

path, where each step corresponds to a reasoning decision and a tool invocation. During data synthesis, CaptionZoom is the only visual access point: it converts frames from a chosen interval into temporally grounded captions, which the LLM then reasons over. Instead of using raw frames via FrameZoom, the LLM interacts with CaptionZoom, en-abling stronger, text-based visual reasoning. To enhance reasoning diversity, we set the LLM’s sampling temperature to 0.7 and generate 5 distinct reasoning trajectories for each input. The LLM iteratively performs reasoning and tool in-vocations until it reaches a final answer or the predefined step limit T . We retain only the trajectories whose predicted answer ˆyi matches the ground-truth yi; if no trajectory sat-isfies this condition, we randomly select one trajectory from the candidates. Through this process, we obtain diverse, in-terpretable, and temporally grounded reasoning traces that facilitate robust video reasoning performance. 

3.3. Multimodal Tool-Reasoning Training 

Based on the tool-augmented reasoning data generated by the LLM, we construct the video-interleaved CoTs dataset 

Dtool = {(vi, x i, ˆri, ˆyi)}Mi=1 . Each reasoning trajectory ˆri

consists of a sequence of reasoning steps and correspond-ing tool invocations. Among these, the FrameZoom tool re-places the CaptionZoom tool and plays a central role, as it is the only component capable of accessing the video content directly. It retrieves temporally localized visual evidence, providing the LLM with perceptual grounding essential for reasoning. During dataset construction, the textual outputs of Caption Zoom are replaced by corresponding video seg-ments represented as special <video> tokens. This con-

Algorithm 2 Adaptive Reasoning in VideoThinker  

> 1:

Input: Video V ; duration L; query Q; confidence threshold 

τ ; number of sampled frames n; VideoThinker pθ 

> 2:

Initialize: retrieved clips C ← ∅  

> 3:

if L < 600 s then  

> 4:

Sample frames {fi}ni=1 ∼ Uniform (V ) 

> 5:

(ˆ y, γ ) ← pθ ({fi}, Q ) ▷ Direct reasoning  

> 6:

else  

> 7:

C ← RetrieveTopKClips (V, Q, k ) 

> 8:

(ˆ y, γ ) ← pθ (C, Q ) ▷ Direct reasoning  

> 9:

end if  

> 10:

if γ < τ then  

> 11:

(ˆ y, γ ) ← pθ (V, Q, Prompt Tool reason ) ▷ Tool reasoning  

> 12:

end if  

> 13:

return ˆy0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  

> Confidence Score
> 0
> 200
> 400
> 600
> 800
> 1000
> 1200
> 1400
> Sample Count
> Correct (52.8%) Incorrect (47.2%)

Figure 4. Confidence–accuracy relationship. The analysis is conducted on samples from VideoMME (2.7k), LongVideoBench (1.3k), and LVBench (1.5k). 

version transforms the reasoning traces into multimodal in-teraction sequences, enabling direct visual-textual supervi-sion for the VideoLLM. The multimodal student VideoLLM 

fθ is trained to reproduce both the reasoning process and the final answer by minimizing the combined objective: 

L = 1

M

> M

X

> i=1

h

ℓ(f (1: T ) 

> θ

(vi, x i), ˆri)

i

, (1) where ℓ(·) denotes the token-level cross-entropy loss. Through this training, the student VideoLLM learns to align textual reasoning with visual grounding, internalizing the teacher’s structured tool-usage logic. Consequently, it can perform interpretable, temporally grounded reasoning di-rectly over video frames. Model #Frames MLVU LVBench VideoMME (L) LongVideoBench Closed-source Models                                                                   

> GPT-4o [11] 384 54.9 48.9 72.1 66.7 Gemini-1.5-pro [29] 256 –33.1 77.4 64.0 Seed1.5VL-pro [28] 32 54.9 46.1 63.3 63.7
> Open-source Models
> Qwen2.5VL-72B [1] 128 53.8 47.4 64.6 60.3 LongVILA-7B [4] 256 49.0 –52.1 57.7 Video-XL-7B [26] 256 45.5 –54.9 50.7 Video-R1-7B [7] 64 ––52.2 –Qwen2.5VL-7B [1] 64 48.0 38.3 50.0 56.7
> Agentic LLMs
> VideoAgent [31] (GPT-4) 52.2 -46.2 -VideoAgent [6] (GPT-4) 55.4 -48.1 -VideoTree [4] (Qwen-plus) 51.6 -39.3 -VideoExplorer [42] (Qwen2.5-7B-tuning + Qwen2.5-VL-32B) 58.6 51.4 --Language Agent (Qwen-235B w/ our tools) 50.9 45.4 -58.2
> VideoThinker (Qwen2.5VL-7B w Think with videos) 64 54.8 (+6.8) 48.9 (+10.6) 53.7 (+3.7) 59.7 (+3.0)

Table 1. Evaluation results on MLVU, LVBench, VideoMME (L), and LongVideoBench. VideoThinker outperforms open-source models and demonstrates competitive performance with both closed-source models and LLM-based language agents. 

3.4. Adaptive Video Reasoning 

As shown in Figure 4, model confidence exhibits a strong correlation with prediction accuracy. Approximately 90% of the samples with confidence scores between 0.9 and 1.0 are correct, whereas more than half of those with confidence below 0.5 are incorrect. Motivated by this observation, we use confidence as a control signal to trigger multi-round tool-based reasoning. Given an input video and a query, VideoThinker first samples n frames if the video duration is shorter than 600 seconds; otherwise, it retrieves k rele-vant clips. The reasoning model V ideoT hinker (pθ ) then produces an initial answer along with a confidence score γ.

γ = exp 1

m

> m

X

> t=1

log fθ (ˆ yt | v, x, ˆy<t )

!

. (2) If γ > τ , the answer is directly returned; otherwise, VideoThinker initiates a tool-augmented reasoning process to refine the response using the tool-reasoning prompt. This adaptive two-stage design effectively balances efficiency and accuracy by combining direct reasoning with deeper, tool-guided reasoning under uncertainty. 

4. Experiments 

4.1. Experimental Setup 

Benchmarks To evaluate the performance of our model, we tested it on four distinct long video benchmarks: LongVideoBench [35], VideoMME [8], LVBench [30], and MLVU [51]. These benchmarks cover a variety of tasks to assess the model’s video comprehension and multimodal reasoning capabilities. LongVideoBench [35] is a bench-mark for long video understanding, consisting of 3,763 videos (up to 1 hour) with subtitles and 6,678 human-annotated multiple-choice QA pairs across 17 categories. We use the validation dataset for evaluation. VideoMME [8] contains 900 videos (totaling 254 hours) with 2,700 human-curated QA pairs across six domains and 30 subcategories. The videos range from 11 seconds to 1 hour and include frames, subtitles, and audio, enabling multimodal reason-ing. We mainly evaluate using the subset of long videos (over 600 seconds). LVBench [30] focuses on extreme long video comprehension, with videos ranging from 70 sec-onds to 4 hours. It includes single-scene, multi-scene, and full-scene settings, covering diverse reasoning types such as temporal, spatial, causal, hypothetical, and external knowl-edge. MLVU [51] is a benchmark for multi-task long video understanding, featuring a diverse set of videos from genres like movies, surveillance, egocentric videos, and cartoons. We use the test set for evaluation. 

Implementation Details We construct the multimodal video-interleaved tool reasoning CoTs based on CG-Bench [2], which includes 10k multiple-choice QA instances. The LLM agent uses Qwen3-235B-A22B-MoE [38] de-ployed on 4 NVIDIA H200 GPUs, while the VideoLLM is Qwen2.5-VL-7B [1]. The VideoLLM is fine-tuned on 10k video-interleaved samples for 3 epochs, utilizing up to 4 NVIDIA H200 GPUs. Training and inference are carried out using MS-Swift [49]. For efficient reasoning, Video-Short Medium Long    

> 45
> 50
> 55
> 60
> 65
> 70
> 75
> Accuracy (%)
> +0.2%
> +1.3%
> +3.7%
> VideoMME
> Short Medium Long
> --0.6%
> +5.3%
> +5.6%
> LongVideoBench
> Qwen2.5VL VideoThinker

Figure 5. Accuracy on LongVideoBench and VideoMME across different video durations. Short: < 2 min; Medium: 2–15 min; Long: > 15 min. 

Figure 6. Performance of the VideoThinker at varying confidence threshold on Long-form VideoQA. 

Thinker uses 32 frames to directly generate the answer and its confidence, with each frame having a maximum reso-lution of 32,768 pixels. During the two-stage reasoning process, the total number of frames involved does not ex-ceed 64. We evaluate VideoThinker against several baseline models, including closed-source and open-source models: • Closed-source models : GPT-4o [11], Gemini-1.5 Pro [29], and Seed 1.5VL-Pro [28]. • Open-source models : Qwen2.5VL-72B [1], LongVILA-7B [4], Video-XL-7B [26], Video-R1-7B [7], and Qwen2.5VL-7B [1]. • Agentic LLMs : VideoAgent (with GPT-4) [31], VideoA-gent (with GPT-4) [6], VideoTree (with Qwen-plus) [34], and VideoExplorer (Qwen2.5-7B-tuning + Qwen2.5-VL-32B) [42]. For the LVBench and MLVU benchmarks, audio sub-titles are generated using FFmpeg and Whisper [25]. All decoding follows the official configurations, with the confi-dence threshold set to 0.7 by default. 

4.2. Main results 

As shown in Table 1, VideoThinker consistently out-performs open-source models and remains competitive with both closed-source systems and LLM-based agents. Specifically, VideoThinker achieves 54.8% on MLVU, surpassing Qwen2.5VL-7B (48.0%) and Qwen2.5VL-72B (53.8%), and performing on par with GPT-4o (54.9%). On LVBench, it reaches 48.9%, outperforming Qwen2.5VL-7B (38.3%) and Seed1.5VL-Pro (46.1%), and matching GPT-4o. For VideoMME, VideoThinker achieves 53.7%, ex-ceeding Qwen2.5VL-7B (50.0%). On LongVideoBench, it obtains 59.7%, surpassing Qwen2.5VL-7B (56.7%) and closely approaching Qwen2.5VL-72B (60.3%). VideoEx-plorer achieves 58 .6% on MLVU and 51 .4% on LVBench, whereas VideoThinker obtains 54 .8% and 48 .9% with a sin-gle 7B model. In contrast, VideoExplorer is implemented as a language-agent pipeline that couples a separate large-scale LLM with an external Video-LLM, while VideoThinker is a single end-to-end 7B VideoLLM, highlighting the effi-ciency of our approach. Notably, in the LLM-based agent setting, we compare against the language agents used in our data synthesis process. Compared with this redundant architecture—where VideoLLM acts as the tool, Video-Thinker achieves an overall improvement of about 3%, in-cluding a +3.9% gain on MLVU. Overall, these results highlight VideoThinker’s strong performance across bench-marks, showing that our ”Thinking with Videos” framework enables competitive long-form video reasoning while main-taining a lightweight design. 

4.3. Ablation studies 

Effect of Video Duration As shown in Figure 5, Video-Thinker performs on par with Qwen2.5-VL when han-dling short videos (under 2 minutes). However, its advan-tage becomes evident as the video length increases. For medium videos (between 2 and 15 minutes), VideoThinker achieves consistently higher accuracy on both VideoMME and LongVideoBench, showing stronger capability in mod-eling longer temporal dependencies. The improvement is even more significant on long videos (exceeding 15 min-utes), where VideoThinker surpasses Qwen2.5-VL by 3.7 percentage points on VideoMME and 5.6 percentage points on LongVideoBench. These results demonstrate that Video-Thinker maintains stable reasoning and robust understand-ing in extended temporal contexts, effectively addressing the information loss and temporal drift commonly encoun-tered in long-form video understanding. 

Impact of n and τ on Performance We analyze the effects of the number of frames ( n) and the confidence threshold ( τ ) on model performance. As shown in Fig-ure 6, increasing n from 8 to 64 significantly improves accuracy—from 56.5% to 59.7% on LongVideoBench and from 57.7% to 62.6% on VideoMME—indicating that denser sampling provides richer visual context for reason-ing. For the confidence threshold τ , which controls the switch between fast and tool-based reasoning, performance improves as τ increases from 0.0 to 0.8 (from 55.3% to Dataset Retrieval clips (Top-k) Retrieval subtitles (Top-k)                               

> 13510 13510
> LongVideoBench 59.7 59.8 58.8 58.6 57.4 58.4 58.0 57.1
> VideoMME (L) 53.7 52.3 54.2 54.7 51.2 50.2 53.1 52.3
> LVBench 48.9 49.1 47.2 49.0 50.6 50.5 49.0 49.3

Table 2. Ablation of Retrieval clips and Retrieval subtitles with varying Top-k values ( 1, 3, 5, 10 ) on three datasets. 

59.0% on LongVideoBench and from 57.8% to 60.8% on VideoMME), but drops when τ = 1 .0 (56.5% and 58.3%, respectively). A low τ causes overreliance on direct rea-soning, while a high τ triggers excessive tool reasoning. These results suggest that adaptive confidence-gated rea-soning, combined with more frame samples, yields the best overall performance. 

Ablation on Retrieval Tools We analyze the effect of re-trieval by varying the number of retrieved clips and subtitles (Top-k). As shown in Table 2, Top-1 is generally subopti-mal, indicating that a single retrieved item provides limited context. Increasing k typically improves performance as the model benefits from richer and more diverse information. On LongVideoBench , the best results appear at Top-3 for clips (59.8) and subtitles (58.4), showing a good balance be-tween relevance and noise. VideoMME (L) favors broader context, peaking at Top-10 for clips (54.7) and Top-5 for subtitles (53.1). Top-1 is often slightly worse or compara-ble to larger k, though on LVBench subtitle retrieval, Top--1 performs best, indicating that the optimal k can be dataset-dependent. 

4.4. Case Study 

To further illustrate the multi-step reasoning process of VideoThinker, we present a representative case in Figure 7. The model is asked the question: “When the subtitle men-tions ‘the old people that can’t work anymore,’ what is the woman with the pink hat wearing?” VideoThinker first invokes the Subtitle Retrieval tool to locate the tem-poral position of the mentioned subtitle. The system ac-curately identifies the corresponding timestamp at 357.11s, which serves as a temporal anchor for subsequent reason-ing. Based on the retrieved timestamp, VideoThinker em-ploys the frame zoom tools to examine the visual context within a narrow temporal window (350–360s). It focuses on frames depicting the woman with the pink hat and an-alyzes her appearance in detail. Through this fine-grained inspection, the model correctly determines that the woman is wearing a red short-sleeve shirt. 

5. Conclusion 

In this work, we present VideoThinker, an agentic Vide-oLLM designed for long-form video understanding through {"name": "subtitle_retrieval", "arguments": {"text": "the old people                                                                                                                            

> thatcan'tworkanymore"}}
> </tool_call>'
> <tool_use_result>
> subtitle_retrieval
> Theinterval:[(357.11,357.12)]
> Question: In front of a light yellow wall, there is a woman wearing a
> pink hat sitting with another woman with long hair wearing a dark blue
> outfit. When the subtitle mentions 'the old people that can't work
> anymore,' what is the woman with the pink hat wearing?"
> A. red short sleeves B. red long sleeves C. black long sleeves
> D. pink short sleeves
> Output:
> To solve this question, I need to determine the exact moment in the
> video when the subtitle "the old people that can't work anymore"
> appears and then analyze the visual content at that time to identify what
> the woman with the pink hat is wearing.
> ### Step 1: Locate the time interval when the subtitle "the old people
> that can't work anymore" appears.
> I will use the `Subtitle_Retrieval` tool to find the exact time interval
> when this subtitle appears.
> <tool_call>
> :
> {"name":"frame_zoom","arguments":{"interval":[350,360]}}
> </tool_call>
> <tool_use_result>
> frame_zoo
> 'theoldpeoplethatcan'tworkanymore
> </tool_use_result>
> Thesubtitle"theoldpeoplethatcan'tworkanymore"appearsat
> approximately**357.11to357.12seconds**.Thisisaveryshort
> interval,soIneedtozoominaroundthistimetoanalyzethevisual
> contentaccurately.
> ###Step2:Zoomintotheintervalaroundthesubtitletoexaminethe
> scene.Iwillusethe`frame_zoom`tooltofocusonaslightlybroader
> windowaround350–360secondstocapturethevisualdetails.
> <tool_call>
> m
> </tool_use_result>
> Thezoomed-ininterval(350,360)revealsthatthewomanwiththepinkhatis
> wearinga**redshortsleeves**shirt.Thisdirectlyanswersthequestionabouther
> clothingwhenthesubtitle"theoldpeoplethatcan'tworkanymore"appears.
> ###FinalAnswer:
> A.redshortsleeves

Think with Videos

Figure 7. Case Study: VideoThinker performs agentic tool use by retrieving subtitles to locate temporal intervals and zooming into relevant frames for video reasoning. 

adaptive Temporal Retrieval and Temporal Zoom tool use. By leveraging temporal retrieval and temporal zoom, VideoThinker enables adaptive exploration of key video moments, allowing for dynamic reasoning over long videos. Trained on synthetic tool-interaction data generated through LLM-guided reasoning, VideoThinker learns to reason across long videos without requiring pre-existing long-form understanding. Extensive evaluations on multiple long-form video benchmarks demonstrate that VideoThinker consistently outperforms strong VideoLLM and LLM-agent baselines, validating the effectiveness of adaptive, tool-augmented reasoning for long-form video understanding. VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning 

# Supplementary Material 

Figure 8. Distribution of Video Durations in CoTs. 

A. Supplementary Materials 

A.1. Prompt Details 

As shown in Figure 9, we provide the detailed Tool reason-ing prompt used in our framework. This prompt specifies the expected output format for tool calls, defines all avail-able tools, and outlines the instructions for each task. For comparison, the Direct reasoning prompt is defined as fol-lows: Question: {question }, output a single letter. Best option: 

A.2. Tools 

As shown in Table 3, our framework provides a set of tools for long-form video reasoning at multiple levels of granu-larity. The toolkit combines retrieval modules, such as Clip Retrieval and Subtitle Retrieval, with zoom-level tools, in-cluding Frame Zoom, Subtitle Zoom, and Caption Zoom, to support both coarse- and fine-grained analysis. The Subtitle Summary module further offers high-level textual abstrac-tion. Together, these tools enable effective and interpretable reasoning over long-duration videos. 

A.3. Training Data 

We analyze the distribution of video lengths in the CG-bench synthetic dataset, as illustrated in Figure 8. More than half of the videos have durations between 20 and 40 min-utes, while those lasting 40- 60 minutes account for 17.1%. Most data samples involve approximately 3-5 tool calls. You are in an environment with access to video analysis tools. \n Tool call format:      

> <tool_call>
> {"name": "tool_name", "arguments": {"param": value}}
> </tool_call>
> Available tools:
> # Tools
> You may call one or more functions to assist with the user query.
> <tools>
> <tool_call>
> <name> frame _zoom </name>
> <function>
> { "name": " frame _zoom ",
> "description": "Time range in seconds as [start_time, end_time]. The smaller the time
> interval, the more detailed the analysis will be. This tool is useful when zoom in the temporal
> interval to get the visual content of interval. ",
> "parameters": {"interval": {"type": "array", "items": {"type": "number"}}},
> "required": ["interval"] }
> </function>
> </tool_call>
> <tool_call>
> <name> subtitle_zoom </name>
> <function>
> {"name": " subtitle_zoom ",
> "description": "Locate the interval where subtitle appears. This tool is useful when the
> question mentions subtitle.",
> "parameters": {"interval": {"type": "array", "items": {"type": "number"}}},
> "required": ["interval"] }
> </function>
> </tool_call> .....
> ---
> ## Task Instructions :Use the available tools as needed, following the XML-style format guidelines,
> to answer the question.

Figure 9. The prompt is designed to enable VideoLLM to think with videos using the available tools. 

A.4. Implementation Details 

We present the training script implemented with Swift, as shown in Figure 10. 

A.5. More results 

Table 4 presents the comparison results on LVBench across six tasks, including Entity Recognition, Event Understand-ing, Key Information Retrieval, Temporal Grounding, Rea-soning, and Summarization. Our method VideoThinker achieves the best overall performance with an average accuracy of 48.9%, comparable to GPT-4o (48.9%) and clearly outperforming previous LLM-based agents such as VideoTree (28.8%) and VideoAgent (29.3%). ReZoom-V also shows strong results on KIR (58.0%) and ER (49.6%), demonstrating its effectiveness in retrieval tasks. We present additional LVBench cases in Figure 11–16. Tool Parameters Description 

Clip Retrieval video path, query, topk Retrieves the top-k video clips most relevant to the given query based on semantic similarity. Subtitle Retrieval video path, query, topk Retrieves the top-k subtitle segments relevant to the query from Whisper-transcribed subtitles based on semantic similarity. Subtitle Summary video path, query Summarizes subtitles using an LLM to provide concise, query-focused contextual understanding for reasoning. Frame Zoom video path, interval Extracts video frames from the specified time interval. Subtitle Zoom video path, interval Extracts subtitles from the specified time interval. Caption Zoom video path, interval Extracts visual captions from the specified time interval. 

> Table 3. Tool suites for long-form video reasoning, integrating retrieval-based and zoom-level analysis capabilities.

Methods ER (%) EU (%) KIR (%) TG (%) Rea (%) Sum (%) Overall (%) 

VideoLLMs 

Gemini-1.5-Pro [29] 32.1 30.9 39.3 31.8 27.0 32.8 33.1 GPT-4o [11] 48.9 49.5 48.1 40.9 50.3 50.0 48.9 Qwen2.5-VL-72B [1] – – – – – – 47.4 VideoChat-Flash [18] 51.1 46.0 49.0 38.9 48.5 34.5 48.2 

Agentic LLMs 

VideoTree [34] 30.3 25.1 26.5 27.7 31.9 25.5 28.8 VideoAgent [31] 28.0 30.3 28.0 29.3 28.0 36.4 29.3 

VideoThinker (Ours) 49.6 48.5 58.0 43.6 45.5 35.2 48.9 

> Table 4. Comparison on LVBench across various tasks: Entity Recognition (ER), Event Understanding (EU), Key Information Retrieval (KIR), Temporal Grounding (TG), Reasoning (Rea), Summarization (Sum), and Overall performance.

swift sft \ 

--model Qwen2.5-VL-7B-Instruct \ 

--train_type lora \ 

--dataset vcbench/train.jsonl \

--load_from_cache_file true \ 

--torch_dtype bfloat16 \ 

--num_train_epochs 3 \ 

--per_device_train_batch_size 1 \ 

--per_device_eval_batch_size 1 \ 

--learning_rate 1e-4 \ 

--lora_rank 64 \ 

--lora_alpha 16 \ 

--freeze_vit True \ 

--target_modules all-linear \ 

--gradient_accumulation_steps 2 \ 

--eval_steps 50 \ 

--save_steps 500 \ 

--save_total_limit 5 \ 

--logging_steps 5 \ 

--output_dir save_vcbench_3b \ 

--warmup_ratio 0.05 \ 

--dataloader_num_workers 8 \ 

--use_chat_template True \ 

--max_length 200000 Figure 10. The training script with Swift. Tool_Prompt: You are in an environment with access to video analysis tools. ...... 

What year appears in the opening caption of the video? 

(A) 1636 (B) 1366 (C) 1363 (D) 1633 

[Begin of Agentic Thinking] To determine the year that appears in the opening caption of the video, I need to locate the very beginning of the video and 

examine the visual content where the caption is likely displayed. 

Step 1: Start by analyzing the earliest part of the video, focusing on the opening scene. I will use the ` Frame_Zoom ` tool to narrow down to the first few seconds 

where opening captions typically appear. 

<tool_call> 

{"name": "Frame _Zoom", "arguments": {"interval": [0, 10]}} 

</tool_call> 

The zoom into the interval [0, 10] revealed the start of the video with "A NETFLIX SERIES" and some animated fire effects, but no visible year in the 

caption. However, opening captions sometimes appear slightly after the initial logo or animation. 

Step 2: Expand the analysis slightly to cover a broader opening segment, just after the Netflix branding, to ensure we don ’t miss any text that might 

appear next. I will now zoom into the interval [10, 20] to check for any subsequent captions. 

<tool_call> 

{"name": " Frame _Zoom ", "arguments": {"interval": [10, 20]}} 

</tool_call>'} 1633 

The zoom into the interval [10, 20] revealed the text: *"In 1633, Japan closed its borders to the outside world." * This clearly indicates that the 

year appearing in the opening caption is **1633 **. [End of Agentic Thinking] 

### Final Ans wer: 

D 1633. 

...... Figure 11. VideoThinker’s agentic tool reasoning on LVBench, testing its ability to retrieve key information from a 61-minute video (Cm73ma6Ibcs). Tool_Prompt: You are in an environment with access to video analysis tools. ...... 

Why does the protagonist tie iron to her limbs? 

(A) Because she wants to strength her body (B) Because irons are decorations (C) Because she has superpower and iron surpress i t

(D) Because tying iron is a culture tradition 

[Begin of Agentic Thinking] To determine why the protagonist ties iron to her limbs, I need to analyze the context in which this action occurs. Since the question 

involves a narrative or symbolic reason (not purely visual), I should first look for any relevant subtitles or dialogue that might explain her motivation. 

I will start by gathering hints from the subtitles that could provide context about the protagonist ’s actions. 

<tool_call> 

{"name": "Subtitle _Summary", "arguments": {}} 

</tool_call> 

Since the subtitles do not provide a clear explanation, I will now attempt to locate specific scenes where the protagonist is seen tying iron to her limbs. 

This visual context may help infer the reason based on surrounding actions or symbolism. I will search for a scene that matches the description of "the 

protagonist tying iron to her limbs." 

<tool_call> 

{"name": "Clip _Retrieval", "arguments": {"text": "protagonist tying iron to her limbs", "top_k": 3}} 

</tool_call> 

I have identified two time intervals where the protagonist is seen tying iron to her limbs: [2140, 2160] and [2450, 2460]. To analyze this behavior in 

detail, I will first zoom into the earlier and longer interval [2140, 2160] to observe the context and actions more clearly. 

<tool_call> 

{"name": " Frame _Zoom ", "arguments": {"interval": [2140, 2160]}} 

</tool_call> 

...... 

<tool_use_result> 

Subtitle _Summary 

Summary : The subtitles do not contain any information about the protagonist tying iron to her limbs, nor do they explain the reason for such an action. None of 

the provided dialogue or scenes reference iron being tied to limbs for strength, decoration, suppression of powers, or cultural tradition. Therefore, based on the 

given subtitles, there is no clear evidence to support or explain any of the answer choices.. 

</tool_use_result> 

<tool_use_result> 

Clip _Retrieval 

The interval: [(2140, 2160), (2450, 2460)] 

</tool_use_result> 

The zoomed-in analysis of the interval [2140, 2160] suggests that the protagonist is using iron as part of a training exercise , specifically 

in connection with a forge and physical conditioning. The context implies that the iron is being used to strengthen her body through 

resistance, not for decoration, cultural tradition, or suppression of powers. 

To ensure this interpretation is consistent, I will now examine the second interval [2450, 2460] where the same action occurs, to confirm 

the pattern and rule out alternative explanations. 

<tool_call> 

{"name": " Frame _Zoom ", "arguments": {"interval": [2450, 2460]}} 

</tool_call> 

The analysis of the second interval [2450, 2460] reinforces the conclusion from the first interval: the protagonist ties iron to her limbs as part of physical 

preparation, likely to strengthen her body for an upcoming challenge or battle. The deliberate and focused manner in which she applies the iron, combined with 

the absence of any symbolic or decorative context, rules out options (B) and (D). There is also no indication that she possesses superpowers or that the iron 

suppresses them, eliminating option (C). 

Instead, both visual intervals consistently support the idea that the iron serves a functional, physical training purpose. [End of Agentic Thinking] 

### Final Ans wer: 

A. Because she wants to str engthen her body Figure 12. VideoThinker’s agentic tool reasoning on LVBench, testing its reasoning ability from a 61-minute video (Cm73ma6Ibcs). Tool_Prompt: You are in an environment with access to video analysis tools. ...... 

What does the woman in red do after she collides with the man? 

(A) She picks up things on the ground on her own and throws them away (B) She picks up things on the ground with the man 

(C) She picks up things on the ground on her own (D) She picks up things on the ground with a passerby 

[Begin of Agentic Thinking] To determine what the woman in red does after she collides with the man, I need to first locate the moment of collision between the 

woman in red and the man. I will start by searching for a scene that matches this description. 

<tool_call> 

{"name": " Clip _Retrieval ", "arguments": {"text": "woman in red collides with man", "top_k": 1}} 

</tool_call> 

The collision between the woman in red and the man occurs in three possible intervals: [850, 860], [940, 950], and [2160, 2170]. To determine which one is 

most relevant and to analyze the aftermath of the collision, I will first zoom into the earliest interval [850, 860] to examine the visual details.\n\n<tool_call>\n 

{"name": "zoom_interval", "arguments": {"interval": [850, 860]}}\n</tool_call> 

Since both visual interval have not yielded a clear answer, I will now attempt to locate more precise moments where actions like "picking up 

things" occur, especially in the context of a collision. 

I will use ` Clip _Retrieval ` with a more focused description: "woman picking up things after collision". 

<tool_call> 

{"name": " Clip _Retrieval ", "arguments": {"text": "woman picking up things after collision", "top_k": 3}} 

</tool_call> 

..... 

<tool_use_result> 

Clip _Retrieval 

The interval: [(850, 860) 

</tool_use_result> 

<tool_use_result> 

Clip _Retrieval 

The interval: [(170, 180), (940, 950), (2960, 2970)] 

</tool_use_result> 

The zoomed-in analysis of the interval [170, 180] clearly shows the woman in red picking up items from the ground in a dimly lit room, 

with no interaction with the man or any passerby. She performs the action independently . [End of Agentic Thinking] 

### Final Ans wer: 

C. She picks up things on the ground on her own 

The new intervals where a "woman picking up things after collision" may have occurred are [170, 180], [940, 950], and [2960, 2970]. Let's now 

zoom into [170, 180], which could be an earlier event in the video. 

<tool_call> 

{"name": " Frame _Zoom ", "arguments": {"interval": [170, 180]}} 

</tool_call> Figure 13. VideoThinker’s agentic tool reasoning on LVBench, testing its event-understanding ability using a 55-minute video (q01CUy gwdU). Tool_Prompt: You are in an environment with access to video analysis tools. ...... 

What color is the first plane? 

(A) Yellow (B) Red (C) White (D) Blue 

[Begin of Agentic Thinking] To determine the color of the first plane, I need to locate when the plane appears in the video. Since the question refers to the "first" 

plane, I should identify the earliest occurrence of a plane in the scene. 

I will start by searching for scenes that match the description of a "plane" to get the relevant time intervals. 

<tool_call> 

{"name": " Clip _Retrieval ", "arguments": {"text": "plane", "top_k": 3}} 

</tool_call> 

The intervals where the plane appears are (570, 580) and (590, 610). Since we are looking for the "first" plane, the earliest occurrence is in the interval (570, 

580). I will now zoom into this interval to get a more detailed view of the scene and observe the color of the plane. 

<tool_call> 

{"name": " Frame _Zoom ", "arguments": {"interval": [570, 580]}} 

</tool_call> 

The zoomed-in analysis of the interval [570, 580] clearly indicates that the first plane is yellow. The description mentions that the plane's body 

and wings are painted in a bright yellow color, and there are no red or blue planes in this sequence. [End of Agentic Thinking] 

### Final Ans wer: 

A. Yellow 

..... 

<tool_use_result> 

Clip _Retrieval 

The interval: [(570, 580), (590, 610)] 

</tool_use_result> Figure 14. VideoThinker’s agentic tool reasoning on LVBench, testing its event-recognition ability using a 55-minute video (q01CUy gwdU). Tool_Prompt: You are in an environment with access to video analysis tools. ...... 

What color are the runes on the bottom of the girl's feet at 38:18 in the video? 

(A) Golden (B) White (C) Red (D) Black 

[Begin of Agentic Thinking] To determine the color of the runes on the bottom of the girl's feet at 38:18, I need to analyze the visual content around that 

timestamp. Since the question refers to a very specific visual detail (runes on the feet) at an exact time (38:18), I should first zoom into a narrow interval around 

that moment to inspect the frame details. 

I'll start by zooming into an interval centered around 38:18, say from 38:15 to 38:21 (2295 to 2301 seconds), to get a close look at the scene. 

<tool_call> 

{"name": " Frame _Zoom ", "arguments": {"interval": [2295, 2301]}} 

</tool_call> 

The zoomed-in analysis of the interval around 38:18 (2295 to 2301 seconds) clearly indicates that the runes on the bottom of the girl's feet are 

glowing with a warm, golden light. The visual description confirms that the runes are golden in color. [End of Agentic Thinking] 

### Final Ans wer: 

A. Golden 

..... Figure 15. VideoThinker’s agentic tool reasoning on LVBench, testing its temporal grounding ability using a 64-minute video (28CIeC8cZks). Tool_Prompt: You are in an environment with access to video analysis tools. ...... 

What happens between 22:17-42:05? 

(A) Reporter spends three months with the Penan people, understanding their psychological characteristics and documenting the lifestyle of some Penan people living 

in the city 

(B) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also documented the lifestyle of 

some Penan people living in the city 

(C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle of some 

Penan people living in the village 

(D) Reporter spends three months with the Penan people, understanding their psychological characteristics and recording the lifestyle of some of the Penan people 

living in the countryside 

[Begin of Agentic Thinking] The question asks about what happens between 22:17 and 42:05 in the video. This is a 19-minute and 48-second interval, which is 

quite long, so we need to analyze the content during this period carefully. 

First, let's convert the timestamps to seconds: 

- 22:17 = (22 × 60) + 17 = 1337 seconds 

- 42:05 = (42 × 60) + 5 = 2525 seconds 

So, we are analyzing the interval [1337, 2525]. 

Since the options involve themes like "learning history, lifestyle, hunting, darts," and "documenting lifestyle in city/village/countryside," we should determine 

whether these topics are discussed or shown in the video during this interval. 

A good starting point is to get subtitle hints from this interval to understand the main themes discussed. 

<tool_call> 

{"name": "Subtitle Summary ", "arguments": {"text": "22:17 to 42:05"}} 

</tool_call>'} 

..... 

<tool_use_result> 

Subtitle Summary 

The hints: Between 22:17 and 42:05, the reporter documents his time living with the Penan people, learning their survival skills, history, and traditional practices 

such as hunting with blowpipes and using poison darts. He stays with semi-nomadic families in jungle camps, observes their daily life, and records their methods 

of foraging, building shelters, and using medicinal plants. The focus is on their traditional lifestyle in the forest, not in the city. Key activities include harvesting 

sago, making bark cloth, and learning about their ethnobotanical knowledge.. 

</tool_use_result> 

The subtitle hints indicate that during the interval [22:17–42:05], the reporter: 

- Lives with the Penan people 

- Learns their history and traditional practices 

- Learns hunting with blowpipes and using poison darts 

- Observes daily life in jungle camps (i.e., village/forest setting) 

- Records foraging, shelter-building, medicinal plant use, sago harvesting, and bark cloth making 

- Focus is on traditional lifestyle in the forest, **not in the city **

Now, let ’s compare this with the options: 

(A) Mentions "psychological characteristics" and "living in the city" — not supported by the hints 

(B) Mentions learning history, lifestyle, hunting, darts — supported; but says "living in the city" — contradicted 

(C) Mentions learning history, lifestyle, hunting, darts — supported; and "living in the village" — consistent with jungle camps/forest 

(D) Mentions "psychological characteristics" — not mentioned; "countryside" is vague, but "understanding psychological characteristics" is not supported 

So, (C) appears to be the best fit. But to confirm, let ’s verify if the term "village" or "jungle camp" appears in the scene description during this interval. 

..... 

(tool_use )

The subtitle at [1611–1620] seconds (~26:51–27:00) states: 

**"The villagers have a hornbill as a pet." **

Followed by: **"Hornbills are the emblem of Sarawak." **

This confirms that the term "village" is used in context to describe the Penan or local community living in a traditional setting (jungle/village), not in an urban 

environment. The mention of a hornbill as a pet also aligns with traditional forest life. 

Combined with earlier evidence: 

- The reporter learns history, lifestyle, hunting, and dart use 

- Focus is on jungle camps and forest living 

- "City" is mentioned once but without contextual support about documenting urban Penan lifestyle 

- "Village" is mentioned in context of traditional life 

Thus, option (C) is fully supported: 

> **(C) Reporter spends three months with the Penan people, learning about their history, lifestyle, how to hunt, how to use darts, and also recorded the lifestyle 

of some Penan people living in the village **

This matches the video content both visually (jungle camp scenes) and through subtitles (village, traditional practices). [End of Agentic Thinking] 

### Final Ans wer: 

CFigure 16. VideoThinker’s agentic tool reasoning on LVBench, testing its summarization ability using a 53-minute video (TiQBTesZUJQ). References 

[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 , 2025. 1, 3, 6, 7, 2 [2] Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cg-bench: Clue-grounded question answering benchmark for long video understanding. arXiv preprint arXiv:2412.12075 ,2024. 6 [3] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering. arXiv preprint arXiv:2311.14906 , 2023. 1 [4] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188 , 2024. 6, 7 [5] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blis-tein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 , 2025. 1 [6] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: A memory-augmented mul-timodal agent for video understanding. In European Confer-ence on Computer Vision , pages 75–92. Springer, 2024. 1, 2, 6, 7 [7] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776 ,2025. 6, 7 [8] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075 , 2024. 1, 3, 6 [9] Shenghao Fu, Qize Yang, Yuan-Ming Li, Xihan Wei, Xi-aohua Xie, and Wei-Shi Zheng. Love-r1: Advancing long video understanding with an adaptive zoom-in mechanism via multi-step reasoning. arXiv preprint arXiv:2509.24786 ,2025. 1 [10] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. arXiv preprint arXiv:2402.02716 , 2024. 2 [11] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perel-man, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Weli-hinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. 

arXiv preprint arXiv:2410.21276 , 2024. 6, 7, 2 [12] Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning pat-terns and interaction turns for visual search. arXiv preprint arXiv:2509.07969 , 2025. 1, 3 [13] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Zi-wei Liu, et al. Llava-onevision: Easy visual task transfer. 

arXiv preprint arXiv:2408.03326 , 2024. 1 [14] Chenglin Li, Qianglong Chen, Zhi Li, Feng Tao, and Yin Zhang. Vcbench: A controllable benchmark for symbolic and abstract challenges in video cognition. arXiv preprint arXiv:2411.09105 , 2024. 1 [15] Chenglin Li, Feng Han, Ruilin Li, Qianglong Chen, Jingqi Tong, Yin Zhang, Jiaqi Wang, et al. Adaptive fast-and-slow visual program reasoning for long-form videoqa. arXiv preprint arXiv:2509.17743 , 2025. 2 [16] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 , 2023. 1, 3 [17] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understand-ing benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 22195– 22206, 2024. 1 [18] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical com-pression for long-context video modeling. arXiv preprint arXiv:2501.00574 , 2024. 2 [19] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represen-tation by alignment before projection. arXiv preprint arXiv:2311.10122 , 2023. 1 [20] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. 

arXiv preprint arXiv:2412.19437 , 2024. 1 [21] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, and Zhuochen Wang. Open-o3 video: Grounded video reasoning with explicit spatio-temporal evi-dence. arXiv preprint arXiv:2510.20579 , 2025. 1, 3 [22] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: A com-prehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103 ,2023. 1 [23] OpenAI. Introducing openai o3 and o4-mini. https:// openai.com/index/introducing-o3-and-o4-mini/ , 2025. Accessed: 2025-11-01. 1 [24] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 ,2023. 2 [25] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In Interna-tional conference on machine learning , pages 28492–28518. PMLR, 2023. 4, 7 [26] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Jun-jie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 26160–26169, 2025. 6, 7 [27] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Jun-tao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617 , 2025. 3 [28] ByteDance Seed Team. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062 , 2025. 6, 7 [29] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of con-text. arXiv preprint arXiv:2403.05530 , 2024. 6, 7, 2 [30] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiao-han Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035 , 2024. 1, 3, 6 [31] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision , pages 58–76. Springer, 2024. 1, 2, 3, 6, 7 [32] Ying Wang, Yanlai Yang, and Mengye Ren. Lifelongmem-ory: Leveraging llms for answering queries in long-form egocentric videos. arXiv preprint arXiv:2312.05269 , 2023. 2[33] Ye Wang, Qianglong Chen, Zejun Li, Siyuan Wang, Shi-jie Guo, Zhirui Zhang, and Zhongyu Wei. Simple o3: To-wards interleaved vision-language reasoning. arXiv preprint arXiv:2508.12109 , 2025. 1, 3 [34] Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. Videotree: Adaptive tree-based video representation for llm reasoning on long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 3272– 3283, 2025. 1, 2, 3, 7 [35] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context interleaved video-language understanding. Advances in Neural Informa-tion Processing Systems , 37:28828–28857, 2024. 1, 3, 6 [36] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF con-ference on computer vision and pattern recognition , pages 9777–9786, 2021. 3 [37] Yuan Xie, Tianshui Chen, Zheng Ge, and Lionel Ni. Video-mtr: Reinforced multi-turn reasoning for long video under-standing. arXiv preprint arXiv:2508.20478 , 2025. 3 [38] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025. 1, 4, 6 [39] Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, and Yi Yang. Doraemongpt: Toward understanding dynamic scenes with large language models (exemplified as a video agent). arXiv preprint arXiv:2401.08392 , 2024. 2 [40] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations , 2022. 2[41] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yuet-ing Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelli-gence , pages 9127–9134, 2019. 1 [42] Huaying Yuan, Zheng Liu, Junjie Zhou, Hongjin Qian, Yan Shu, Nicu Sebe, Ji-Rong Wen, and Zhicheng Dou. Think with videos for agentic long-video understanding, 2025. 3, 6, 7 [43] Huaying Yuan, Zheng Liu, Junjie Zhou, Ji-Rong Wen, and Zhicheng Dou. Videodeepresearch: Long video understanding with agentic tool using. arXiv preprint arXiv:2506.10821 , 2025. 1, 2 [44] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang. Easy-tool: Enhancing llm-based agents with concise tool instruc-tion. arXiv preprint arXiv:2401.06201 , 2024. 2 [45] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video un-derstanding. arXiv preprint arXiv:2306.02858 , 2023. 1 [46] Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, and Yansong Tang. Thinking with videos: Multimodal tool-augmented reinforcement learning for long video reasoning. 

arXiv preprint arXiv:2508.04416 , 2025. 1 [47] Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Deep video discovery: Agen-tic search with tool use for long-form video understanding. 

arXiv preprint arXiv:2505.18079 , 2025. 3 [48] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Zi-wei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713 , 2024. 1 [49] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yun-lin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scal-able lightweight infrastructure for fine-tuning, 2024. 6 [50] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deep-eyes: Incentivizing” thinking with images” via reinforce-ment learning. arXiv preprint arXiv:2505.14362 , 2025. 1, 3[51] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. Mlvu: Benchmarking multi-task long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 13691– 13701, 2025. 6 [52] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Wang HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zong-wei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment, 2023. 4