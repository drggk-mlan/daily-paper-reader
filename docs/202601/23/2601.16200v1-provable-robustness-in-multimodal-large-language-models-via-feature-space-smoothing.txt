Title: Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing

URL Source: https://arxiv.org/pdf/2601.16200v1

Published Time: Fri, 23 Jan 2026 02:06:29 GMT

Number of Pages: 14

Markdown Content:
# Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing 

## Song Xia Nanyang Technological University Meiwen Ding Nanyang Technological University Chenqi Kong Nanyang Technological University Wenhan Yang Peng Cheng Laboratory Xudong Jiang Nanyang Technological University 

## Abstract 

Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vul-nerable to adversarial perturbations that distort their fea-ture representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certi-fied robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under â„“2-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Puri-fier and Smoothness Mapper (PSM), a plug-and-play mod-ule that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, with-out requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the At-tack Success Rate (ASR) of various white-box attacks from nearly 90% to about 1%. 

## 1. Introduction 

The emergence of the Multimodal Large Language Mod-els (MLLMs), such as GPT-5, Gemini 2.5, and Claude 3.7 Sonnet, has fundamentally reshaped existing working paradigms and significantly advanced societal productiv-Adv input ð’™â€²           

> Feature
> encoder ð‘“ ð‘’
> LLM
> Adv targeted ð’™ ð’•
> min (ð’› â€², ð’› ð’• )
> Feature
> encoder ð‘“ ð‘’
> PSM
> â€¦â€¦
> Adv feature ð’›â€²
> Smoothed
> feature à·œð’›
> Describe this image
> Adv.
> response :A dog
> is licking a
> person's hand.
> Protected
> response : A
> close up of a
> panda bear's
> face.
> Cos à·œð’› ,ð’› â‰¥FCSB Feature -space
> smoothing
> Clean feature

Figure 1. Illustration of the FS-PSM, which guarantees that the cosine similarity of the adversarial and clean features extracted by MLLMâ€™s encoder is larger than FCSB for robust predictions. 

ity. Despite their remarkable capabilities across a broad spectrum of real-world tasks, these models still encounter critical safety challenges, such as adversarial vulnerabili-ties [5, 14, 23, 51]. Adversaries can manipulate predictions of the MLLMs to a malicious state by injecting subtle and imperceptible perturbations to the clean inputs, exploiting the modelsâ€™ insufficient local smoothness and uncontrolled Lipschitz continuity [4, 10, 12, 47]. Countermeasures towards those threats can be roughly classified into empirical defense and certified defense. Typ-ical empirical approaches include adversarial training [2, 27â€“29, 35, 37, 41, 45] and input purification [2, 27â€“29, 35, 37, 41, 45]. Despite their demonstrated empirical effective-ness, the multimodal nature of MLLMs also poses a great challenge for existing adversarial defense methods. Since these models accept heterogeneous inputs across diverse domains, ensure a robust encoder via adversarial training that can generalize to various scenarios is challenging and computationally expensive. Moreover, these approaches lack formal robustness guarantees and remain susceptible to stronger or adaptive adversaries [3, 38, 44]. In contrast, 1

> arXiv:2601.16200v1 [cs.LG] 22 Jan 2026

certified approaches aim to guarantee that the model returns a constant prediction result within a certain range, usually a 

â„“2 or â„“âˆž-norm constrained area [4, 11, 15, 34, 36, 43, 47]. However, most previous certified defense approaches [4, 11, 36, 42, 47] predominantly assume the prediction of one-dimensional outputs (e.g., class label), thereby limiting their applicability to more general tasks on MLLMs. To address the aforementioned challenges, we propose the Feature-space Smoothing (FS), a provable defense method that offers certified robustness for the feature rep-resentations of MLLMs. FS smooths the vanilla feature encoder of the MLLM, and the resulted smoothed encoder is guaranteed to maintain a provable lower bound on the cosine similarity between clean and adversarial representa-tions under â„“2-norm bounded attacks. Moreover, we demonstrate that the Feature Cosine Sim-ilarity Bound (FCSB) of the smoothed feature encoder is intrinsically determined by a Gaussian robustness score, which is defined to measure the prediction consistency of the vanilla feature extractor under Gaussian noise. Nonethe-less, without Gaussian noiseâ€“augmented training, the Gaus-sian robustness of these MLLMs remains limited, leading to a correspondingly suboptimal FCSB. To address this and avoid the heavy computational cost of fine-tuning MLLMs, we propose a plug-and-play Purifier and Smoothness Map-per (PSM) that effectively enhance the Gaussian robust-ness score for the vanilla feature encoder in a training-free way. Specifically, the purifier operates prior to feature ex-traction and is trained to denoise Gaussian perturbations and maximize the Gaussian robustness score. Meanwhile, the smoothness mapper performs post-extraction refinement and is trained to preserve the feature distribution while fur-ther enhancing this score without fine-tuning the encoder. These two components of PSM work synergistically to en-hance the certified robustness of MLLMs under FS. To implement our PSM, we employ a pre-trained guided diffusion model as the Gaussian noise purifier and design a noise-aware residual network as the smoothness mapper. The PSM is optimized using the proposed utilityâ€“robustness loss and trained on data from diverse visual domains to en-hance the Gaussian robustness while preserving the feature utility for the encoder. To comprehensively assess the per-formance, we evaluate our method against state-of-the-art (SOTA) adversarial attacks tailored for MLLMs under the white-box setting and compare it with strong adversarial training defense. Experimental results demonstrate that our FS with PSM enhanced provides strong protection for var-ious MLLMs under diverse downstream tasks. Overall, the main contributions of this work are: â€¢ We propose the Feature-space Smoothing (FS) to turn any feature encoder of MLLMs into a smoothed version, and theoretically prove that the smoothed encoder maintains a certified lower bound on the feature cosine similarity between clean and adversarial representations. â€¢ We propose a plug-and-play Purifier and Smoothness Mapper (PSM) that effectively enhances the value of the FCSB of the smoothed encoder in a training-free manner. â€¢ We conduct extensive experiments demonstrating that in-tegrating our proposed FS-PSM greatly enhances the ad-versarial performance of various MLLMs and dramati-cally reduces the ASR under various white-box attacks. 

## 2. Related Work 

Adversarial attacks on MLLMs. While MLLMs continue to achieve remarkable performance across diverse applica-tions, extensive works [5, 14, 23, 31, 39, 48â€“51] have ex-posed their adversarial vulnerabilities, raising serious safety concerns. Early work, such as AttackVLM [51], explores transferable attacks by disrupting the feature representa-tions of CLIP [33] and BLIP [19], showing strong adver-sarial transferability among open-source models but lim-ited effectiveness against closed-source commercial sys-tems. More recent approaches, such as M-Attack [23] and FOA-Attack [14], further advance this direction by leverag-ing multi-extractor ensembles and feature-space alignment, achieving over 90% targeted attack success rates on image-captioning tasks against powerful closed-sourced commer-cial MLLMs ( e.g ., ChatGPT-4o). This highlights an ur-gent need for trustworthy defenses that provide effective and provable protection for MLLMs. 

Adversarial defense on MLLMs. Adversarial defense methods can be broadly classified into empirical and prov-able approaches. Empirical defenses for MLLMs mainly in-clude adversarial training [2, 27â€“29, 35, 37, 41, 45], which enhances robustness by augmenting training data with ad-versarial examples, and input purification [17, 22, 30], which employs generative mechanisms such as diffusion models or autoencoders to recover clean inputs prior to inference. Recent studies [28, 29, 37] have revealed that utilization of adversarially trained CLIP-feature encoders can enhance adversarial robustness for MLLMs. However, these methods lack formal robustness guarantees and re-main vulnerable to adaptive and unseen threats [3, 38, 44]. Moreover, adversarial training demands costly retraining and often leads to degradation in clean performance. Alternatively, certified defenses aim to provide mathe-matically provable robustness guarantees. The use of Gaus-sian smoothing for certified robustness was initially intro-duced for classification models [4, 16, 18], yet its theo-retical formulation is restricted to one-dimensional outputs, limiting its applicability to tasks such as auto-regression or multimodal generation. To overcome these limitations and ensure trustworthy protection for MLLMs, we propose the Feature-space Smoothing (FS), a defense mechanism that establishes provable adversarial robustness for MLLMs. 23. Feature Space Smoothing 

3.1. Preliminary 

Let F denote a general deep learning model consisting of a feature extractor fe : x â†’ z that maps the input x to a feature representation z, and a predictor fd : z â†’ y that produces the final output y. Let L denote the general loss function (e.g., cross-entropy) that measures the discrepancy between the modelâ€™s output and the ground truth. 

Adversarial attacks: Let BÏµ(x) = 

n

xâ€² : âˆ¥xâ€² âˆ’ xâˆ¥p â‰¤ Ïµ

o

be an â„“p-norm ball centered at the input x, where Ïµ is a pre-defined perturbation bound. For each input x, the ad-versarial attacks aim to find an adversarial input xâ€² = x + Î´

that misleads the model by solving: 

max  

> x+Î´âˆˆB Ïµ(x)

L (F (x) , F (x + Î´)) . (1) 

Adversarial effects on feature representations: While adversarial attacks primarily aim to alter the modelâ€™s pre-dictions, numerous studies [8, 13, 14, 20, 21, 23, 32, 40, 46] have shown that successful attacks typically induce sub-stantial distortions in the modelâ€™s feature representations. Let xâ€² denote the adversarial example with adversarial fea-ture zâ€². Let zt be the adversarial targeted feature with ma-licious semantic meaning. The attack generally leads to 

max L (zâ€², z) for untargeted attacks and min L (zâ€², zt) for targeted attacks. Thus, ensuring a robust feature encoder that min L (zâ€², z) is crucial for the trustworthy prediction. 

Randomized smoothing: Consider a k classes classifica-tion problem with the input x âˆˆ Rd and the label y âˆˆ Y =

{c1, . . . , c k}. Randomized Smoothing (RS) first corrupts each input x by adding the Gaussian noise Îµ âˆ¼ N (0 , Ïƒ 2I).Then it turns an arbitrary base classifier F into a smoothed version Ë†F that possesses â„“2 certified robustness guarantees. The smoothed classifier Ë†F returns whichever the class the base classifier F is most likely to return among the distri-bution x + Îµ âˆ¼ N (x, Ïƒ 2I), which is: 

Ë†F(x) = arg max 

> câˆˆY

P(F(x + Îµ) = c). (2) RS then guarantees a certified radius R for this smoothed classifier Ë†F. For any perturbation Î´ satisfying âˆ¥Î´âˆ¥2 â‰¤ R ,the smoothed classifier is guaranteed to return a robust pre-diction that makes Ë†F(x + Î´) = F (x).

Limitations for RS. While previous RS provides effective certified protection for classification models, it suffers from two limitations. First, the theoretical framework of RS in-herently restricts its certification to classification tasks ( e.g .possibility of input x belongs to a certain class c). Second, estimating P(F(x + Îµ) = c) in Equation 2, incurs substan-tial computational overhead, since each estimation requires multiple forward passes through the entire model. 

3.2. Certified Bound via Feature Space Smoothing 

Considering the limitations inherent in the RS, we intro-duce the Feature-space Smoothing (FS). By turning any fea-ture encoder fe into a smoothed version Ë†fe, FS theoretically guarantees that Ë†fe maintains a certified lower bound on the cosine similarity between clean and adversarial representa-tions under â„“2-norm constrained perturbations. 

Smoothed feature encoder. For any feature encoder fe :

x â†’ z, where z is the representation normalized into the l2

unit sphere, FS defines the smoothed encoder Ë†fe(x) as: 

Ë†fe(x) = EÎµâˆ¼N (0 ,I )[fe(x + Îµ)] = 1(2 Ï€)d/ 2

Z

> Rd

fe(x + Îµ) exp  âˆ’ 12 âˆ¥Îµâˆ¥2 dÎµ, (3) where I denotes the d Ã— d identity and âˆ¥ Â· âˆ¥ is the Euclidean norm. Îµ âˆ¼ N (0 , I ) denotes Gaussian noise with zero mean and standard deviation one. The smoothed feature encoder outputs the expectation of feature representations over the Gaussian distribution N (x, I ).

Gaussian robustness score. Define Sxt (x) as the score function that evaluates feature discrepancy between an input 

x and a targeted example xt, which is: 

Sxt (x) = 12



1 + Cos  fe(x), f e(xt)

(4) where Cos (Â·, Â·) denotes the cosine similarity and Sxt (x) âˆˆ

[0 , 1] . The Gaussian robustness score Ë†S(x) is defined as: 

Ë†S(x) = E 

> Îµâˆ¼N (0 ,I )

Sx(x + Îµ)

= 12



1 + E 

> Îµâˆ¼N (0 ,I )

Cos  fe(x + Îµ), f e(x) 

, (5) The score Ë†S(x) evaluates the expected cosine similarity be-tween the feature representation of a Gaussian-perturbed in-put x + Îµ and that of the clean input x, characterizing the Gaussian robustness of the vanilla feature encoder fe(x).Notably, we prove that this score Ë†S(x) preserves a good Lipschitz property, which serves as a theoretical foundation for proving the certified robustness for Ë†fe(x).

Lemma 1 (Lipschitz property for the Gaussian robust-ness score ). Let Î¦( a) = 1âˆš2Ï€

R a 

> âˆ’âˆž

exp( âˆ’ 12 s2)ds be a stan-dard Gaussian cumulative distribution function and Î¦âˆ’1 be its inverse. For any feature encoder fe : x â†’ z, the map-ping x â†’ Î¦âˆ’1( Ë†S(x)) is 1 âˆ’ Lipschitz .

The proof of Lemma 1 is in the supplementary ma-terial, Section S.1. It implies that the mapping from x

to Î¦âˆ’1( Ë†S(x)) exhibits strong adversarial robustness, as it satisfies a 1-Lipschitz constraint. Generally, Ë†S(x) mea-sures the Gaussian robustness of the vanilla feature encoder 

fe(x). We then prove that this score Ë†S(x) fundamentally determines the value of the certified robustness bound of its smoothed encoder Ë†fe(x). The following Theorem es-tablishes an explicit relationship between this score and the lower bound on the adversarial feature cosine similarity. 

Theorem 1 (Certified lower bound on the adversarial feature cosine similarity ). For any feature encoder fe and its smoothed version Ë†fe, let x and xâ€² be clean and adversar-ial inputs with âˆ¥xâ€² âˆ’ xâˆ¥ â‰¤ Ïµ. The cosine similarity between 

3Clean input ð’™  Gaussian noise  

> ðœº ~ð’© (0,ðœŽ 2ð¼ )

+ Visual feature             

> encoder ð‘“ ð‘’
> Noise purifier ð’«
> Noised inputs ð’™ +ðœº Purified inputs ð’« (ð’™ +ðœº )
> LLM
> Residual smoother
> mapper â„³
> â€¦
> â€¦
> â€¦
> ð‘“ ð‘’ (ð’™ )à·¤ð’› ð’Ž
> ðŸŽ
> Robustness
> score

Æ¸ ð‘  (ð‘¥ )

> â„’â„³â„’ð’«
> Frozen parameters
> Trainable parameters
> Forward calculation
> Gradient backward

â€¦

> ð”¼

+      

> à·¤ð’› ð’Ž
> ð’ Figure 2. The training framework of the PSM. The purifier performs pre-processing and the smoothness mapper refines post-extracted features to enhance the Gaussian robustness. Parameters of MLLMs are frozen, and the purifier and mapper are optimized with LPand
> LM. For evaluation, the input xis replaced with adversarial input xâ€²and the forward calculation marked by orange color will be removed.

the adversarial feature Ë†fe(xâ€²) and clean feature fe(x) sat-isfies: Cos( Ë†fe(xâ€²), f e(x)) â‰¥ 2Î¦ 



Î¦âˆ’1( Ë†S(x)) âˆ’ Ïµ



âˆ’ 1.

Denote 2Î¦ 



Î¦âˆ’1( Ë†S(x)) âˆ’ Ïµ



âˆ’ 1 as the Feature Cosine Similarity Bound (FCSB). Theorem 1 reveals that: â€¢ By FS, we can turn any given feature encoder fe into a smoothed version Ë†fe that maintains a FCSB between the adversarial and clean feature representations. â€¢ By maximizing the robustness score Ë†S(x) of the given feature encoder fe, we can effectively enhance the value of FCSB derived on its smoothed version Ë†fe.The proof of Theorem 1 is in the supplementary material, Section S.1. 

Corollary 1 (Certified radius R for adversarial cosine similarity â‰¥ 0.5 ). Let x be the clean input, and xâ€² be the adversarial input. Then Cos( Ë†fe(xâ€²), f e(x)) â‰¥ 0.5, for all 

xâ€² with âˆ¥xâ€² âˆ’ xâˆ¥2 â‰¤ R , where: 

R = Î¦ âˆ’1  Ë†S(x) âˆ’ Î¦âˆ’1(0 .75) . (6) Building upon Theorem 1, Corollary 1 establishes a cer-tified radius R for the smoothed feature encoder. For any adversarial perturbation satisfying âˆ¥Î´âˆ¥2 â‰¤ R , the FCSB of the smoothed encoder is guaranteed to remain above 0.5. 

Why feature-space smoothing for MLLMs? Unlike pre-vious smoothing methods, performing smoothing in the fea-ture space offers several advantages: â€¢ Efficiency : Compared to the previous RS that smooths the entire model F, smoothing the feature encoder fe,which is substantially lighter than the whole MLLM F,significantly reduces the computational time of multiple forward calculations required by smoothing. â€¢ Generality : The FS provides certified robustness at the feature representation level, making it applicable to var-ious downstream tasks ( e.g . image captioning, classifica-tion and visual question answering). â€¢ Effectiveness : Since feature representations play a criti-cal role in the final prediction, guaranteeing a trustworthy feature representation effectively improves the prediction reliability and robustness under adversarial attacks. 

## 4. Purifier and Smoothness Mapper 

Theorem 1 and Corollary 1 reveal an intriguing robust-ness property of the smoothed feature encoder Ë†fe. How-ever, MLLMsâ€™ feature encoders fe generally exhibits lim-ited Gaussian robustness, which restricts the value of FCSB derived on Ë†fe. One solution is to estimate the smoothness score Ë†S(x) through Monte Carlo sampling and training fe

to maximize this score via gradient backpropagation. How-ever, this could reduce the adaptability and practicality of our FS protection, as fine-tuning and re-aligning the LLM with multi-modal feature encoders is highly complex and costly . This thereby motivates us to propose the Purifier and Smoothness Mapper (PSM), a plug-and-play module that can be seamlessly integrated with MLLMs to enhance its Gaussian robustness score Ë†S(x).The training framework of our proposed PSM is shown in Figure 2. The purifier P operates before feature extrac-tion to denoise the Gaussian perturbations, and the smooth-ness mapper M performs post-extraction to do the feature refinement. Those two modules work together to enhance the Gaussian robustness of the given feature encoder. 

4.1. Noise purifier 

To denoise the Gaussian noise, P is trained to minimize the reconstruction loss lmse , defined as: 

lmse = E  

> xâˆ¼D ,Îµâˆ¼N (0 ,Ïƒ 2I)

âˆ¥x âˆ’ P (x + Îµ)âˆ¥, (7) where D represents the data distribution. Meanwhile, to further enhance robustness score after plugging P, we also introduce a robustness loss lP

> rb

, defined as: 4lP 

> rb

= E  

> xâˆ¼D ,Îµâˆ¼N (0 ,Ïƒ 2I)

Cos  fe(P(x + Îµ)) , f e(x) , (8) which encourages feature consistency between the puri-fied and clean representations. Practically, we adopt the ImageNet-pretrained guided-diffusion [7] as our purifier P

and perform one-step diffusion for denoising in our main experiments. We then fine-tune this model on the dataset D

using the purifier loss LP , which is: 

LP = ldiff + Î»1lP 

> rb

+ Î»2lmse , (9) where ldiff denotes the original diffusion loss, and Î»1 and 

Î»2 are the weighting coefficients. More details of the fine-tuning process on P can be found in the supplementary ma-terial, Section S.2. 

4.2. Residual smoothness mapper 

For the residual smoothness mapper M, we utilize a noise-aware residual module to enhance feature robustness while preserving its statistical distribution. The main process of this mapper can be formulated as: 

Ëœzm = Ëœ z + M( Ëœ z, Ïƒ ) = Ëœ z +

> kâˆ’1

X

> i=0

mi ( Ëœ zi, Ïƒ ) , (10) where Ëœz = fe(P(x + Îµ)) denotes the purified feature repre-sentations and Ëœzi+1 = mi( Ëœ zi, Ïƒ ) is the intermediate output with Ëœz0 = Ëœ z. Ïƒ is the noise strength that adaptively con-trols the output magnitude of the mapper. k is the number of blocks ( k = 3 in our experiments unless otherwise spec-ified), and each block mi(Â·) contains multi-head attention, depthwise convolution, and MLP branches to refine the pu-rified representation. To enhance the Gaussian robustness of the refined representation, we introduce the mapper ro-bustness loss lM 

> rb

, defined as: 

lM 

> rb

= E  

> xâˆ¼D ,Îµâˆ¼N (0 ,Ïƒ 2I)

Cos   Ëœzm, f e(x) , (11) which encourages feature consistency between the refined and clean representations. Meanwhile, to ensure that the refined feature preserves the statistical characteristics of the clean feature, we introduce two regularization terms: the identical loss lid and the statistical loss lstats , defined as: 

ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³

lstats = E 

> xâˆ¼D ,
> Îµâˆ¼N (0 ,Ïƒ 2I)

1

D

> D

X

> d=1

h

(Î¼(d)Ëœzm âˆ’ Î¼(d) 

> z

)2 + ( Ïƒ(d)Ëœzm âˆ’ Ïƒ(d) 

> z

)2i

,lid = E

> xâˆ¼D

âˆ¥M ( Ëœ z, 0) âˆ¥22.

(12) where Ëœzm, z âˆˆ RBÃ—LÃ—D with bath size B, token number 

L and feature dimension D. The lstats enforces consistency between the element-wise mean Î¼d and standard deviation 

Ïƒd of two representations, thereby preserving the statistical characteristics. Meanwhile, the identity loss lid constrains the mapping network M when the noise strength Ïƒ = 0 ,promoting stability and preventing undesired distortions on clean inputs. The overall training loss LM is defined as: 

Algorithm 1 Training algorithm of PSM  

> 1:

Input: dataset D, feature encoder fe, purifier P, mapper M,sampling number n0, noise standard deviation Ïƒ, and loss weights Î»1, Î» 2, Î» 3, Î» 4. 

> 2:

Output: trained purifier P and mapper M. 

> 3:

// Stage 1: Fine-tune the purifier P 

> 4:

for each batch x âˆ¼ D do  

> 5:

z = fe(x) 

> 6:

for i = 1 , . . . , n 0 do  

> 7:

Sample Îµi âˆ¼ N (0 , Ïƒ 2I) 

> 8:

Ëœxi = P(x + Îµi), and Ëœzi = fe( Ëœ xi) 

> 9:

Compute lP 

> rb

and lmse using Equations 7, 8.  

> 10:

Update P by gradient descent on âˆ‡P LP 

> 11:

// Stage 2: Train the smoothness mapper M 

> 12:

for each batch x âˆ¼ D do  

> 13:

z = fe(x) 

> 14:

for i = 1 , . . . , n 0 do  

> 15:

Sample Îµi âˆ¼ N (0 , Ïƒ 2I) 

> 16:

Ëœxi = P(x + Îµi), and Ëœzim = fe( Ëœ xi) + M( Ëœ zi, Ïƒ ) 

> 17:

Compute lM 

> rb

, lid and lstats using Equations 11, 12.  

> 18:

Update M by gradient descent on âˆ‡MLM

LM = lM 

> rb

+ Î»3lstats + Î»4lid , (13) where Î»3 and Î»4 are the weighting coefficients. More de-tails of the training process on the residual smoothness map-per can be found in the supplementary material, Section S.2. 

4.3. Further discussion on PSM 

Certified robustness for the encoder fe with PSM. Let 

f â€² 

> e

denote the feature encoder integrated with the proposed PSM. Then the forward process can be formulated as: 

f â€²

> e

(x + Îµ) = fe(P(x + Îµ)) + M( Ëœ z, Ïƒ ). (14) Under this condition, the smoothed feature encoder and smoothness score are defined as: 

Ë†f â€²

> e

(x) = 1(2 Ï€)d/ 2

Z

> Rd

f â€²

> e

(x + Îµ) exp  âˆ’ 12 âˆ¥Îµâˆ¥2 dÎµ,

Ë†Sâ€²(x) = 12



1 + E 

> Îµâˆ¼N (0 ,Ïƒ 2I)

Cos  f â€²

> e

(x + Îµ), f e(x) 

,

(15) Where Ë†Sâ€²(x) âˆˆ [0 , 1] . We can prove that the Lipschitz property derived in Lemma 1 still holds, and the theoreti-cal bound in Section 3 remains valid. Then, utilizing The-orem 1 and Corollary 1, we can derive the certified lower bound on Cos( Ë†f â€²

> e

(xâ€²), f e(x)) for any adversarial input xâ€².

Training algorithm and dataset construction. The train-ing procedure of PSM is summarized in Algorithm 1, where the purifier P and the smoothness mapper M are trained sequentially via a two-stage manner. The expectation over Gaussian perturbations is approximated by Monte Carlo sampling with n0 samples drawn from N (0 , Ïƒ 2I), where we set n0 = 8 in our training to balance efficiency and es-timation accuracy. Notably, all modules can be trained in a 5Table 1. Experimental results on adversarial robustness of different defense methods and MLLMs on image captioning tasks. Values in parentheses denote the average textual similarity measured by GPTScore. The overall best results are shown in bold , and the best results without smoothing are underlined, highlighting the significant performance gain introduced by our proposed FS-PSM.                                                                                                                              

> Model Method M-Attack [23] FOA [14] AttackVLM [51]
> FCS â†‘ACC â†‘ASR â†“FCS â†‘ACC â†‘ASR â†“FCS â†‘ACC â†‘ASR â†“
> LLaVA-1.5-7B Org. 0.385 1% (0.06) 93% (0.54) 0.388 1% (0.049) 94% (0.578) 0.430 3% (0.089) 88% (0.490) Smoothed org. 0.650 82% (0.694) 2% (0.028) 0.652 87% (0.735) 1% (0.021) 0.660 89%(0.727) 2% (0.018)
> FARE [37] 0.588 44% (0.409) 24% (0.097) 0.504 19% (0.197) 51% (0.222) 0.499 18%(0.190) 44%(0.221) Smoothed FARE 0.716 59% (0.501) 14% (0.069) 0.656 35% (0.327) 29% (0.153) 0.729 60% (0.503) 11% (0.061) TeCoA [29] 0.720 51% (0.458) 16% (0.081) 0.674 21% (0.236) 37% (0.187) 0.606 17% (0.179) 43% (0.218) Smoothed TeCoA 0.805 59% (0.498) 14% (0.056) 0.789 39% (0.364) 20% (0.094) 0.755 42%(0.358) 18% (0.104) Open Flamingo -9B Org. 0.351 1%(0.089) 86%(0.583) 0.347 1%(0.091) 87%(0.569) 0.442 16%0.222) 59%(0.379) Smoothed org. 0.673 73%(0.655) 1%(0.063) 0.672 69%(0.623) 0%(0.061) 0.703 73%(0.687) 0%(0.055)
> FARE [37] 0.588 36%(0.407) 17%(0.145) 0.504 20%(0.249) 40%(0.240) 0.499 15%(0.243) 40%(0.247) Smoothed FARE 0.716 55%(0.529) 9%(0.088) 0.656 40%(0.414) 17%(0.125) 0.729 50%(0.488) 4%(0.089) TeCoA [29] 0.720 51%(0.472) 13%(0.116) 0.674 21%(0.240) 30%(0.210) 0.606 19%(0.244) 34%(0.225) Smoothed TeCoA 0.805 56%(0.518) 5%(0.090) 0.789 50%(0.454) 8%(0.095) 0.755 38%(0.398) 12%(0.126)

self-supervised fashion, without relying on task-specific la-bels. Thus, to promote generalization across diverse visual domains and ensure compatibility with various MLLMsâ€™ downstream tasks, we collect the training dataset D com-prising 5,000 images from heterogeneous domains, includ-ing medical, cartoon, and natural images. 

Table 2. The average FCSB at different predetermined adversarial bounds Ïµ and the average certified radius R for FCSB â‰¥ 0.5.                                   

> Encoder ÏƒAvg. FCSB at different adv-bound ÏµAvg. R
> 0.125 0.25 0.375 0.50 0.75 CLIP-B16 0.25 0.828 0.623 0.313 -0.06 /0.31 CLIP-B16+ P&M0.975 0.920 0.789 0.553 /0.53 CLIP-B16 0.50 0.717 0.586 0.439 0.253 -0.132 0.33 CLIP-B16+ P&M0.970 0.945 0.907 0.846 0.649 0.89

Certified robustness bound of FS-PSM. To evaluate the effectiveness of the proposed PSM module, we adopt CLIP-B16 [33] as the vanilla feature encoder fe and assess the certified robustness bound of both the original smoothed en-coder Ë†fe and its PSM-enhanced counterpart Ë†f â€² 

> e

under the FS. We report both the certified FCSB under different adversar-ial constraints Ïµ and the certified radius R for FCSB â‰¥ 0.5.The results are shown in Table 2. For implementation, we randomly sample 100 images from the ImageNet dataset [6] and approximate the Gaussian expectation using n = 1 ,000 

Monte Carlo samples. As shown in Table 2, integrating PSM consistently enhances both the certified FCSB across a range of perturbation magnitudes and the average certi-fied radius R, indicating substantial improvements in certi-fied robustness. To further assess the practical protection of our FS-PSM, we then conduct extensive experiments across multiple MLLMs and downstream tasks in the next sec-tion, evaluating their performance before and after smooth-ing under various strong white-box attacks. 

## 5. Experiments 

5.1. Experimental setup 

Evaluated models and tasks. Since the proposed FS re-quires access to the forward feature computation process of MLLMs, we primarily validate its effectiveness on open-sourced MLLMs, including LLaVA-V1.5-7B [24, 25] and OpenFlamingo9B [1]. We comprehensively assess the per-formance of plugging the FS-PSM under adversarial condi-tions across multiple downstream tasks, including: â€¢ Image captioning: Following the setup in [14, 23], we randomly take 100 images from the NIPS 2017 Adver-sarial Attacks and Defenses Competition dataset 1 and ask the model to caption the image. â€¢ Image Classification: We randomly sample 500 images from 10 classes in the ImageNet dataset [6] and ask the model to classify the input. â€¢ Visual Question Answering (VQA): We utilize 100 image-question pairs from the ScienceQA dataset [26]. Meanwhile, we also evaluate our FS-PSM on the original CLIP-L14 [33] on image classification for the generality. 

The threat model. To comprehensively assess the robust-ness under strong adversaries, we employ three SOTA ad-versarial attacks specifically designed for MLLMs, named AttackVLM [51], M-Attack [23], and FOA [14] using the white-box setting . All attacks are implemented following their original best configurations, with the adversarial per-turbation budget Ïµ set to âˆ¥Ïµâˆ¥âˆž = 16 /255 . We also consider a stronger bound with âˆ¥Ïµâˆ¥âˆž = 32 /255 for a stress test. 

Compared defense methods. As this work pioneers the research for MLLMsâ€™ certified robustness, we primarily compare our defense with adversarial training based ap-proaches, namely FARE [37] and TeCoA [29]. Both FARE and TeCoA adopt adversarial training to obtain robust fea-           

> 1https ://nips .cc /Conferences /2017 /CompetitionTrack

6Table 3. Experimental results on adversarial robustness of different defense methods and MLLMs on image classification tasks. The overall best results are shown in bold , and the best results without smoothing are underlined.                                                                                                                             

> Model Method M-Attack [23] FOA [14] AttackVLM [51]
> FCS â†‘ACC â†‘ASR â†“FCS â†‘ACC â†‘ASR â†“FCS â†‘ACC â†‘ASR â†“
> LLaVA-1.5-7B Org. 0.427 8.2% 78.2% 0.437 3.8 % 81.0 % 0.458 6.0% 78.4% Smoothed org. 0.681 90.4% 0.6% 0.697 88.8% 0.4% 0.700 88.4% 0.2%
> FARE [37] 0.574 55.4% 14.6% 0.521 24.8% 39.8% 0.508 25.8% 45.0% Smoothed FARE 0.859 75.6% 5.2% 0.712 72.0% 5.6% 0.699 74.0% 6.2% TeCoA [29] 0.731 67.4% 2% 0.626 29.0% 20.0% 0.592 33.6% 17.8% Smoothed TeCoA 0.819 74.4% 0.4% 0.727 57.2% 3.0% 0.711 57.4% 2.0% CLIP -L14 Org. 0.427 18.0% 37.2% 0.437 10.0 % 52.4 % 0.458 16.0% 48.4% Smoothed org. 0.681 92.4 %0.4% 0.697 92.0 %0.4% 0.700 92.0 %0.4%
> FARE [37] 0.574 70.4% 5.2% 0.521 42.0% 14.4% 0.508 35.6% 24.4% Smoothed FARE 0.859 85.2% 0.8% 0.712 81.6% 0% 0.699 82.4% 0% TeCoA [29] 0.731 81.2% 2.4% 0.626 39.2% 20.2% 0.592 35.8% 24.4% Smoothed TeCoA 0.819 88.4% 1.6% 0.727 76.0% 4.8% 0.711 73.2% 6.0%

ture encoders that can be directly integrated into models such as LLaVA-1.5-7B and OpenFlamingo-9B. To ensure a fair and consistent comparison, we obtain all adversari-ally trained feature encoders from their official repositories, without any modification. 

Implementation details. For practical inference efficiency, we set the number of samples to n0 = 4 for smoothing, which provides a favorable trade-off between robustness and runtime. We train three independent PSM modules, utilizing the vanilla feature encoders from models includ-ing LLaVA-1.5-7B, OpenFlamingo-9B, and CLIP-L14. To further assess cross-model generalization, we evaluate the PSM trained on a vanilla encoder by directly applying it to adversarially trained encoders FARE and TeCoA. For all feature encoders, we set hyperparameters Î»1, Î» 2, Î» 4 =0.25 , Î»3 = 100 , and Ïƒ = 0 .25 . In our tables, the term â€smoothedâ€ denotes the process of smoothing the encoder via FS and enhancing it with PSM. 

Evaluation metrics. We mainly report: the Feature Co-sine Similarity ( FCS ), the Accuracy ( ACC ), and the Attack Success Rate ( ASR ). Specifically, FCS measures the cosine similarity between clean and adversarial features extracted 

by the same feature encoder , reflecting its feature-space robustness. ACC denotes the proportion of correctly com-pleted tasks, while ASR indicates the proportion of cases where the model is successfully manipulated to produce the adversarially targeted outputs. For image classification and VQA tasks, ACC and ASR are determined by whether the MLLM outputs match the correct or adversarial targets. For image captioning, following [23], we adopt the LLM-as-a-judge protocol [52] to evaluate ACC and ASR. Specifically, we first generate the clean and adversarially targeted captions by feeding the clean and targeted in-puts into the vanilla MLLM ( e.g ., the original LLaVA). We then obtain the predicted caption by feeding the ad-versarially perturbed input into the tested model ( e.g ., Smoothed LLaVA). The textual similarity is computed us-ing GPTScore [9], where a task is considered successful if the GPTScore between the predicted and clean cap-tions is â‰¥ 0.5, and an attack is considered successful if the GPTScore between the predicted and adversarially tar-geted captions is â‰¥ 0.3.

5.2. Experimental results on different tasks 

Image captioning. The comparative results of different de-fense methods and MLLMs on the image captioning task are presented in Table 1. In this task, the adversaries aim to manipulate the MLLMs into producing the caption cor-responding to a completely unrelated and maliciously cho-sen target image. These results demonstrate that converting MLLMs into their smoothed variants via the proposed FS-PSM yields consistently strong robustness across diverse adversarial attacks, while empirical defenses exhibit a no-ticeable performance drop under stronger attacks. When adversaries vary from M-Attack to FOA, the accuracy of 

LLaVA with FARE drops from 44% to 19%, and that of TeCoA declines from 51% to 21% . Notably, utilizing FS-PSM achieves remarkable improvements, with the ac-curacy of LLaVA increasing from 1% to 87% and the ASR dropping from 94% to 1% under the strongest FOA attack. Furthermore, the results demonstrate that FS-PSM possesses strong cross-model generalization; directly inte-grating it into FARE and TeCoA without any fine-tuning consistently enhances their robustness. 

Image classification. The comparative results of differ-ent defense methods on the image classification are pre-sented in Table 3. The adversarial objective is to mislead the model into classifying an adversarial image into a ma-liciously targeted class that is semantically unrelated to the original one. For image classification with CLIP-L14, we train a one-layer classification head on top of its extracted features. The results indicate that FS-PSM provides consis-7Table 4. Experimental results on adversarial robustness of different defense methods and MLLMs on VQA tasks. The overall best results are shown in bold , and the best results without smoothing are underlined.                                                                       

> Model Method M-Attack [23] FOA [14] AttackVLM [51]
> FCS â†‘ACC â†‘ASR â†“FCS â†‘ACC â†‘ASR â†“FCS â†‘ACC â†‘ASR â†“
> LLaVA-1.5-7B Org. 0.398 31% 28% 0.383 22% 22% 0.474 25% 27% Smoothed org. 0.732 47% 0% 0.676 43% 0% 0.765 43% 1% FARE [37] 0.657 47% 5% 0.590 32% 0% 0.550 38% 7% Smoothed FARE 0.859 48% 1% 0.768 38% 0% 0.818 39% 0%
> TeCoA [29] 0.788 31% 2% 0.748 31% 2% 0.667 31% 1% Smoothed TeCoA 0.917 32% 1% 0.858 34% 1% 0.862 29% 1%

tent and substantial robustness improvements for classifica-tion tasks. Meanwhile, those adversarial training-based de-fenses exhibit a noticeable performance drop under strong FOA attacks. The accuracy of FARE with LLaVA drops from 55.4% to 24.8%, and that of TeCoA declines from 67.4% to 29.0% when adversaries vary from M-Attack to FOA. Integrating FS-PSM into the vanilla model achieves remarkable improvements, with the accuracy of LLaVA increasing from 3.8% to 88.0% and the ASR dropping from 81% to 0.4% under the strongest FOA attack. Mean-while, directly integrating the FS-PSM brings huge robust-ness gains for those adversarially trained feature encoders. 

VQA. The comparative results of different defense meth-ods on LLaVA-1.5-7B on the VQA are presented in Ta-ble 4. The performances of OpenFlamingo and CLIP are omitted, as these models are not directly applicable to the VQA task. In this setting, the adversarial objective is to mis-lead the model into selecting a wrong option, â€None of the aboveâ€ , for each imageâ€“question pair. This task presents a greater challenge for pure vision-based adversaries than the previous two, as MLLMs can often infer correct an-swers directly from textual cues without relying heavily on visual inputs. The results demonstrate that incorporating our method yields substantial performance improvements across all attack types, significantly enhancing prediction accuracy while reducing the attack success rate to nearly zero. More implementation details for the main results are presented in our supplementary material, section S.3. 

5.3. Further analysis on the main results 

Reasons for FARE and TeCoA performing poorly with high FCS. Although those encoders preserve high FCS with respect to their clean inputs, the adversarial training process inherently induces a distributional shift in the feature rep-resentations. Subsequent adversarial perturbations further amplify this shift, leading to more pronounced performance degradation. In addition, the limited diversity of adversar-ial training data restricts their feature generalization ability, amplifying the performance drop when evaluated on down-stream tasks with unseen or mismatched data distributions. 

Advantages of our proposed FS with PSM. The exper-imental results highlight three key advantages: (1) Effec-

Table 5. The ablation study. We report the average FCSB and the average R for FCSB â‰¥ 0.5 for certified robustness. We report the average accuracy and ASR for empirical robustness under FOA attack. The clean accuracy without attack is 92.4% .                                

> Encoder ÏƒAvg. FCSB at different ÏµAvg.
> RAcc ASR 0.25 0.50 0.75 CLIP-B16 /////1.6% 95.6% FS CLIP-B16 0.50 0.586 0.253 -0.132 0.33 42.4% 0.8% CLIP-B16+ M0.884 0.717 0.436 0.71 66.8% 0.4% CLIP-B16+ P&M0.970 0.907 0.846 0.89 91.6% 0.4%

tiveness : utilizing the proposed FS-PSM substantially en-hances the performance of various MLLMs under adversar-ial attacks; (2) Trustworthiness : compared with adversar-ial training, FS-PSM provides more stable and consistently stronger protection across a wide range of attacks; (3) Gen-erality : FS-PSM can be combined with other defenses to enhance the effectiveness without additional fine-tuning. 

5.4. Ablation study 

To rigorously assess the contribution of the proposed FS and each module of PSM, we adopt CLIP-B16 as the base feature encoder and evaluate the performance on the image classification task. We report both the theoretically certified robustness and the empirical performance under the strong FOA attack. The results are summarized in Table 5, where the first row indicates the performance of the vanilla CLIP-B16 model without utilizing the FS. The next three rows are the results of the smoothed CLIP-B16 with FS, smoothed CLIP-B16 with FS and mapper M, and smoothed CLIP-B16 with FS and PSM. The results demonstrate that enforc-ing smoothness in the feature space substantially enhances practical robustness against adversarial perturbations, rais-ing the accuracy from 1.6% to 91.6% and reducing the ASR from 95.6% to 0.4%. Meanwhile, each component makes a significant contribution to improving both the certified ra-dius R and adversarial robustness. 

More experimental results on: 1) the performance of us-ing different purifier (a lightweight U-Net denoiser), 2) the performance under a stronger adversarial bound âˆ¥Ïµâˆ¥âˆž =32 , and 3) efficiencyâ€“robustness trade-off under various n0,are provided in our supplementary material, section S.3. 86. Conclusion. 

This work pioneers the research on establishing the certi-fied robustness of MLLMs via a feature-space perspective. By introducing the Feature-space Smoothing (FS) frame-work, we show how to transform any feature encoder into a smoothed version that is equipped with a theoretical lower bound on the cosine similarity between clean and adver-sarial representations. Moreover, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that can be seamlessly integrated into existing MLLMs to amplify the certified robustness under FS without any fine-tuning. Extensive experiments demonstrate that the pro-posed FS-PSM not only provides strong theoretical robust-ness guarantees but also achieves superior empirical protec-tion against various attacks. 

## References 

[1] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390 , 2023. 6[2] Stephen Casper, Lennart Schulze, Oam Patel, and Dy-lan Hadfield-Menell. Defending against unforeseen fail-ure modes with latent adversarial training. arXiv preprint arXiv:2403.05030 , 2024. 1, 2 [3] Bin Chen, Jiali Yin, Shukai Chen, Bohao Chen, and Xi-meng Liu. An adaptive model ensemble adversarial attack for boosting adversarial transferability. In CVPR , 2023. 1, 2 [4] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In ICML ,2019. 1, 2 [5] Xuanming Cui, Alejandro Aparcedo, Young Kyun Jang, and Ser-Nam Lim. On the robustness of large multimodal models against image adversarial attacks. In CVPR , 2024. 1, 2 [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR , 2009. 6 [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS , 2021. 5 [8] Xinlong Ding, Jiansheng Chen, Hongwei Yu, Yu Shang, Yin-ing Qin, and Huimin Ma. Transferable adversarial attacks for object detection using object-aware significant feature distor-tion. In AAAI , 2024. 3 [9] Jinlan Fu, See Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. In Proceedings of the 2024 Conference of the North American Chapter of the Associa-tion for Computational Linguistics: Human Language Tech-nologies , 2024. 7 [10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 , 2014. 1 [11] Zhongkai Hao, Chengyang Ying, Yinpeng Dong, Hang Su, Jian Song, and Jun Zhu. Gsmooth: Certified robustness against semantic transformations via generalized random-ized smoothing. In ICML , 2022. 2 [12] Matthias Hein and Maksym Andriushchenko. Formal guar-antees on the robustness of a classifier against adversarial manipulation. NeurIPS , 2017. 1 [13] Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Be-longie, and Ser-Nam Lim. Enhancing adversarial example transferability with an intermediate level attack. In ICCV ,2019. 3 [14] Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, and Yang Liu. Adversarial attacks against closed-source mllms via fea-ture optimal alignment. NeurIPS , 2025. 1, 2, 3, 6, 7, 8, 13, 14 [15] Kazuya Kakizaki, Kazuto Fukuchi, and Jun Sakuma. Cer-tified defense for content based image retrieval. In CVPR ,2023. 2 [16] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to ad-versarial examples with differential privacy. In 2019 IEEE symposium on security and privacy (SP) , 2019. 2 [17] Chun Tong Lei, Hon Ming Yam, Zhongliang Guo, Yifei Qian, and Chun Pong Lau. Instant adversarial purification with adversarial consistency distillation. In CVPR , 2025. 2 [18] B Li, C Chen, W Wang, and L Carin. Second-order adver-sarial attack and certifiable robustness. arXiv preprint arXiv: 1809.03113 , 2018. 2 [19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML ,2023. 2 [20] Qizhang Li, Yiwen Guo, and Hao Chen. Yet another intermediate-level attack. In ECCV , 2020. 3 [21] Qizhang Li, Yiwen Guo, Wangmeng Zuo, and Hao Chen. Improving adversarial transferability via intermediate-level perturbation decay. NeurIPS , 2023. 3 [22] Xiao Li, Wenxuan Sun, Huanran Chen, Qiongxiu Li, Yingzhe He, Jie Shi, and Xiaolin Hu. ADBM: Adversarial diffusion bridge model for reliable adversarial purification. In ICLR , 2025. 2 [23] Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, and Zhiqiang Shen. A frustratingly simple yet highly effec-tive attack baseline: Over 90% success rate against the strong black-box models of gpt-4.5/4o/o1. In ICML 2025 Workshop on Reliable and Responsible Foundation Models , 2025. 1, 2, 3, 6, 7, 8, 13 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS , 2023. 6 [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR ,2024. 6 [26] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. NeurIPS ,2022. 6 

9[27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR , 2018. 1, 2 [28] Hashmat Shadab Malik, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar, Fahad Khan, and Salman Khan. Robust-llava: On the effectiveness of large-scale ro-bust image encoders for multi-modal large language models. 

arXiv preprint arXiv:2502.01576 , 2025. 2 [29] Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, and Carl Vondrick. Understanding zero-shot adversarial robust-ness for large-scale models. In ICLR , 2023. 1, 2, 6, 7, 8, 13, 14 [30] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Animashree Anandkumar. Diffusion models for adversarial purification. In ICML , 2022. 2 [31] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Hen-derson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models. In AAAI ,2024. 2 [32] Yixiang Qiu, Hao Fang, Hongyao Yu, Bin Chen, MeiKang Qiu, and Shu-Tao Xia. A closer look at gan priors: Exploit-ing intermediate features for enhanced model inversion at-tacks. In ECCV , 2024. 3 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-vision. In ICML , 2021. 2, 6 [34] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Cer-tified defenses against adversarial examples. In ICLR , 2018. 2[35] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, and Timothy A Mann. Data augmentation can improve robustness. NeurIPS , 2021. 1, 2 [36] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed clas-sifiers. NeurIPS , 2019. 2 [37] Christian Schlarmann, Naman Deep Singh, Francesco Croce, and Matthias Hein. Robust clip: unsupervised ad-versarial fine-tuning of vision embeddings for robust large vision-language models. In ICML , 2024. 1, 2, 6, 7, 8, 13, 14 [38] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial ex-ample defenses. NeurIPS , 2020. 1, 2 [39] Yubo Wang, Chaohu Liu, Yanqiu Qu, Haoyu Cao, Deqiang Jiang, and Linli Xu. Break the visual perception: Adversar-ial attacks targeting encoded visual tokens of large vision-language models. In ACM MM , 2024. 2 [40] Zhibo Wang, Hengchang Guo, Zhifei Zhang, Wenxin Liu, Zhan Qin, and Kui Ren. Feature importance-aware transfer-able adversarial attacks. In CVPR , 2021. 3 [41] Zeyu Wang, Xianhang Li, Hongru Zhu, and Cihang Xie. Re-visiting adversarial training at scale. In CVPR , 2024. 1, 2 [42] Zekai Wang, Zhengyu Zhou, and Weiwei Liu. Drf: Improv-ing certified robustness via distributional robustness frame-work. In AAAI , 2024. 2 [43] Eric Wong and Zico Kolter. Provable defenses against adver-sarial examples via the convex outer adversarial polytope. In 

ICML , 2018. 2 [44] Kaiwen Wu, Allen Wang, and Yaoliang Yu. Stronger and faster wasserstein adversarial attacks. In ICML , 2020. 1, 2 [45] Sophie Xhonneux, Alessandro Sordoni, Stephan GÂ¨ unnemann, Gauthier Gidel, and Leo Schwinn. Effi-cient adversarial training in llms with continuous attacks. 

NeurIPS , 2024. 1, 2 [46] Song Xia, Wenhan Yang, Yi Yu, Xun Lin, Henghui Ding, Lingyu Duan, and Xudong Jiang. Transferable adversarial attacks on sam and its downstream models. NeurIPS , 2024. 3[47] Song Xia, Yi Yu, Xudong Jiang, and Henghui Ding. Mitigat-ing the curse of dimensionality for certified robustness via dual randomized smoothing. In ICLR , 2024. 1, 2 [48] Peng Xie, Yequan Bie, Jianda Mao, Yangqiu Song, Yang Wang, Hao Chen, and Kani Chen. Chain of attack: On the robustness of vision-language models against transfer-based adversarial attacks. In CVPR , 2025. 2 [49] Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, and Jitao Sang. Adversarial prompt tuning for vision-language models. In ECCV , 2024. [50] Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Yunhao Chen, Jitao Sang, and Dit-Yan Yeung. Anyat-tack: Towards large-scale self-supervised adversarial attacks on vision-language models. In CVPR , 2025. [51] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongx-uan Li, Ngai-Man Man Cheung, and Min Lin. On evaluat-ing adversarial robustness of large vision-language models. 

NeurIPS , 2023. 1, 2, 6, 7, 8, 13 [52] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. NeurIPS , 2023. 7 

10 Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing 

# Supplementary Material 

## S.7. Proof of core theorem 

S.7.1. Proof of lemma1 

Note that: 

âˆ‡Î¦âˆ’1( Ë†Sxc (x)) = âˆ‡ Ë†Sxc (x)Î¦â€² Î¦âˆ’1( Ë†Sxc (x))  . (S.16) Denote p = Ë†Sxc (x), we can get that: 

Î¦â€² Î¦âˆ’1( Ë†Sxc (x))  = 1

âˆš2Ï€ exp  âˆ’ 12 (Î¦ âˆ’1(p)) 2 (S.17) Thus, we need to prove that for any unit direction u.

u Â· âˆ‡ Ë†Sxc (x) â‰¤ 1

âˆš2Ï€ exp  âˆ’ 12 (Î¦ âˆ’1(p)) 2. (S.18) For the left-hand side, we can get: 

âˆ‡ Ë†Sxc (x) = 1(2 Ï€)n/ 2

Z

> Rn

Sxc (t) ( xâˆ’t) exp  âˆ’ 12 âˆ¥xâˆ’tâˆ¥2 dt,

(S.19) which can also be rewritten as follows: 

E 

> Xâˆ¼N (0 ,I )

Sxc (x + X)X Â· u.

We can now claim that the supremum of the above quantity over all encoders f : x â†’ z, subject to the constraint that 

p = E[Sxc (x + X)] , is equal to: 

E(XÂ·u) 1 {XÂ·u â‰¥ âˆ’ Î¦âˆ’1(p)} = 1

âˆš2Ï€ exp 



âˆ’ 12

 Î¦âˆ’1(p)2

,

(S.20) which concludes the proof of Lemma 1. 

S.7.2. Proof of Theorem1 

From Lemma 1, we can get: 

âˆ¥x âˆ’ xâ€²âˆ¥ â‰¥ Î¦âˆ’1( Ë†Sx(x)) âˆ’ Î¦âˆ’1( Ë†Sx(xâ€²)) . (S.21) As âˆ¥x âˆ’ xâ€²âˆ¥ â‰¤ Ïµ, we can get: 

Î¦âˆ’1( Ë†Sx(xâ€²)) â‰¥ Î¦âˆ’1( Ë†Sx(x)) âˆ’ Ïµ. (S.22) Meanwhile, as fe(x) is a unit vector, using Equation 5 in the main paper, we can get that: 

Ë†S(x) = 12



1 + E[fe(x + Îµ)] , f e(x)



= 12



1 + âŸ¨ Ë†fe(x), f e(x)âŸ©



. (S.23) where âŸ¨Â· , Â·âŸ© is the inner product. Hence, we can get: 

âŸ¨ Ë†fe(x), f e(x)âŸ© = 2 Ë†S(x) âˆ’ 1. (S.24) As Ë†fe(xâ€²) â‰¤ 1 and âˆ¥fe(x)âˆ¥ = 1 , we can get: 

Cos   Ë†f (z), f e(x) = âŸ¨ Ë†f (z), f e(x)âŸ©âˆ¥ Ë†f (z)âˆ¥ (S.25) 

â‰¥ âŸ¨ Ë†f (z), f e(x)âŸ©

By the monotonicity of Î¦,

Ë†Sx(xâ€²) â‰¥ Î¦



Î¦âˆ’1  Ë†S(x) âˆ’ Ïµ



. (S.26) Combining Equations S.24, S.25, S.26, we thereby can de-rive that: 

Cos   Ë†f (xâ€²), f e(x) â‰¥ 2 Î¦ 



Î¦âˆ’1  Ë†S(x) âˆ’ Ïµ



âˆ’ 1, (S.27) which concludes the proof. For noise variance Ïƒ2, the same proof gives the bound with Ïµ replaced by Ïµ/Ïƒ .

## S.8. Training details of PSM 

S.8.1. Details of fine-tuning noise purifier 

The Purifier P is optimized in a self-supervised manner us-ing the objective Lp defined in Equation 9. We employ the ImageNet-pretrained 256 Ã— 256 unconditional diffusion model 2 as the backbone of P. All input images are resized to 256 Ã— 256 .To perform denoising, we instantiate a diffusion process consisting of 1,000 timesteps with a linear noise scheduler. Following common practices, the rescaled learned noise variance (i.e., â€rescale learned sigmaâ€) is disabled to pre-vent instability during fine-tuning. We fine-tune the model for a total of 12,000 optimization steps using the AdamW optimizer, with an initial learning rate of 5 Ã— 10 âˆ’5. An ex-ponential moving average (EMA) with a decay rate of 0.99 is maintained throughout training to stabilize updates and improve the effective generalization of the learned purifier. 

S.8.2. Details of training residual mapper 

The structure of the proposed residual smoothness mapper 

M is illustrated in Figure S.3. Each block is designed to re-fine the purified feature representation while preserving its 

> 2https : / / github . com / openai / guided - diffusion ? tab=readme-ov-file

11 Original      

> feature à·¤ð’›
> Layer norm
> +FiLM (ðœŽ )
> Multi -head
> Attention
> MLP branch
> Depth -wise
> Convolution
> Noise -aware
> residual
> Refined
> feature à·¤ð’› ð’Ž
> 3 x
> Block ð’Ž ð’Š
> Noise
> strength ðœŽ
> +

Figure S.3. Structure of the residual smoothness mapper. 

statistical structure and ensuring that the refinement mag-nitude adapts smoothly to the injected noise magnitude Ïƒ.A single block consists of: (1) noise-aware LayerNorm-FiLM( Ïƒ) module, (2) multi-head attention, (3) a channel-wise MLP branch, (4) a depthwise convolution, and (5) a noise-aware residual module. 

LayerNorm and FiLM( Ïƒ). Each block begins by normal-izing the input feature and injecting noise-level condition-ing: 

Ëœzi = LN( Ëœ zi), [Î³(Ïƒ), Î² (Ïƒ)] = FiLM( Ïƒ), (S.28) where the FiLM module takes the scalar noise strength 

Ïƒ and outputs channel-wise affine parameters through a lightweight MLP: 

FiLM( Ïƒ) = W2 Ï•(W1Ïƒ) âˆˆ R2D, (S.29) with Ï• denoting a GELU activation. This modulation injects explicit noise awareness into each block: the transformation smoothly diminishes as Ïƒ â†’ 0, ensuring that the mapper leaves clean features nearly unchanged. 

Main computation structure. Each mi(Â·) contains three parallel refinement pathways: â€¢ Multi-head attention: Applied only in the first block, the lightweight attention layer captures long-range structural dependencies, producing global-context features hattn .â€¢ MLP branch: A two-layer feed-forward network with GELU activation produces channel-wise refinement hmlp .â€¢ Depthwise convolution: A depthwise 1D convolution captures local continuity in the feature sequence and con-tributes hconv .The outputs are fused as: 

hi = hattn + hmlp + 0 .5 hconv , (S.30) where the conv branch is down-weighted following design practices for local/global feature fusion. 

Noise-aware residual. We first decompose hi into a unit direction vector by vi = hi \ âˆ¥ hiâˆ¥. To ensure that the re-finement adapts smoothly to the noise level, we modulate its magnitude through two learnable functions of Ïƒ:

Î±i = softplus(MLP Î±([ Ïƒ, log Ïƒ])) ,ÏƒÎ²i = Ïƒsoftplus(MLP Î² (Ïƒ)) . (S.31) Here, Î±i controls the amplitude of the refinement, while Î²i

modulates the exponent of the noise term, enabling nonlin-ear noiseâ€“feature interactions. The residual update is then computed as 

âˆ†zi = (1 + Î³(Ïƒ))  vi Â· Î±iÏƒÎ²i  + Î²(Ïƒ)âŠ™Scale i, (S.32) where Î³(Ïƒ) and Î²(Ïƒ) are FiLM-generated per-channel affine parameters, and Scale i is a learnable per-channel damping factor initialized to 5 Ã— 10 âˆ’4 for stability. The next feature is then updated via 

Ëœzi+1 = Ëœ zi + âˆ† zi. (S.33) 

Overall effect of the mapper. The residual smoothness mapper thus provides the following benefits: â€¢ Noise-adaptive refinement : FiLM (Ïƒ) and ÏƒÎ² ensure the introduced modification decreases as Ïƒ â†’ 0, allowing the mapper to preserve the natural feature distribution while refining noisy features more aggressively. â€¢ Multi-scale feature enhancement : Attention, MLP, and depth-wise convolution jointly capture global structure, per-channel adaptation, and local smoothness. â€¢ Stable and expressive residual learning : Fixup-style initialization and Scale i ensure training stability even with multiple stacked residual blocks. Collectively, M significantly enhances the smoothness and robustness of purified representations while avoiding distri-butional drift. 

Training details. The mapper M is trained in a self-supervised manner using the objective Lm defined in Equa-tion 13. We train M for 8 epochs with a batch size of 16. The training process employs the AdamW optimizer with an initial learning rate of 2 Ã— 10 âˆ’4 and a cosine annealing learning rate schedule. 

## S.9. More experimental results 

S.9.1. Implementation details for main results 

Image captioning. For this task, we prompt the MLLM with â€œDescribe this image in one concise sentence, no longer than 20 words.â€ to generate a caption for each in-put image. The predicted caption is then compared against the ground-truth caption and the adversarial target caption for evaluation. 

Image classification. For this task, we prompt the MLLM with â€You are a precise visual classifier. What are the main objects in this image?: 0. rooster, 1. gibbon, 2. golden retriever, 3. goldfish, 4. hen, 5. hognose snake, 6. ice bear, 7. killer whale, 8. king crab, 9. kite, 10. shark. Output only a single integer between 0 and 10 with no ex-planation, no text, no punctuation.â€ to obtain the predicted class for each input image. All images in the dataset belong to classes 0â€“9, while class 10 (shark) is used as the adversar-ial target class. We then compute the ACC and ASR based 12 Table S.6. Experimental results on adversarial robustness of different defense methods and MLLMs on image classification tasks. The Mapper M is a lightweight U-Net. The overall best results are shown in bold , and the best results without smoothing are underlined. 

Model Method M-Attack [23] FOA [14] AttackVLM [51] 

FCS â†‘ ACC â†‘ ASR â†“ FCS â†‘ ACC â†‘ ASR â†“ FCS â†‘ ACC â†‘ ASR â†“

LLaVA-1.5-7B Org. 0.427 8.2% 78.2% 0.437 3.8 % 81.0 % 0.458 6.0% 78.4% Smoothed org. 0.590 84.8% 0.2% 0.600 87.2% 0.2% 0.605 87.0% 0.2% 

FARE [37] 0.574 55.4% 14.6% 0.521 24.8% 39.8% 0.508 25.8% 45.0% Smoothed FARE 0.695 75.6% 5.2% 0.681 66.2% 1% 0.671 68.0% 1% TeCoA [29] 0.731 67.4% 2% 0.626 29.0% 20.0% 0.592 33.6% 17.8% Smoothed TeCoA 0.790 70.6% 0.6% 0.714 58.8% 0.8% 0.712 56.4% 0.4% CLIP -L14 Org. 0.427 18.0% 37.2% 0.437 10.0 % 52.4 % 0.458 16.0% 48.4% Smoothed org. 0.590 88.0 % 0.4% 0.600 90.0 % 0.4% 0.604 90.4 % 0.4% 

FARE [37] 0.574 70.4% 5.2% 0.521 42.0% 14.4% 0.508 35.6% 24.4% Smoothed FARE 0.695 84.4% 0% 0.682 78.4% 0% 0.671 76.4% 0% TeCoA [29] 0.731 81.2% 2.4% 0.626 39.2% 20.2% 0.592 35.8% 24.4% Smoothed TeCoA 0.790 87.6% 1.2% 0.714 75.6% 2.4% 0.712 73.2% 2.0% MLLM s     

> Describe this image in one concise sentence, â€¦
> You are a precise visual classifier, â€¦
> You are a knowledgeable multimodal assistant, â€¦
> Captioning Classification VQA
> Prediction
> A dog is
> licking a
> person's
> hand.
> 10
> B(the
> Atlantic
> Ocean)
> GT
> Aclose up
> of a panda
> bear's face.
> 0
> A(the
> Arctic
> Ocean)
> Adv. target
> Adog is
> licking a
> person's
> hand.
> 10
> E(None of
> above)

Figure S.4. Illustration of the evaluation process. 

on the predicted labels, the ground-truth labels, and the ad-versarial target label. 

VQA. For this task, we prompt the MLLM using the pro-vided question associated with each input image and ask it to predict the corresponding answer. An example prompt is as follows: â€You are a knowledgeable multimodal assis-tant. Please answer the multiple-choice question ONLY by outputting the single letter of the correct option. Do not in-clude explanations or extra text. Question: Which ocean is highlighted? Options: A. the Arctic Ocean B. the Atlantic Ocean C. the Pacific Ocean D. the Southern Ocean E. None of the above. If the visual information is not clear or un-correlated with the questions, you should select the choice: None of the above. Answer with exactly one letter from the options above (e.g., A).â€ . In this evaluation, each question has a unique correct answer among the listed options, ex-cept for â€œNone of the above,â€ which is reserved for the ad-versarial target. The adversarial objective is to induce the MLLM to incorrectly choose â€œNone of the aboveâ€. An illustration of the implementation process is shown in Figure S.4. 

S.9.2. Experimental results of using a lightweight U-net as noise purifier 

The comparative results of different defense methods for image classification are summarized in Table S.6. In this experiment, the original diffusion-based noise purifier is replaced with a lightweight U-Net denoiser containing approximately 43.7M parameters . The results show that with this compact denoiser, FS-PSM consistently achieves strong and superior robustness, outperforming adversarial trainingâ€“based baselines by a substantial margin. Further-more, integrating FS-PSM with existing defenses yields ad-ditional robustness gains, leading to consistently favorable 13 Table S.7. Experimental results on adversarial robustness of dif-ferent defense methods and MLLMs on image classification tasks. The attack bound Ïµ = 32 /255 .

Model Method FOA [14] 

FCS â†‘ ACC â†‘ ASR â†“

LLaVA-1.5-7B Org. 0.37 3.6% 78.6% Smoothed org. 0.512 35.2% 24.2% FARE [37] 0.408 10.4% 64.2% Smoothed FARE 0.605 48.4% 19.2% TeCoA [29] 0.454 12.0% 29.6% Smoothed TeCoA 0.576 32.0% 9.6% Clean image AttackVLM  (16/255)  FOA (16/255)  M attack (16/255) FOA (32/255) 

Figure S.5. Visualization of the adversarial examples. 

improvements across all evaluated settings. These results collectively demonstrate the effectiveness and generaliz-ability of the proposed method. 

S.9.3. Experimental results on large attack bound 

The experimental results under a large adversarial perturba-tion of Îµ = 32 /255 are presented in Table S.7. In addition, Figure S.5 provides visualizations of the adversarial exam-ples generated by different attacks and perturbation bounds. These results show that nearly all existing defenses show great performance drop under such a strong attack. Under this challenging setting, the best accuracy is obtained by in-tegrating FS-PSM with FARE, reaching 48.4% on LLaVA, while the lowest ASR (9.6%) is achieved by combining FS-PSM with TeCoA. These findings further highlight the effectiveness and adaptability of the proposed framework, even against large-magnitude adversarial perturbations. 

Table S.8. Analysis of the robustness-efficiency trade-off of using different n0.                                              

> Model n0
> Robustness Efficiency FCS â†‘ACC â†‘ASR â†“Infer time (s) â†“
> LLaVA-1.5-7B (diffusion purifier) 10.595 86.2% 0.6% 0.28 20.652 88.4% 0.4% 0.41 40.697 88.8% 0.4% 0.66 80.719 89.6% 0.4% 1.12 16 0.728 89.2% 0.4% 2.10 LLaVA-1.5-7B (lightweight purifier) 10.494 84.4% 0.2% 0.20 20.565 85.4% 0% 0.24 40.600 87.2% 0.2% 0.32 80.623 87.8% 0.4% 0.49 16 0.643 86.8% 0.4% 0.84

S.9.4. Analysis of robustness-efficiency trade-off 

The robustnessâ€“efficiency trade-off results are summarized in Table S.8. In this experiment, we adopt the strong FOA attack as the adversary and report the average inference time per sample on LLaVA-1.5-7B using a single RTX 4090 GPU. The results show that increasing the sampling number 

n0 consistently improves the FCS under adversarial pertur-bations. Specifically, increasing n0 from 0 to 16 raises FCS from approximately 0.595 to 0.728, which aligns with our theoretical analysis in Section 3. However, this robustness gain comes at a substantial computational cost. For exam-ple, increasing n0 from 4 to 16 raises the inference latency from 0.66 seconds to 2.10 seconds per sample, revealing an inherent trade-off between robustness and efficiency. 14