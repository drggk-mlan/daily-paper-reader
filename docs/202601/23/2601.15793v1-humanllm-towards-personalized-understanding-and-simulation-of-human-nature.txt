Title: HumanLLM: Towards Personalized Understanding and Simulation of Human Nature

URL Source: https://arxiv.org/pdf/2601.15793v1

Published Time: Fri, 23 Jan 2026 01:34:27 GMT

Number of Pages: 12

Markdown Content:
# HumanLLM: Towards Personalized Understanding and Simulation of Human Nature 

# Yuxuan Lei 

University of Science and Technology of China Hefei, China leiyuxuan@mail.ustc.edu.cn 

# Tianfu Wang 

The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, China twang566@connect.hkust-gz.edu.cn 

# Jianxun Lian âˆ—

Microsoft Research Asia Beijing, China jialia@microsoft.com 

# Zhengyu Hu 

The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, China zhu021@connect.hkust-gz.edu.cn 

# Defu Lian 

University of Science and Technology of China Hefei, China liandefu@ustc.edu.cn 

# Xing Xie 

Microsoft Research Asia Beijing, China xing.xie@microsoft.com 

## Abstract 

Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behaviorâ€”a capability with profound implications for transforming social sci-ence research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and per-sonalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, un-contextualized web data does not capture the continuous, situated context of an individualâ€™s decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to em-power the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence bench-marks, indicating enhanced generalization. This work paves the way for more human-centric AI systems by advancing research in social simulation, developing personalized companions, enabling  

> âˆ—Corresponding author.
> This work is licensed under a Creative Commons Attribution 4.0 International License.
> KDD â€™26, Jeju Island, Republic of Korea
> Â©2026 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2258-5/2026/08 https://doi.org/10.1145/3770854.3780294

marketing intelligence through simulated customer feedback, and powering more realistic user simulation for recommender systems. 

## CCS Concepts 

â€¢ Computing methodologies â†’ Natural language processing .

## Keywords 

Cognitive Genome Dataset, Human-centric LLMs 

ACM Reference Format: 

Yuxuan Lei, Tianfu Wang, Jianxun Lian, Zhengyu Hu, Defu Lian, and Xing Xie. 2026. HumanLLM: Towards Personalized Understanding and Simulation of Human Nature. In Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD â€™26), August 09â€“13, 2026, Jeju Island, Republic of Korea. ACM, New York, NY, USA, 12 pages. https: //doi.org/10.1145/3770854.3780294 

## 1 Introduction 

In recent years, large language models (LLMs) have demonstrated remarkable proficiency across a wide range of professional bench-marks, often achieving or even surpassing human-level perfor-mance in areas such as code generation [ 17 ], math reasoning [ 3], and text summarization [ 41 ]. Beyond these technical domains, LLMs have begun to exhibit intriguing, nascent human-like characteris-tics, including the ability to express personality traits [ 15 , 16 ] and a rudimentary understanding of theory of mind [ 7, 29 ]. This evolu-tion has spurred significant interest in leveraging LLMs for applica-tions that require simulating human behavior, such as intelligent non-player characters (NPCs), emotional companions, personalized assistants, and large-scale sociological simulations [ 10 , 12 , 31 , 37 ]. These advancements highlight the considerable potential of LLMs to capture and simulate facets of human nature, opening new fron-tiers in both AI and social science research. However, a critical gap remains in the personalized and nuanced understanding of human cognition and behavior. Although LLMs excel in professional benchmarks, their training on vast, general-purpose web corpora [1, 8, 38] is not optimized for simulating the intricacies of individual humans. This limitation stems from a funda-mental misalignment: standard pretraining relies on disconnected text snippets, which fail to capture the continuous, situated context shaping an individualâ€™s decisions, thoughts, and behaviors over  

> arXiv:2601.15793v1 [cs.CL] 22 Jan 2026 KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea Yuxuan Lei et al.

time. Consequently, when simulating human feedback or actions in complex social scenarios, these models often struggle to accurately predict motivations, infer inner states, or forecast future actions. This challenge motivates our central research question: Can we build a universal foundation model for deeper, more personalized understanding and simulation of human cognition and behavior? Recent research has explored pathways to make LLMs more human-like. Some studies have focused on training models to pre-dict outcomes from controlled psychology experiments or economic games [ 5 , 12 , 35 ]. Others have utilized role-playing benchmarks based on fictional characters [ 33 ]. While valuable, the data used in these approaches are often limited in scale, collected in artificial settings that emphasize group averages, or derived from fictional narratives. Consequently, they may not fully capture the complexity, spontaneity, and long-term dynamics of authentic human experi-ences as they unfold in the real world. To address these limitations, we propose a novel approach: har-nessing large-scale, real-world user data from diverse online plat-forms to ground the learning of human nature in authentic expe-riences. Platforms like Reddit, Twitter, Blogger, and Amazon host a wealth of unstructured user-generated content that reflect the genuine, spontaneous behaviors and expressed thoughts of millions of individuals over extended periods. This data provides an unprece-dented resource for modeling the intricate relationship between a personâ€™s identity ( ð‘ƒ ), their environment ( ð¸ ), and their resulting ac-tions ( ðµ ), as captured by Lewinâ€™s seminal equation, ðµ = ð‘“ (ð‘ƒ, ð¸ ) [ 20 ]. Nevertheless, the critical challenge lies in transforming these raw, noisy logs into a well-narrative form that chains an individualâ€™s scattered behaviors into a coherent trajectory for model learning. Inspired by this, we introduce HumanLLM, a foundation model tailored for the personalized understanding and simulation of hu-man individuals. The core of our approach is the construction of the Cognitive Genome Dataset. We collect real-world user data from multiple online platforms and employ a rigorous, three-stage pipelineâ€”comprising data filtering, data synthesis, and data quality controlâ€”to automatically curate over 5.5 million user logs. This process distills the raw data into high-quality representations of user profiles, situational scenarios, and associated social question-answer pairs, effectively capturing patterns of behavior and thought. Based on this dataset, we design a suite of diverse learning tasks, including profile generation, social question answering, and writ-ing imitation, and apply supervised fine-tuning to equip the model with the ability to predict a wide range of individualized human behaviors and cognition. To mitigate catastrophic forgetting of gen-eral capabilities, we adopt a model merging strategy [ 34 ], blending the fine-tuned model with its original base, which proves more effective than joint fine-tuning with general instruction data. Our comprehensive evaluations demonstrate that HumanLLM achieves superior performance on in-domain tasks, significantly outperforming base models in predicting user actions, inner thoughts, and stylistic imitation. Furthermore, HumanLLM shows enhanced generalization capabilities, delivering substantial gains on out-of-domain social intelligence benchmarks like MotiveBench [ 40 ] and TomBench [ 7] . Through real-world application studies in profile generation, human behavior explanation, and personalized writ-ing, we confirm that HumanLLM serves as a superior social data generator, human explainer, and user simulator. Our main contributions are as follows: 

â€¢ To the best of our knowledge, we are the first to leverage large-scale, real-world user data from multiple online platforms to en-able LLMs to learn personalized human behaviors and thoughts. We construct the Cognitive Genome Dataset through a rigorous curation pipeline, resulting in a rich resource of hundreds of thousands of users, millions of scenarios, and social QA pairs. 

â€¢ We design a series of training tasks to help LLMs learn a wide range of personalized user behaviors and thoughts, resulting in HumanLLMâ€”a foundation model with enhanced social intelli-gence. 

â€¢ We conduct comprehensive evaluations on in-domain test data, public social benchmarks, and real-world applications, demon-strating that our model is a superior social data generator, hu-man explainer, and user simulator. Code is available at https: //aka.ms/humanllm 

## 2 Cognitive Genome Dataset 

We select our data sources according to the following criteria: (1) each user must have multiple records; (2) the user population should be diverse; and (3) user records must reflect authentic, spontaneous behaviors and thoughts from everyday life. Based on these stan-dards, we identify four high-quality sources: Reddit [ 19 ] (47.2M submissions from 50 subreddits), Twitter [ 9] (88.1M original tweets from CommonCrawl), Blogger [ 28 ] (681K blogs), and Amazon [ 13 ](571.54M item reviews). For details regarding the data sources, please refer to the Appendix A.1. Based on these data sources, our data curation process consists of three stages: data filtering, data synthesis, and data quality control. We introduce each of these steps in detail in following sections. 

## 2.1 Data Filtering 

The primary goal of this step is to clean the raw data, ensuring its authenticity, narrative richness, and safety. We first apply rule-based filtering. Specifically, for Reddit data, we retain posts with score â‰¥ 2, num_comments â‰¥ 2, and length â‰¥

300 characters, and filter out all posts from bots, moderators, deleted accounts, or system users. For Twitter data, we keep tweets with length â‰¥ 300 characters, removing retweets, quote tweets, corpo-rate or brand accounts, and advertisement-like content. For Blogger data, we exclude blogs shorter than 300 characters and those that do not contain first-person pronouns. For Amazon data, we filter out reviews shorter than 100 characters and apply a k-core filter, keeping only items with at least three purchase records and users with at least eight purchase histories in the dataset. For the retained text, we use Llama-3.3-70B-Instruct for text rewriting, removing HTML tags, URLs, non-English content, and formatting noise. For Twitter, we additionally remove hashtags and mentions. Furthermore, we require the LLM to replace any sensitive or private information, such as home addresses or ID numbers, with fictitious data to protect user privacy. Finally, we perform model-based scoring to further ensure qual-ity and safety. Using Llama-3.3-70B-Instruct, each text is assigned two labels: a three-level quality label (high, medium, low) and a binary harmfulness label. High-quality content must contain de-tailed personal experiences, inner thoughts, and social behaviors, HumanLLM: Towards Personalized Understanding and Simulation of Human Nature KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea 1.Data Filtering 

Rule -based Filtering 

Text Rewriting 

2.Data Synthesis 

User  Persona 

Model -

based 

Scoring 

> Profile
> Style Stories

Scenario  Characters 

> Plots Background

Social 

QA 

> Scenario
> Answer Question

3.Data Quality Control 

Model -based Scoring  

> Coverage
> Conciseness
> Fidelity
> Novelty
> Relevance Leakage
> Overall
> Text Quality
> Harmfulness

Model Merging 

HumanLLM 

Cognitive Genome Dataset 

Social Knowledge  Profile Generation 

Social Interaction  Scenario Generation 

Social Reasoning  Social QA 

Communication  Personalized Writing 

Preference  Item Selection 

> Hallucination

SFT LLM 

Phi  Llama  Qwen 

Base LLM   

> Reddit Twitter
> Blogger Amazon

Figure 1: A graphical illustration for our pipeline. 

and avoid excessive advertising or noisy content. Harmful content includes anything involving violence, pornography, or privacy vio-lations. We retain only high-quality and non-harmful text, resulting in a final dataset of 2.8 million Reddit posts, 673,000 tweets, 368,000 blogs, and 1.7 million reviews. 

## 2.2 Data Synthesis 

The core objective of this stage is to distill meaningful content from the raw data. Since the original user records inevitably contain some noise and are essentially unstructured textual logs, simply memorizing these texts does little to enhance a modelâ€™s social intel-ligence. Inspired by Lewinâ€™s Equation [ 20 ] from social psychology, 

ðµ = ð‘“ (ð‘ƒ, ð¸ ), which posits that behavior ( ðµ ) is determined jointly by the person ( ð‘ƒ ) and their environment ( ð¸ )â€”we recognize that un-derstanding human behavior requires considering both individual characteristics and the situational context. Accordingly, we extract three types of content from the data: user, scenario, and social QA, which correspond to the components ð‘ƒ , ð¸ , and ðµ in Lewinâ€™s for-mulation. In this stage, we also employ Llama-3.3-70B-Instruct to synthesize the data. 

User. The user component focuses on providing a comprehen-sive, global, and long-term summary of an individual, encompassing their demographics, personality traits, core values, interests, emo-tional tone, and salient life experiences. This holistic profile serves as a foundation for understanding and predicting the userâ€™s behav-iors and thoughts. Specifically, for each user, we concatenate all available information (e.g., their entire sequence of Reddit posts) in chronological order and prompt the LLM to generate a corre-sponding profile. Importantly, because no oneâ€™s experiences can be fully captured and information gaps are inevitable, we choose to generate profiles in the form of natural paragraphs rather than structured templates. This approach avoids the common issues of missing values in structured profiles and preserves user-specific details that might otherwise be lost in rigid formats. We generate four distinct levels of user profiles to offer diverse perspectives and provide richer task types for LLM training: 

â€¢ User Persona: A concise, short-form profile (less than 100 words) capturing a userâ€™s essential identity and key personality traits. 

â€¢ User Profile: A more detailed and comprehensive profile (100â€“400 words) that includes background, interests, and summaries of significant experiences. 

â€¢ User Stories: A collection of the userâ€™s important life experiences or events, with each story containing a summary and a detailed narrative describing a significant or unique occurrence. These stories reveal insights into the userâ€™s cognition and behaviors. 

â€¢ Writing Style: A summary of the userâ€™s distinctive writing style. Capturing linguistic patterns and stylistic tendencies enables the model to better imitate the userâ€™s authentic voice and en-hances personalized generation tasks. 

Scenario. For every individual record, we aim to identify a com-plete story that includes the time, location, characters, and events, and organize this information into a structured triplet of back-ground, characters, and plots. On one hand, these scenario data capture the full chain of user behaviors within specific contexts, enabling the LLM to learn to generate rich, contextually grounded content through scenario generation task. On the other hand, these structured scenarios also provide the necessary context for subse-quent context-aware tasks within the social QA component. KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea Yuxuan Lei et al. 

Social QA. For the behavior ( ðµ ) component, our goal is to enable the LLM to learn how individuals are likely to behaveâ€”both in terms of actions and inner thoughtsâ€”given their unique characteristics and the context of their situation. The most direct and effective way to achieve this is through question-answering tasks. Specifically, for each user record, we extract multiple scenarioâ€“questionâ€“answer triplets, with each question focusing on a different aspect: the userâ€™s next action, inner mental state, or social reasoning. Unlike questions about next actions or inner mental states, social reasoning questions are designed to foster the LLMâ€™s capacity for attributionâ€”to explore why a user behaves in a certain way, rather than simply what they do or feel. Importantly, the scenario in these triplets is refined to ensure that the answer is not directly revealed, thus making the QA pairs more challenging and promoting deeper reasoning. 

## 2.3 Data Quality Control 

During the data synthesis process, we observe that despite strict prompt engineering, the LLM-generated content still exhibits cer-tain issues. For example, some social QA pairs suffer from data leakage, where the answers could be directly copied from the sce-nario, making the pairs overly simplistic. Similarly, some generated profiles contain significant hallucinations, describing experiences that the user had not actually encountered. Therefore, a rigorous quality control process is essential. Given the scale of our data, we adopt an automated evaluation approach using Qwen-2.5-72B. Based on our analysis of the issues present in the synthesized data, we define several evaluation metrics, with each type of synthesized data requiring only a relevant subset of these metrics. Full metric description can be found in Appendix A.3. 

Hallucination. All generated content must be faithful to the source text. This metric evaluates whether personas, profiles, sto-ries, scenarios, and QA samples remain grounded in the main story and intent of the original records. 

Coverage. This metric assesses how well the generated con-tentâ€”whether a persona, profile, story, scenario, or QA sampleâ€” captures the salient, meaningful, and unique aspects of the original user data. 

Conciseness. This metric evaluates whether the generated per-sona or profile is succinct, cohesive, and free from redundancy or unnecessary repetition. 

Relevance. This metric assesses whether the generated per-sona or profile focuses exclusively on user-relevant information, excluding generic, promotional, off-topic material from the original data. 

Fidelity. This metric evaluates whether generated scenarios are vivid, detailed, and coherent rather than generic or superficial. It also assesses the logical connection and complementarity among scenario, question, and answer. 

Novelty. This metric assesses how interesting, distinctive, and memorable the generated content is. It measures whether QA sam-ples, profiles, stories, and scenarios highlight the userâ€™s unique personality, experiences, and perspectives, evoking specific per-sonal or social resonance rather than being formulaic or generic. 

Leakage. This metric measures the extent to which the answer is directly revealed or obvious from the scenario description. 

Overall. This metric provides a holistic assessment of the overall quality of the generated content. Each metric is scored on a scale from 1 to 10. We retain samples with an overall score greater than 8 and all other scores above 7. This step filters out approximately 30% of users, 60% of scenarios, and 40% of social QA samples from each source. Complete data statistics are provided in Appendix A.2. Overall, the synthesized data and the cleaned raw data together constitute our Cognitive Genome Dataset. The whole pipeline is illustrated in Figure 1, which provides a comprehensive overview of each key component and the overall workflow. 

## 3 HumanLLM 

To train a powerful human-like foundation model, we first describe how we design training tasks based on the Cognitive Genome Dataset, and then present the specific implementation details for training HumanLLM. 

## 3.1 Task Design 

The Cognitive Genome Dataset encompasses a rich array of user behaviors and thoughts, providing a robust foundation for LLMs to learn the full spectrum of human nature. To enable the model to capture this diversity, we design six complementary learning tasksâ€”profile generation, scenario generation, social question an-swering, writing imitation, personalized commenting, and item selection. Together, these tasks collectively span the major dimen-sions of human activity: self-representation, contextual interaction, social reasoning, communication, and preference-based decision making. 

Profile Generation. Formally, given a brief user persona ð‘ ð‘Ž ,we prompt the LLM to generate a more detailed user profile ð‘ ð‘’ 

as follows: ð‘ ð‘’ = ð¿ð¿ð‘€ (ð‘ ð‘Ž ). This task enables the model to infer a comprehensive user background and life trajectory from minimal information, thereby enhancing its social knowledge. 

Scenario Generation. Formally, given a set of characters ð‘ and background information ð‘ , we prompt the LLM to reconstruct the complete story plots: ð‘ð‘™ð‘œð‘¡ð‘  = ð¿ð¿ð‘€ (ð‘, ð‘ ). This task encourages the LLM to learn what constitutes a complete and authentic social scenario, as well as realistic social interactions. 

Social Question Answering. Given a user persona ð‘ ð‘Ž or pro-file ð‘ ð‘’ , a scenario ð‘  , and a question ð‘ž , we prompt the LLM to answer the question related to the user in that specific context: 

ð‘Ž = ð¿ð¿ð‘€ (ð‘ ð‘Ž ð‘œð‘Ÿ ð‘ ð‘’ , ð‘ , ð‘ž ). This task enables the model to learn per-sonalized user behaviors, thoughts, and motivations within specific scenarios, thereby strengthening its social reasoning abilities. 

Writing Imitation. Given a set of text segments from the userâ€™s writing history ð‘  â„Žð‘–ð‘ ð‘¡ , or a description of the userâ€™s writing style ð‘¤ ,along with a new topic ð‘¡ , we prompt the LLM to generate a new text (e.g., post, blog, tweet, or review) in the userâ€™s style for the given topic: ð‘  ð‘›ð‘’ð‘¤ = ð¿ð¿ð‘€ (ð‘  â„Žð‘–ð‘ ð‘¡ ð‘œð‘Ÿ ð‘¤, ð‘¡ ). This task improves the modelâ€™s capacity for personalized writing. 

Personalized Commenting. Given a history of the userâ€™s pre-vious comments ð‘ â„Žð‘–ð‘ ð‘¡ and a new post ð‘ ð‘›ð‘’ð‘¤ , we prompt the LLM to generate a comment that the user might write in response to the new post: ð‘ ð‘›ð‘’ð‘¤ = ð¿ð¿ð‘€ (ð‘ â„Žð‘–ð‘ ð‘¡ , ð‘ ð‘›ð‘’ð‘¤ ). This task further enhances the modelâ€™s ability for personalized, context-aware writing. HumanLLM: Towards Personalized Understanding and Simulation of Human Nature KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea 

Item Selection. Given ð‘– â„Žð‘–ð‘ ð‘¡ , a sequence of items previously pur-chased by the user, and the userâ€™s persona ð‘ ð‘Ž or profile ð‘ ð‘’ , we prompt the LLM to select the most preferred item from a set of can-didates ð‘– ð‘ð‘Žð‘›ð‘‘ : ð‘– ð‘¡ð‘Žð‘Ÿð‘”ð‘’ð‘¡ = ð¿ð¿ð‘€ (ð‘– â„Žð‘–ð‘ ð‘¡ , ð‘ ð‘Ž ð‘œð‘Ÿ ð‘ ð‘’ , ð‘– ð‘ð‘Žð‘›ð‘‘ ). This task allows the model to learn and simulate personalized user preferences. Guided by Lewinâ€™s Equation, our task design reflects a com-mon cognitive chain in human behavior: recognizing the person, understanding the context, and generating contextually grounded actions, thoughts, or decisions. Each task captures a high-frequency instance of this process in real-world settings, enabling the model to learn how personal traits and situational factors jointly shape behavior. Moreover, the data are drawn from multiple platforms and cover diverse topics, ensuring both breadth and representative-ness, and reinforcing the modelâ€™s ability to generalize across social contexts. 

## 3.2 Implementation Details 

Based on the Cognitive Genome Dataset, we construct training data covering six task types. Given the sheer volume of available data and the prohibitive computational cost of training, we sample each task to ensure roughly equal representation across tasks and data sources. This results in a final dataset comprising 1,195,717 training samples and 132,869 test samples. We organize the data in ShareGPT format and train our models using the next-token prediction task, masking out non-response positions from the loss calculation. To comprehensively demonstrate the effectiveness of our dataset, we train several mainstream open-source LLMs, including Qwen2.5-3B-Instruct [ 30 ], Qwen2.5-7B-Instruct [ 30 ], Qwen3-8B [ 38 ], Phi-3-mini-128k-instruct [ 2], and Llama-3.1-8B-Instruct [ 8]. We use Llama-Factory [ 42 ] to implement model training. All models are fully fine-tuned on 8 Ã—40GB A100 GPUs, using a learning rate of 5e-6, a total batch size of 64, max sequence length of 8192, cosine learning rate scheduler, and a warm-up ratio of 0.5. We enable different stages of DeepSpeed Zero [ 27 ] based on model size. Training is conducted for 3 epochs, with the 8B models requiring approximately 120 hours. The Inference cost of HumanLLM is identical to that of the base model, and we use the vLLM framework [ 18 ] for LLM inference, with the temperature set to 0.7. It is important to note that catastrophic forgetting is a well-known issue in supervised fine-tuning (SFT) of LLMs, where the model tends to overfit to the new data at the expense of its general intelligence and generalization ability. A common remedy is to include a portion of general-purpose instruction tuning data during training. However, in our experiments, this approach yields subopti-mal results: although models perform relatively well on the test set, their generalization to public social intelligence benchmarks is sig-nificantly degraded, and human evaluation of generations reveals a notable decline in response quality. Inspired by Lm-cocktail [ 34 ], we adopt an alternative approachâ€”model mergingâ€”to mitigate cat-astrophic forgetting. Since the instruction-tuned version of base models already possesses strong general intelligence, we perform a 1:1 weight merge between the fine-tuned and original models, producing our final HumanLLM, which combines enhanced social intelligence with robust general capabilities. In our experiments, we report results using the merged models by default, and provide further analysis in the ablation study section. 

## 4 Experiments 4.1 Overview 

Understanding and modeling human cognition and behaviors is a foundational challenge across the social sciences and artificial intelligence. Existing AI models are seldom optimized for social intelligence, which limits their effectiveness and adaptability in a wide range of social contexts. By training on the comprehensive and richly structured Cognitive Genome Dataset, our HumanLLM aims to transcend these limitations, enabling more robust, nuanced, and context-aware modeling of human cognition and behaviors. To systematically evaluate the breadth and depth of our modelâ€™s social intelligence, we design a diverse suite of benchmark tasks that collectively reflect key dimensions of real-world human activ-ity. Specifically, we first introduce an in-domain benchmark cover-ing item selection, profile generation, scenario generation, social question answering, and writing imitation, spanning everything from individual preference modeling to social reasoning and ex-pressive communication. To further assess generalization and the advancement of core social reasoning abilities, we conduct rigorous evaluations on two leading public social intelligence benchmarks. Finally, we illustrate HumanLLMâ€™s practical utility and creative generative capacity through a series of real-world applications. Regarding experimental baselines, in addition to the direct compar-ison between HumanLLM and its base model, we introduce three additional models: Centaur [ 5 ], BE.FM [ 35 ], and GPT-4o 1[14 ]. The former two are also open foundation models for human behavior and cognition, both fine-tuned from the Llama-3.1-8B family, but they primarily focus on learning from human decision-making data such as psychological experiments, economic games, and sur-vey responses. GPT-4o, in contrast, represents a state-of-the-art, high-capacity proprietary LLM. 

## 4.2 Results on In-domain Evaluation Tasks 

We evaluate HumanLLM and baseline models on five in-domain tasks: item selection, profile generation, scenario generation, so-cial question answering, and writing imitation. To better quantify model performance, we convert open-ended generation tasks into multiple-choice questions with standard answers. Specifically, for the item selection taskâ€”which is naturally a multiple-choice prob-lemâ€”we require the model to choose its preferred item from 20 candidates. The ground truth is the actual item purchased by the user, while the remaining 19 negative examples are randomly sam-pled from the item pool. For the other four tasks, we formulate each as an eight-way multiple-choice question. That is, given a task instruction and eight candidate answers, we prompt the LLM to select the most appropriate response. The correct answer is the result extracted from real user data. For the seven negative options, we use responses generated by leading LLMs (GPT-4o [ 14 ], Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-72B-Instruct, Phi-4 [ 1 ], and Qwen3-8B) for each task. Since these models have not seen the userâ€™s actual behavior, their answers are considered suboptimal. Table 1 summarizes the results. HumanLLM consistently outperforms its vanilla counter-parts across all tasks and multiple base models. The best result  

> 1The version we use is gpt-4o-2024-11-20. KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea Yuxuan Lei et al.

Table 1: In-domain evaluation (%) on core tasks. Models with HumanLLM augmentation show significant improvements. The "Imp." column reports the performance gains of HumanLLM over its corresponding base model. The best value and the second best for each column are marked in red and blue, respectively. 

Model Item Selection Profile Generation Scenario Generation Social QA Writing Imitation Avg. Imp. (%) Centaur 4.36 3.58 5.40 0.17 1.14 2.93 â€“BE.FM 11.57 11.83 14.20 9.50 10.39 11.49 â€“GPT-4o 25.54 5.75 7.49 5.83 7.28 10.38 â€“Phi-3-mini-128k-instruct 10.20 10.89 12.67 8.00 5.12 9.38 HumanLLM phi-3 30.86 11.11 15.33 12.33 11.14 16.15 72.17 Llama-3.1-8B-Instruct 16.86 3.33 6.67 1.67 4.01 6.51 HumanLLM Llama 36.56 15.33 16.33 14.67 26.06 21.79 234.72 Qwen3-8B 14.50 5.33 10.33 1.67 3.79 7.12 HumanLLM Qwen3 32.20 31.78 22.00 12.67 13.36 22.40 214.61 Qwen2.5-3B-Instruct 12.56 1.78 3.67 2.00 1.78 4.36 HumanLLM Qwen2.5-3B 28.86 16.89 12.67 6.67 8.02 14.62 235.32 Qwen2.5-7B-Instruct 15.23 4.22 9.33 1.67 3.12 6.71 HumanLLM Qwen2.5-7B 33.16 39.56 27.33 11.00 25.84 27.38 308.05 HumanLLM raw Qwen2.5-7B 19.44 8.67 10.67 1.33 8.02 9.63 43.52 HumanLLM gen Qwen2.5-7B 35.73 38.00 18.33 15.00 28.54 27.12 304.17 HumanLLM t1 Qwen3 15.01 23.25 14.90 11.75 10.77 15.13 112.50 HumanLLM t2 Qwen3 31.97 4.58 8.36 6.58 13.58 13.01 82.72 

is achieved by HumanLLM Qwen2.5-7B (27.38% average). The largest improvements are observed on item selection and profile genera-tion, suggesting that HumanLLM enhances the modelâ€™s capacity to understand user preferences and life trajectories. Improvements in writing imitation and scenario generation indicate better modeling of user style and context. These findings highlight the effectiveness of large-scale user-centric data and multi-task learning in improv-ing social intelligence and personalization in foundation models. We also observe that the Centaur model, which is fine-tuned from the Llama-3.1-8B rather than its instruction-tuned version, tends to overfit to its training data and exhibits poor instruction-following capabilities. As a result, its performance falls short of expectations across all evaluated tasks. In contrast, BE.FM shows improved per-formance compared to Llama-3.1-8B-Instruct, but still lags signifi-cantly behind HumanLLM. Notably, in the in-domain evaluation setting, GPT-4o also underperforms HumanLLM, indicating that after targeted training, HumanLLM becomes more sensitive to real user behaviors and is better able to infer responses that align with user-specific characteristics. 

## 4.3 Results on Out-of-domain Benchmarks 

To evaluate the generalization of models, we evaluate HumanLLM and baseline models on MotiveBench [ 40 ] and TomBench [ 7], two out-of-domain social intelligence benchmarks. Both benchmarks provide a comprehensive assessment of social reasoning, encom-passing a wide spectrum of human attributes such as motivation, emotion, desire, knowledge, belief, intention, and communication. Together, they reflect the essential facets of human cognition and interpersonal understanding that are fundamental for advanced AI systems. The results are provided in Table 2 and Table 3, respec-tively. HumanLLM outperforms its respective base models on all 

Table 2: OOD Performance on MotiveBench. The "Imp." col-umn reports performance gains of HumanLLM over its base model. The best value and the second best for each column are marked in red and blue.                                                                                      

> Model Amazon Blogger Persona Avg. Imp. (%) Centaur 0.0366 0.0488 0.0266 0.0373 â€“BE.FM 0.7300 0.7244 0.6061 0.6868 â€“GPT-4o 0.9011 0.7744 0.7744 0.8166 â€“Phi-3-mini-128k-instruct 0.7244 0.6088 0.5933 0.6421 HumanLLM phi-3 0.7422 0.6644 0.6311 0.6792 5.77 Llama-3.1-8B-Instruct 0.5866 0.6499 0.5216 0.5860 HumanLLM Llama 0.7355 0.7388 0.6311 0.7018 19.76 Qwen3-8B 0.8188 0.6655 0.6727 0.7190 HumanLLM Qwen3 0.8222 0.7033 0.7027 0.7427 3.30 Qwen2.5-3B-Instruct 0.6511 0.5711 0.5800 0.5990 HumanLLM Qwen2.5-3B 0.6744 0.6588 0.5833 0.6388 6.64 Qwen2.5-7B-Instruct 0.7433 0.6366 0.6633 0.6810 HumanLLM Qwen2.5-7B 0.7677 0.6766 0.6866 0.7103 4.30 HumanLLM raw Qwen2.5-7B 0.7488 0.6711 0.6661 0.6953 2.10 HumanLLM gen Qwen2.5-7B 0.7544 0.6911 0.6688 0.7047 3.48 HumanLLM t1 Qwen3 0.7988 0.6911 0.6866 0.7254 0.89 HumanLLM t2 Qwen3 0.7966 0.6955 0.7138 0.7353 2.26

metrics. On MotiveBench, HumanLLM Qwen3 achieves the highest average score (0.7427). On TomBench, the same model also leads (0.6589), particularly in intention and non-literal communication. Performance gains are consistent across different LLM backbones, indicating that the Cognitive Genome Dataset and our training ap-proach improve LLMâ€™s social reasoning and generalization. These results suggest that HumanLLM can be broadly applied to social intelligence tasks beyond the original training distribution. In con-trast, BE.FM exhibits unstable performance. It outperforms Llama-3.1-8B-Instruct on MotiveBench, but performs worse on TomBench, HumanLLM: Towards Personalized Understanding and Simulation of Human Nature KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea 

Table 3: OOD Performance on TomBench. NLC refers to Non-Literal Communication. The "Imp." column reports performance gains of HumanLLM over its base model. The best value and the second best for each column are marked in red and blue. 

Model Emotion Desire Intention Knowledge Belief NLC Avg. Imp. (%) Centaur 0.4246 0.3686 0.4072 0.2820 0.3513 0.4164 0.3750 â€“BE.FM 0.6514 0.5392 0.6658 0.4186 0.6190 0.6870 0.5969 â€“GPT-4o 0.7524 0.6278 0.8235 0.5779 0.8594 0.7861 0.7378 â€“Phi-3-mini-128k-instruct 0.5585 0.5455 0.5245 0.4626 0.4662 0.5468 0.5173 HumanLLM phi-3 0.5725 0.5302 0.5706 0.4310 0.5347 0.6117 0.5418 4.74 Llama-3.1-8B-Instruct 0.5928 0.5448 0.6461 0.5240 0.6406 0.6526 0.6001 HumanLLM Llama 0.6135 0.5806 0.6961 0.4664 0.6178 0.6608 0.6059 0.97 Qwen3-8B 0.6468 0.6002 0.6639 0.5039 0.6282 0.7922 0.6392 HumanLLM Qwen3 0.6614 0.6163 0.7230 0.4848 0.6624 0.8054 0.6589 3.08 Qwen2.5-3B-Instruct 0.6089 0.5292 0.5211 0.4626 0.7758 0.7400 0.6063 HumanLLM Qwen2.5-3B 0.6289 0.5671 0.5721 0.3676 0.7468 0.7905 0.6122 0.97 Qwen2.5-7B-Instruct 0.6067 0.5864 0.5942 0.4198 0.7962 0.7135 0.6195 HumanLLM Qwen2.5-7B 0.6235 0.5655 0.6826 0.3873 0.8329 0.7568 0.6414 3.54 HumanLLM raw Qwen2.5-7B 0.4888 0.4812 0.6000 0.4444 0.8166 0.7000 0.5885 -5.00 HumanLLM gen Qwen2.5-7B 0.4555 0.5250 0.6083 0.3777 0.5805 0.7000 0.5412 -12.64 HumanLLM t1 Qwen3 0.6317 0.6027 0.6783 0.4619 0.6671 0.8043 0.6410 0.28 HumanLLM t2 Qwen3 0.6278 0.6079 0.6600 0.4781 0.6615 0.8080 0.6406 0.22 

suggesting that training on data sources such as economic games and survey responses may provide only a partial view of human behavior and cognition, limiting the modelâ€™s ability to generalize across diverse behavioral settings. Given its substantially larger model capacity, GPT-4o outperforming HumanLLM on OOD bench-marks is expected. 

## 4.4 Ablation Study 

We aim to investigate three key questions: Q1: Does the carefully curated Cognitive Genome Dataset genuinely improve model per-formance? Q2: What are the advantages of the model merging approach compared to simply incorporating general-purpose in-struction tuning data? Q3: Do all training tasks contribute to the modelâ€™s performance? To address Q1, we directly train Qwen2.5-7B-Instruct on the original raw corpora (posts, tweets, blogs, and reviews) using a text completion task. Given the first half of a text, the model is tasked with generating the remaining half, with the split point randomly determined. The results of the trained model, noted as HumanLLM raw Qwen2.5-7B , are shown in Tables 1, 2, and 3. We observe that training on the raw data yields only marginal improvements over the base model on in-domain and MotiveBench benchmarks, and actually results in a notable performance drop on TomBench. On one hand, the original data are fragmented and fail to capture usersâ€™ personalized behaviors and long-term trajectories. On the other hand, they contain substantial noise, making it difficult for models to learn meaningful behavioral patterns. To answer Q2, we incorporate general-purpose instruction tun-ing data into the training. Specifically, we include the SmolTalk [ 4] and Orca [ 24 ] datasets, two widely used public SFT datasets covering more than twenty task types, including text editing, sum-marization, reasoning, mathematics, coding, long-context under-standing, and more. We mix these datasets with our training data from the Cognitive Genome Dataset in a 0.25:0.25:0.5 ratio, aiming to preserve the modelâ€™s generalization ability. The results of the trained model, noted as HumanLLM gen Qwen2.5-7B shown in Tables 1, 2, and 3, demonstrate that although the modelâ€™s performance on the in-domain test set is comparable to that of the model merging ap-proach, it falls significantly short on OOD benchmarks. Notably, on TomBench, it lags behind even the original base model by 0.0783. The reasons for this may be that model merging preserves the complementary strengths of both the base and SFT models by di-rectly combining their weights, thus retaining both general and social intelligence and reducing catastrophic forgetting. Unlike joint fine-tuning on mixed data, merging reduces interference between different objectives and enables better out-of-domain generaliza-tion. Whatâ€™s more, the instruction-tuned base model has already undergone extensive, proprietary optimization by its original team using high-quality, undisclosed instruction data. In contrast, open-source instruction-tuning datasets may differ in domain and are often of lower quality, making them less effective at maintaining the base modelâ€™s capabilities and preventing forgetting during joint fine-tuning. To address Q3, and considering the training cost, we divide the six training tasks into two groups: Task Group 1 (Profile Generation, Scenario Generation, and Social Question Answering) and Task Group 2 (Writing Imitation, Personalized Commenting, and Item Selection). We then train Qwen3-8B using only the tasks within each group. The resulting models, denoted as HumanLLM t1 Qwen3 and HumanLLM t2 Qwen3 , are reported in Tables 1, 2, and 3. The results indi-cate that, in in-domain evaluations, training on a specific task group leads to substantial performance improvements on the correspond-ing tasks, while performance on unseen tasks degrades compared to the full HumanLLM. Nevertheless, both models retain a certain degree of generalization ability and still slightly outperform the KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea Yuxuan Lei et al. 

base Qwen3-8B. On OOD benchmarks, models trained on either Task Group 1 or Task Group 2 achieve performance levels between Qwen3-8B and HumanLLM, suggesting that both task groups con-tribute meaningfully and act synergistically in improving overall performance. 

## 4.5 Real-World Application Studies 

We further explore the value of HumanLLM across three real-world applications: profile generation, human explainer, and personalized writing. We first conduct a systematic evaluation of HumanLLMâ€™s generation quality using GPT-4o, with 1,600 samples per application. We then select several representative cases to provide an intuitive comparison of generation quality between models. 

Profile Generation. Generating realistic and richly detailed user profiles is crucial for real-world applications such as data synthesis and user simulator. High-quality profiles enhance the diversity of synthetic data and authenticity of simulated user be-haviors. To evaluate profile quality, we score generated profiles on faithfulness (how well the profile expands on the original per-sona), realism (realistic, coherent, and consistent with how a real person might be described), and coverage (matching the depth and breadth of human references). Results in Figure 2 demonstrate that our model outperforms the base model across all three metrics. Case analysis in Figure B1 further shows that our model avoids hallucinated demographic details and produces nuanced, faithful expansions, leading to profiles that are more consistent with real user characteristics. 

Human Explainer. Understanding and explaining human be-havior in specific contexts is crucial for advancing personal AI assistants, social simulation, and research on human cognition. This application shows modelsâ€™ ability to interpret usersâ€™ thoughts, motivations, and reactions to nuanced scenarios, allowing for more empathetic, context-aware, and personalized interactions. To eval-uate the quality of generated explanations, we scores them on three key dimensions: contextual relevance (whether the answer is grounded in the user persona, scenario, and question), logical consistency (whether the reasoning is sound and coherent), and alignment with the reference (how well the answer matches the meaning and intent of real answer). Our results in Figure 2 show that HumanLLM consistently achieves high scores across these dimensions. For instance, in the provided case in Figure 3, Human-LLMâ€™s answer stands out for its thorough reference to scenario details, its integration of the userâ€™s background and perspective, and its comprehensive logical reasoning, leading to superior per-formance compared to the baseline model. Faithfulness Realism Coverage      

> Profile Generation
> 6
> 7
> 8
> 9
> GPT Score
> Relevance Consistency Alignment
> Human Explainer
> 7
> 8
> 9
> 10 Qwen2.5-7B-Instruct
> HumanLLM
> Style Relevance Similarity
> Personalized Writing
> 5
> 6
> 7
> 8
> 9

Figure 2: Performance comparison between HumanLLM and its base model across three applications. Full results can be found in Appendix B.1. The user is a 24 -year -old married woman, a mother to a young daughter, and a doula,    

> who has experienced significant life challenges, including the loss of her father, son,
> and multiple pregnancies, as well as her mother's struggles with MS, and is now
> navigating her own emotional and physical health struggles, while trying to make
> decisions about her future, including the possibility of having more children, and is
> deeply passionate about parenting and views her daughter as a gift, not property.
> The person has been observing several women whose behaviors and attitudes towards
> pregnancy, children, and motherhood have been causing frustration. One woman is
> trying to conceive again, but seems more focused on the experience of being pregnant
> than on having another child. Another woman is overly fixated on her new baby, barely
> acknowledging her older children. A third woman, who has gone through infertility, is
> now adopting from an Eastern country but is obsessed with what others will think of her
> 'different' looking baby. The person also notices that some people refer to their children
> as 'theirs' in a possessive manner, which is bothersome.
> User

Persona  

> Scenario
> Why does the person find the behaviors and attitudes of these women towards
> pregnancy, children, and motherhood so frustrating?
> Question
> The person finds these behaviors frustrating because they believe that being a mother
> should be about loving and nurturing a child, not about seeking attention or treating
> children as possessions. The person's own experiences with loss and parenthood have
> taught them to view every child as a precious gift, and they are disheartened by others
> who do not seem to share this perspective. The person is also annoyed by the
> possessive language used by some parents, as they believe that children are
> individuals who should be loved and respected, not owned.
> Reference

Answer   

> The person finds these behaviors and attitudes frustrating because they reflect a lack
> of genuine connection and focus on the inherent value of children, and instead
> prioritize personal experiences or societal expectations. This perspective conflicts with
> her deeply held beliefs about parenting as a sacred gift, not a possession, and her
> understanding that each child is a unique and precious blessing.

Qwen -2.5 -7B 

> The person finds these behaviors and attitudes frustrating because they seem to be more
> focused on the experience of having a child rather than the child itself, which is
> frustrating to the person who views children as gifts, not property. Additionally, the
> person is bothered by the lack of consideration for the needs and feelings of other
> children, as well as the fixation on how others perceive them, rather than the well -being
> of the child. The person's own experiences with infertility and loss may have contributed
> to their strong reactions to these behaviors.
> HumanLLM

Figure 3: Case Study for Human Explainer Application. Personalized Writing. Personalized writing is a highly practi-cal application in numerous real-world scenarios, such as person-alized review writing, automated email generation, and customer service communication. The ability to accurately imitate an individ-ual userâ€™s unique style, tone, and perspective not only enhances the authenticity and relevance of generated content but also improves user satisfaction and engagement. To evaluate the quality of gener-ated writings, we score them on three metrics: style match (how well the generated text matches the userâ€™s writing style, tone, and perspective), content relevance (the topical and logical coherence of the content), and content similarity (the extent to which the answer aligns with user generated text in terms of information coverage, depth, and main ideas). Results in Figure 2 show that HumanLLM delivers consistently better performance than the baseline across all metrics. Case analysis in Figure B2 shows that HumanLLMâ€™s output not only mirrors the userâ€™s style but also faithfully repro-duces critical details and reasoning patterns found in the reference review. This demonstrates the modelâ€™s strong ability to capture both stylistic nuances and substantive content in these applications. 

## 5 Related Work 5.1 The Need for Human-Centric LLMs 

LLMs such as Llama, Qwen, and Phi are pretrained on vast, general-purpose corpora, yet these sources rarely capture the intricacies of HumanLLM: Towards Personalized Understanding and Simulation of Human Nature KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea 

real human behaviors or the kinds of structured persona-scenario-behavior data necessary for advanced social simulation. This limi-tation has become increasingly relevant as LLMs are deployed in settings that require human-like understanding, such as AI com-panions, social simulation, and virtual agents [ 10 , 31 , 37 ]. As LLMs are now widely adopted in applications that interact directly with humans, the importance of grounding their capabilities in authen-tic, diverse human experiences has grown more apparent. Recent works show that models excelling at academic or professional tasks often fail at social intelligence, underlining the distinction between academic and social capabilities in LLMs [ 36 ]. These models may generate fluent and coherent text, yet still lack the nuanced rea-soning needed to simulate individual personalities, motivations, or dynamic social contexts. Several studies have sought to bridge this gap: for example, using LLMs to predict outcomes of psychology or economics experiments [ 12 ], or to serve as social participants in agent-based modeling [ 10 ] and virtual environments [ 31 ]. Such efforts underscore the growing recognition that standard web-scale pretraining alone is insufficient for authentic social intelligence. Broader research trends, sometimes described as "machine psychol-ogy", increasingly advocate for new data collection methodologies, more sophisticated simulation tasks, and human-centered training regimes to align LLMs with the complexities of human cognition and behavior [11]. 

## 5.2 Measuring Human-Like Abilities in LLMs 

The need for systematic evaluation of LLMsâ€™ human-like abilities, such as social intelligence, has given rise to numerous special-ized benchmarks and datasets, as researchers seek to move beyond conventional metrics that focus mainly on linguistic or logical ac-curacy. Role-playing and theory-of-mind tasks are now commonly used to assess whether models can reason about othersâ€™ beliefs, intentions, and personalities [ 7, 15 , 16 , 29 , 33 ]. Benchmarks like TOMBench [ 7], Coser [ 33 ], SocialEval [ 43 ], and MotiveBench [ 40 ]probe modelsâ€™ abilities to maintain persona consistency, simulate established social roles, and perform human-like motivational rea-soning. MotiveBench, in particular, targets the assessment of moti-vational reasoning, which is a key but often overlooked component of human-like intelligence. Meanwhile, works such as BigToM [ 32 ]and AgentSENSE [ 25 ] examine LLMsâ€™ social reasoning in both static and interactive, multi-agent settings, further expanding the scope of evaluation to more complex, real-world-like interactions. Several studies demonstrate that, despite rapid progress, even advanced LLMs lag significantly behind humans in theory-of-mind, emotional intelligence, and multi-party interaction [ 21 , 26 , 39 ]. Comprehen-sive reviews and benchmarking efforts [ 21 , 39 ] emphasize that advancing LLMsâ€™ social abilities requires not only better evaluation protocols but also richer training data that reflect the subtleties of everyday social cognition and interpersonal dynamics. 

## 5.3 Post-Training for Human Behavior Modeling 

To enhance LLMsâ€™ modeling of individualized human behavior, re-cent research has explored targeted post-training approaches. Open foundation models such as BE.FM [ 35 ] and Centaur [ 5] leverage large-scale behavioral and cognitive datasets to improve LLMsâ€™ ability to predict, explain, and simulate human decisions. Personal-ity conditioning through dialogue, as in Big5-Chat [ 22 ], has been shown to help LLMs maintain consistent persona traits during interactions, resulting in more authentic and engaging conversa-tional agents. SocialBench [ 6] and related multi-party conversa-tional datasets promote sociality and relational intelligence, while ToM-RL [ 23 ] demonstrates that reinforcement learning from spe-cialized curricula can significantly enhance LLMsâ€™ theory-of-mind capabilities, even for relatively small models. Despite these promis-ing advances, existing efforts often focus on optimizing LLMs for just some specified isolated social capabilities, such as personality expression or theory of mind, which can lead to overfitting and limited generalization. Broader and more integrated approaches remain rare. In contrast, our work aims to leverage large-scale post-training with diverse, real-world user data to comprehensively enhance LLMsâ€™ social understanding, reasoning, and simulation abilities across a wide spectrum of human behaviors, ultimately moving toward the goal of universal, human-centric social intelli-gence in language models. 

## 6 Conclusion 

In this work, we introduce HumanLLM, a foundation model specif-ically designed to advance the personalized understanding and simulation of human cognition and behaviors. By curating the large-scale Cognitive Genome Dataset from multiple real-world online platforms, we enable the model to capture the nuanced dynamics of user behaviors and thoughts. Extensive experiments across in-domain tasks, out-of-domain benchmarks, and real-world applications demonstrate that HumanLLM is a superior social data generator, human explainer, and user simulator. These results high-light the potential of leveraging large-scale, user-centric data to drive meaningful progress in human-like AI. Looking forward, we believe that our approach opens promising directions for building AI systems that are not only more socially intelligent and empa-thetic, but also better equipped to understand, collaborate with, and serve real people across diverse scenarios. 

## Acknowledgments 

The work was supported by grants from the National Natural Sci-ence Foundation of China (No. U24A20253). 

## References       

> [1] Marah Abdin, Jyoti Aneja, Harkirat Behl, SÃ©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauff-mann, et al .2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905 (2024). [2] Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Hassan Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, SÃ©bastien Bubeck, Martin Cai, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen El-dan, Dan Iter, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauff-mann, Nikos Karampatziakis, Dongwoo Kim, Mahmoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Xihui (Eric) Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Olatunji Ruwase, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna

KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea Yuxuan Lei et al. 

Zhang, Yi Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone . Technical Report MSR-TR-2024-12. Microsoft. https://www.microsoft.com/en-us/research/publication/phi-3-technical-report-a-highly-capable-language-model-locally-on-your-phone/ [3] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. 

arXiv preprint arXiv:2402.00157 (2024). [4] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel MartÃ­n BlÃ¡zquez, Guil-herme Penedo, Lewis Tunstall, AndrÃ©s Marafioti, Hynek KydlÃ­Äek, AgustÃ­n Pi-queres LajarÃ­n, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, ClÃ©mentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. 2025. SmolLM2: When Smol Goes Big â€“ Data-Centric Training of a Small Language Model. arXiv:2502.02737 [cs.CL] https://arxiv.org/abs/2502.02737 [5] Marcel Binz, Elif Akata, Matthias Bethge, Franziska BrÃ¤ndle, Fred Callaway, Julian Coda-Forno, Peter Dayan, Can Demircan, Maria K Eckstein, NoÃ©mi Ã‰ltetÅ‘, et al .2025. A foundation model to predict and capture human cognition. Nature (2025), 1â€“8. [6] Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, et al . 2024. Social-bench: Sociality evaluation of role-playing conversational agents. arXiv preprint arXiv:2403.13679 (2024). [7] Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, et al . 2024. Tombench: Benchmarking theory of mind in large language models. arXiv preprint arXiv:2402.15052 (2024). [8] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv e-prints (2024), arXivâ€“2407. [9] Enryu. 2023. twitter100m_tweets: A Large-Scale Dataset of Tweets and Users for Political and Sentiment Analysis . https://huggingface.co/datasets/enryu43/ twitter100m_tweets [10] Ã–nder GÃ¼rcan. 2024. Llm-augmented agent-based modelling for social simula-tions: Challenges and opportunities. HHAI 2024: Hybrid human AI systems for the social good (2024), 134â€“144. [11] Thilo Hagendorff, Ishita Dasgupta, Marcel Binz, Stephanie CY Chan, Andrew Lampinen, Jane X Wang, Zeynep Akata, and Eric Schulz. 2023. Machine psychol-ogy. arXiv preprint arXiv:2303.13988 (2023). [12] Luke Hewitt, Ashwini Ashokkumar, Isaias Ghezae, and Robb Willer. [n. d.]. Pre-dicting results of social science experiments using large language models (2024). 

URL https://docsend. com/view/ity6yf2dansesucf ([n. d.]). [13] Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley. 2024. Bridging Language and Items for Retrieval and Recommendation. arXiv preprint arXiv:2403.03952 (2024). [14] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al . 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [15] Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, and Yixin Zhu. 2023. Evaluating and inducing personality in pre-trained language models. Advances in Neural Information Processing Systems 36 (2023), 10622â€“ 10643. [16] Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Deb Roy, and Jad Kabbara. 2023. PersonaLLM: Investigating the ability of large language models to express personality traits. arXiv preprint arXiv:2305.02547 (2023). [17] Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024. A survey on large language models for code generation. arXiv preprint arXiv:2406.00515 

(2024). [18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Mem-ory Management for Large Language Model Serving with PagedAttention. In 

Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles .[19] ClÃ©ment Lefourrier. 2023. REDDIT_submissions: High-Quality Submissions from 50 Subreddits (2006â€“2023) . https://huggingface.co/datasets/HuggingFaceGECLM/ REDDIT_submissions [20] Kurt Lewin. 2013. Principles of topological psychology . Read Books Ltd. [21] Minzhi Li, Weiyan Shi, Caleb Ziems, and Diyi Yang. 2024. Social intelligence data infrastructure: Structuring the present and navigating the future. arXiv preprint arXiv:2403.14659 (2024). [22] Wenkai Li, Jiarui Liu, Andy Liu, Xuhui Zhou, Mona Diab, and Maarten Sap. 2024. Big5-chat: Shaping llm personalities through training on human-grounded data. 

arXiv preprint arXiv:2410.16491 (2024). [23] Yi-Long Lu, Chunhui Zhang, Jiajun Song, Lifeng Fan, and Wei Wang. 2025. Tom-rl: Reinforcement learning unlocks theory of mind in small llms. arXiv e-prints 

(2025), arXivâ€“2504. [24] Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos, Corby Rosset, et al . 2024. Agentinstruct: Toward generative teaching with agentic flows. arXiv preprint arXiv:2407.03502 (2024). [25] Xinyi Mou, Jingcong Liang, Jiayu Lin, Xinnong Zhang, Xiawei Liu, Shiyue Yang, Rong Ye, Lei Chen, Haoyu Kuang, Xuanjing Huang, et al . 2024. Agentsense: Benchmarking social intelligence of language agents through interactive scenar-ios. arXiv preprint arXiv:2410.19346 (2024). [26] Samuel J Paech. 2023. Eq-bench: An emotional intelligence benchmark for large language models. arXiv preprint arXiv:2312.06281 (2023). [27] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: Inter-national Conference for High Performance Computing, Networking, Storage and Analysis . IEEE, 1â€“16. [28] Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W Pennebaker. 2006. Effects of age and gender on blogging.. In AAAI spring symposium: Computational approaches to analyzing weblogs , Vol. 6. 199â€“205. [29] James WA Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, et al . 2024. Testing theory of mind in large language models and humans. 

Nature Human Behaviour 8, 7 (2024), 1285â€“1295. [30] Qwen Team. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671 

(2024). [31] Hongyu Wan, Jinda Zhang, Abdulaziz Arif Suria, Bingsheng Yao, Dakuo Wang, Yvonne Coady, and Mirjana Prpa. 2024. Building llm-based ai agents in social virtual reality. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems . 1â€“7. [32] Qian Wang, Zhenheng Tang, and Bingsheng He. 2025. From ChatGPT to DeepSeek: Can LLMs simulate humanity? arXiv preprint arXiv:2502.18210 (2025). [33] Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Shuchang Zhou, et al . 2025. Coser: Coordinating llm-based persona simulation of established roles. arXiv preprint arXiv:2502.09082 (2025). [34] Shitao Xiao, Zheng Liu, Peitian Zhang, and Xingrun Xing. 2023. Lm-cocktail: Resilient tuning of language models via model merging. arXiv preprint arXiv:2311.13534 (2023). [35] Yutong Xie, Zhuoheng Li, Xiyuan Wang, Yijun Pan, Qijia Liu, Xingzhi Cui, Kuang-Yu Lo, Ruoyi Gao, Xingjian Zhang, Jin Huang, et al . 2025. Be. FM: Open Foundation Models for Human Behavior. arXiv preprint arXiv:2505.23058 (2025). [36] Ruoxi Xu, Hongyu Lin, Xianpei Han, Le Sun, and Yingfei Sun. 2024. Academ-ically intelligent llms are not necessarily socially intelligent. arXiv preprint arXiv:2403.06591 (2024). [37] Zhenyu Xu, Hailin Xu, Zhouyang Lu, Yingying Zhao, Rui Zhu, Yujiang Wang, Mingzhi Dong, Yuhu Chang, Qin Lv, Robert P Dick, et al . 2024. Can large language models be good companions? An LLM-based eyewear system with conversational common ground. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 8, 2 (2024), 1â€“41. [38] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al . 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [39] Lance Ying, Katherine M Collins, Lionel Wong, Ilia Sucholutsky, Ryan Liu, Adrian Weller, Tianmin Shu, Thomas L Griffiths, and Joshua B Tenenbaum. 2025. On benchmarking human-like intelligence in machines. arXiv preprint arXiv:2502.20502 (2025). [40] Xixian Yong, Jianxun Lian, Xiaoyuan Yi, Xiao Zhou, and Xing Xie. 2025. Mo-tiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models? arXiv preprint arXiv:2506.13065 (2025). [41] Haopeng Zhang, Philip S Yu, and Jiawei Zhang. 2025. A systematic survey of text summarization: From statistical methods to large language models. Comput. Surveys 57, 11 (2025), 1â€“41. [42] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations) .Association for Computational Linguistics, Bangkok, Thailand. http://arxiv.org/ abs/2403.13372 [43] Jinfeng Zhou, Yuxuan Chen, Yihan Shi, Xuanming Zhang, Leqi Lei, Yi Feng, Zex-uan Xiong, Miao Yan, Xunzhi Wang, Yaru Cao, et al . 2025. Socialeval: Evaluating social intelligence of large language models. arXiv preprint arXiv:2506.00900 

(2025). 

## A Data Details A.1 Data Sources 

Reddit [19 ] (47.2M submissions from 50 subreddits): We use posts from active users on Reddit, a large-scale social platform organized by interest-based communities. Reddit posts capture a wide variety of personal opinions, experiences, and discussions. HumanLLM: Towards Personalized Understanding and Simulation of Human Nature KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea 

Table A1: Statistics of the Cognitive Genome Dataset at each data processing stage. 

Source Raw After DF After DS After QC Texts Texts Users Scenarios Social QA Users Scenarios Social QA Reddit 47,200,000 2,800,000 102,861 612,012 898,264 75,661 259,470 549,383 Twitter 88,100,000 673,000 75,243 393,684 766,918 54,010 178,761 456,388 Blogger 681,000 368,000 41,607 1,048,883 1,012,066 30,738 448,020 271,210 Amazon 571,540,000 1,700,000 210,000 â€“ â€“ 122,020 â€“ â€“Total 707,520,000 5,541,000 429,711 2,054,579 2,677,248 282,429 886,251 1,276,981  

> Note: â€œDFâ€: Data Filtering, â€œDSâ€: Data Synthesis, â€œQCâ€: Quality Control.

Twitter [ 9 ] (88.1M original tweets from CommonCrawl): We lever-age tweets from a broad and diverse set of users on Twitter, a global microblogging platform. Tweets offer real-time, spontaneous expressions of usersâ€™ thoughts, emotions, and daily activities. 

Blogger [28 ] (681K blogs): We utilize blog entries from the Blogger platform, where users share long-form personal stories and reflec-tions. These blogs provide rich narratives of individual experiences, opinions, and life events. 

Amazon [13 ] (571.54M reviews): We extract user purchase histories from Amazon, including item names, ratings, and reviews. These records capture real-world consumer preferences and feedback, directly linked to personal decision-making and experiences. 

## A.2 Data Statistics 

Statistics of the Cognitive Genome Dataset at each data processing stage can be found in Table A1. 

## A.3 Data Quality Control 

To ensure the quality of the synthetic data, we design the following metrics for quality control. 

Hallucination. All generated content must be faithful to the source text. Key details, events, and characterizations should be explicitly supported by the original user data or be reasonable, justifiable inferences. Minor details may be inferred if they do not alter the core narrative. This metric evaluates whether personas, profiles, stories, scenarios, and QA samples remain grounded in the main story and intent of the original records. 

Coverage. This metric assesses how well the generated con-tentâ€”whether a persona, profile, story, scenario, or QA sampleâ€” captures the salient, meaningful, and unique aspects of the original user data. Good coverage requires that the most important and distinctive features from the source are distilled and included, with minimal omission of critical information. 

Conciseness. This metric evaluates whether the generated per-sona or profile are succinct, cohesive, and free from redundancy or unnecessary repetition. 

Relevance. This metric assesses whether the generated persona or profile focus exclusively on user-relevant information, excluding generic, promotional, off-topic material from the original data. 

Fidelity. This metric evaluates whether generated scenarios are vivid, detailed, and coherent rather than generic or superficial. It also assesses the logical connection and complementarity among scenario, question, and answer, as well as the depth of contents and the preservation of emotional or personal resonance. 

Novelty. This metric assesses how interesting, distinctive, and memorable the generated content is. It measures whether QA sam-ples, profiles, stories, and scenarios highlight the userâ€™s unique personality, experiences, and perspectives, evoking specific per-sonal or social resonance rather than being formulaic or generic. 

Leakage. This metric measures the extent to which the answer is directly revealed or obvious from the scenario description. Higher scores indicate that the answer requires genuine social reasoning or inference, while lower scores suggest that the answer can be simply copied or restated from the scenario with minimal reasoning. 

Overall. This metric provides a holistic assessment of the overall quality of the generated content. 

## A.4 Human Evaluation 

During the data synthesis stage, the LLM is instructed to distill and summarize information from the original human-authored texts. In the subsequent data quality control stage, the synthesized data are further filtered using an LLM-as-judge framework to remove low-quality or inconsistent samples. To explicitly check whether the LLM introduces biases or hallucinations in the synthesized data, we conduct an additional round of human evaluation. Specifically, we stratify and sample 120 instances from the training set and ask three graduate students specializing in societal AI, none of whom are involved as co-authors, to assess the degree of hallucination and semantic plausibility. Each instance is independently evaluated by two annotators on a 1â€“5 Likert scale, where 5 indicates no hallucina-tion and high semantic plausibility. The evaluation results show an average score of 4.65 and a Krippendorffâ€™s ð›¼ of 0.65, demonstrating that our synthesized data exhibit high reliability and strong fidelity to the original human behaviors. 

## B Application Results B.1 GPT-score Results 

Full results of the five backbone models across the three applications are presented in Tables B2, B3, and B4. 

## B.2 Case Study 

Due to space limitations in the main text, we present representative cases for the profile generation and personalized writing applica-tions in this section, as shown in Figures B1 and B2, respectively. KDD â€™26, August 09â€“13, 2026, Jeju Island, Republic of Korea Yuxuan Lei et al. 

Table B2: Performance on Profile Generation Application. The best value and the second best for each column are marked in red and blue. 

Model Faith-Fulness Realism Coverage Avg. Imp. (%) Centaur 5.7295 6.2459 4.4016 5.4590 â€“BE.FM 4.7500 6.1400 3.3508 4.7469 â€“GPT-4o 7.4166 8.8858 6.4483 7.5835 â€“Phi-3-mini-128k-instruct 7.2333 8.5450 6.4858 7.4213 HumanLLM phi-3 7.7516 8.3833 6.6400 7.5916 2.30 Llama-3.1-8B-Instruct 6.6875 8.2333 5.9325 6.9511 HumanLLM Llama 7.9500 8.6166 6.8675 7.8113 12.37 Qwen3-8B 7.7616 8.8758 6.9091 7.8488 HumanLLM Qwen3 8.0258 8.7216 6.9166 7.8879 0.50 Qwen2.5-3B-Instruct 7.6966 8.4616 6.5066 7.5549 HumanLLM Qwen2.5-3B 7.7133 8.4016 6.5975 7.5707 0.21 Qwen2.5-7B-Instruct 7.5183 8.4250 6.3208 7.4213 HumanLLM Qwen2.5-7B 7.8391 8.4983 6.7500 7.6958 3.70 

Table B3: Performance on Human Explainer Application. The best value and the second best for each column are marked in red and blue. 

Model Contextual Relevance Logical Consistency Alignment Avg. Imp. (%) Centaur 4.6200 5.9300 5.1010 5.2170 â€“BE.FM 8.3000 8.0375 6.6008 7.6461 â€“GPT-4o 9.5866 9.5675 8.3516 9.1685 â€“Phi-3-mini-128k-instruct 8.9900 8.9575 7.5558 8.5011 HumanLLM phi-3 9.1125 9.0091 7.7708 8.6308 1.52 Llama-3.1-8B-Instruct 9.1633 9.1108 7.7833 8.6857 HumanLLM Llama 9.2291 9.1933 7.9608 8.7944 1.25 Qwen3-8B 9.2541 9.1983 7.8966 8.7830 HumanLLM Qwen3 9.2808 9.2241 7.9750 8.8266 0.50 Qwen2.5-3B-Instruct 8.9400 8.8041 7.4325 8.3922 HumanLLM Qwen2.5-3B 9.0375 8.9216 7.6283 8.5291 1.63 Qwen2.5-7B-Instruct 9.0533 8.9900 7.6300 8.5577 HumanLLM Qwen2.5-7B 9.1775 9.1308 7.8675 8.7252 1.96 

Table B4: Performance on Personalized Writing Applica-tion. The best value and the second best for each column are marked in red and blue. 

Model Style Match Content Relevance Content Similarity Avg. Imp. (%) Centaur 3.9270 4.9354 3.0453 3.9692 â€“BE.FM 6.4131 7.3355 5.0267 6.2584 â€“GPT-4o 8.6800 9.4091 7.6841 8.5910 â€“Phi-3-mini-128k-instruct 6.6495 7.9285 5.6941 6.7573 HumanLLM phi-3 7.2321 8.2500 6.5066 7.3295 8.46 Llama-3.1-8B-Instruct 7.2792 8.3333 6.3246 7.3123 HumanLLM Llama 7.6822 8.6281 6.9701 7.7601 6.12 Qwen3-8B 7.5306 8.5766 6.6901 7.5991 HumanLLM Qwen3 7.6257 8.5552 6.9417 7.7075 1.43 Qwen2.5-3B-Instruct 6.9751 8.0173 5.9205 6.9709 HumanLLM Qwen2.5-3B 7.2890 8.2444 6.4826 7.3386 5.28 Qwen2.5-7B-Instruct 7.1111 8.1333 6.0246 7.0896 HumanLLM Qwen2.5-7B 7.4382 8.3740 6.6259 7.4793 5.50 The user is a history enthusiast, who is deeply interested in ancient civilizations, 

particularly those of Europe and the Mediterranean, with a curious and inquiring mind, 

seeking knowledge and understanding of historical events and cultural interactions 

User Persona 

The user is a history enthusiast â€¦, particularly those of Europe and the Mediterranean, 

as evident from their queries about the Anglo -Saxon kingdoms, Carthage, the Roman 

Empire, and Ancient Greece. â€¦ often seeking recommendations for books and 

sources to deepen their understanding of historical topics, such as the factors of life 

in the British Isles during the Anglo -Saxon period or the military tactics of Carthage. â€¦, 

they pose thoughtful questions, like whether the Romans made contact with Nordic 

people or how Ancient Greece and Rome dealt with homicide, demonstrating a desire 

to learn and understand the complexities of the past. â€¦ they have developed a 

fascination with the cultural, social, and political aspects of ancient societies, and 

their inquiries reflect a nuanced and multidisciplinary approach to historical inquiry. 

Reference Profile 

This user is a 45 -year -old history enthusiast with a particular fascination for ancient 

civilizations, especially those in Europe and the Mediterranean. Alex holds a Masterâ€˜s 

degree in Archaeology and often participates in online forums and discussion groups 

dedicated to ancient history. They often spend weekends visiting museums and 

historical sites, immersing themselves in exhibits and detailed descriptions of 

artifacts. ... Professionally, Alex works as a software engineer â€¦ 

Qwen -2.5 -7B 

The user is a history enthusiast with a deep fascination for ancient civilizations, 

particularly those of Europe and the Mediterranean, as evidenced by their numerous 

questions about topics such as Roman imperialism, Greek city -states, and the 

interactions between different cultures. â€¦ Their curiosity extends to the cultural and 

social aspects of ancient societies, including the role of women, the lives of slaves, and 

the impact of technological advancements. The user's inquiries also reveal a desire to 

explore the broader context of historical events, such as the development of different 

writing systems and the exchange of ideas between cultures. With a focus on primary 

sources and archaeological evidence, the user seeks to gain a more nuanced 

understanding of the past and its relevance to the present. 

HumanLLM 

Figure B1: Case Study for Profile Generation Application. Swiffer WetJet Starter Kit, Includes: 1 Power Mop, 5 Pads, Solution, Batteries, 6 Piece Set 

The user replaced their product after a few years due to failure, having initially 

purchased it during a drought to reduce water use for mopping. It provided light surface 

cleaning but struggled with deep cleaning in bathrooms due to a rigid head that couldnâ€™t 

navigate tile grout, leading them to keep it for quick cleanups, and was replaced every 

five years as it is affordable despite not being designed for indefinite use. 

Target Item Name 

Topic 

I am getting a replacement. I had the first one for a few years, and I initially purchased it 

because we were experiencing a drought and did not want to use water to mop floors. 

This product has helped keep things semi -clean. However, it generally only performs 

light surface cleaning. I also found that the flat plastic head is too rigid, which prevents 

it from navigating tile dips, and it fails to reach down between the tiles to the grout. As a 

result, in the bathroom, it is not effective for thorough bacterial cleaning. Nevertheless, 

it is better than nothing. I keep it in the bathroom because we have male housemates 

who can quickly grab it and wipe up after using the toilet. The one I am replacing is a few 

years old and has stopped spraying, as it is no longer able to dispense liquid onto the 

floor - and yes, I did change the batteries. It is not designed to last forever, but it is not 

so expensive that I cannot afford to replace it every five years or so. 

Reference Review 

I figured it was time for a new mop after a few years. The old one was handy for quick 

cleanups, but it struggled with deep cleaning, especially in the bathroom where grout 

is a pain. I remember when we had a drought and I switched to this Swiffer WetJet to 

save on water. Itâ€™s a nice idea, really light on your hands and easy to use, but itâ€™s not 

built for indefinite use. The rigid head just canâ€™t navigate the tiny spaces in tile grout. I 

still use a traditional mop for those spots, but for the quick cleanups, this is great. Itâ€™s 

affordable and does the job, even if itâ€™s not the most robust. The battery life is decent, 

but since itâ€™s rechargeable, thatâ€™s a plus. Overall, itâ€™s a good tool for what it is, but 

donâ€™t expect it to last you a lifetime. 

Qwen -2.5 -7B 

I had the old one for a few years, and it worked well for light surface cleaning. I bought it 

during a drought, so I was happy not having to use water to mop. However, I did find that 

it didnâ€˜t do a great job of deep cleaning, especially in the bathrooms. The head was too 

rigid to navigate the grout between the tiles. I also found that it left a sticky residue on 

the floor that was hard to remove. It was okay for quick cleanups, such as picking up pet 

hair, but for actual mopping, I wasnâ€™t satisfied with the results. I replaced it after about 

five years since it wasnâ€˜t designed to last forever and was affordable enough to simply 

replace when it stopped working. 

HumanLLM 

Figure B2: Case Study for Personalized Writing Application.