Title: ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models

URL Source: https://arxiv.org/pdf/2601.15812v1

Published Time: Fri, 23 Jan 2026 01:36:01 GMT

Number of Pages: 39

Markdown Content:
# ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Shir Ashury-Tahan 1 Yifan Mai 2 Elron Bandel 1 Michal Shmueli-Scheuer 1 * Leshem Choshen 1 3 * 

# Abstract 

Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. Awrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model im-provement. We introduce ErrorMap , the first method to chart the sources of LLM failure. It extracts a model’s unique “failure signature”, clar-ifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns bench-mark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate 

ErrorAtlas , a taxonomy of model errors, re-vealing recurring failure patterns. ErrorAtlas 

highlights error types that are currently under-explored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap 

and ErrorAtlas enable advanced evaluation — one that exposes hidden weaknesses and di-rects progress. Unlike success, typically mea-sured by task-level metrics, our approach intro-duces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available 1,with plans to periodically update ErrorAtlas 

as new benchmarks and models emerge.  

> *

Equal supervision. 1IBM Research 2Stanford University 3MIT. Correspondence to: Shir Ashury-Tahan <shir.ashury.tahan@ibm.com >, Leshem Choshen 

<leshem.choshen@ibm.com >.

Preprint. January 23, 2026. 

> 1

https://github.com/IBM/ErrorMap 

“It is possible to fail in many ways. . . while to succeed is possible only in one way. ”

Aristotle, Nicomachean Ethics , Book II, ∼320BC 

# 1. Introduction 

Benchmarking plays a central role in advancing large lan-guage models (LLMs), offering a standard bottom-line score to assert progress (Biderman et al., 2024). This abstraction eases proving a model’s success or its overall superiority, but it also obscures the nature and origin of model errors, complicating skill comparisons and hindering efforts to di-agnose limitations or guide improvements. In response to these limitations, there is growing interest in developing more interpretable and diagnostic evaluation frameworks (Maimon et al., 2025; Zeng et al., 2025; Tjuatja & Neubig, 2025) or in highlighting specific errors models make (Mukherjee et al., 2025; Pan et al., 2025; Li et al., 2024; Honovich et al., 2022; Kryscinski et al., 2019). While current diagnostic methods offer valuable insights, their analysis primarily relies on the challenges posed by the input (e.g., counting failures on differential equations ques-tions). However, where success on a challenge necessarily proves competence, a failure can have many causes (e.g., misunderstanding the question, miscalculation or applying a wrong axiom). Moreover, benchmark examples themselves may introduce ambiguity or error, further complicating eval-uation, especially when the model’s answer is not consid-ered. Ultimately, pinpointing the cause of failure requires analyzing both the input, including the question and the instruction, and the resulting answer. We introduce ErrorMap (§2) to address this gap. 

ErrorMap offers a model-oriented error analysis, high-lighting why models fail. The method transforms raw lan-guage model failures into a structured, interpretable taxon-omy in natural language. To do so, ErrorMap follows a pipeline, first profiling the issue underlying each failure, then recursively generates taxonomy categories through an iterative refinement process, and finally applies the con-structed taxonomy to all failures (see Figure 1). As our analysis requires simple unstructured text, it applies seamlessly to any language model and domain. Overall, 1  

> arXiv:2601.15812v1 [cs.AI] 22 Jan 2026 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models

Per -Instance Error Analysis 

Final Taxonomy 

> Error
> Categorization
> Error
> Assignment

1 2   

> 1) Unit Conversion
> Error
> 2) Incorrect
> Assumption
> 100) Incorrect
> Formula
> Wrong
> Predictions
> Analyzed
> Errors

Taxonomy Hierarchy Building     

> a. Calculation Errors :
> [1, 100, … ]
> b. Assumpion Error:
> [2, … ]
> c. Refusal Errors:
> […]
> …
> Recursion: the procedure is repeated for every
> category (e.g., the calculation errors group)
> Calculation Errors
> Assumption Errors
> Refusal Errors
> Figure 1. ErrorMap Schema: Two Method Stages. The first stage performs a one-time analysis of each incorrect prediction at the instance level. The second stage constructs a layered taxonomy through recursive iterations, based on either the maximum grouping depth or a defined layer threshold.

ErrorMap provides a flexible way to analyze a practi-tioner’s specific setting or to compare the failure fingerprint of several models or datasets. While a dynamic taxonomy that would fit an ad hoc analysis is best for many needs, a stable taxonomy simplifies comparisons across time and replicability. Applying ErrorMap to 83 models and 35 datasets, we release ErrorAtlas – a taxonomy of current LLM fail-ures (see §3). ErrorAtlas is a comprehensive and static taxonomy of model errors designed to facilitate cross-field comparisons, enhance efficiency, and ensure replicability. We present several findings on common model failures, which both stand on their own and highlight the effective-ness of our methods. Applying the ErrorAtlas taxon-omy to current models and datasets we find issues that are prevalent but understudied (§4). For example, models of-ten misinterpret the question’s intent or provide incomplete answers. Beyond ErrorAtlas , ErrorMap can support practition-ers throughout the LLM lifecycle, from development and fine-tuning to evaluation and benchmarking. We provide in §5 two test cases, a model developer diagnosing the differ-ences between two versions of Gemini (Team et al., 2024) and a benchmark curator testing MMLU-pro (Wang et al., 2024a). Furthermore, in §6, we validate that the stages involved in extracting the error analysis are accurate, robust and cover well the errors models make. This work advances our ability to understand and analyze model behavior at a deeper level. Our contributions are as follows: 1. We introduce ErrorMap , an LLM-based technique to generate a dedicated taxonomy of LLM errors. It enables analysis across a diverse set of domains, input formats and model comparisons. 2. We present ErrorAtlas , a static taxonomy of model errors, generated using ErrorMap . It captures com-mon failure modes across benchmarks and models. These errors reflect underlying limitations in model be-havior, supporting meaningful and interpretable com-parisons of model weaknesses. 3. We provide analysis across a large number of models and datasets. Finding common errors that are under-studied. Moreover, we find model versions, types, and families exhibit distinct error patterns, allowing for nuanced behavioral profiling and more targeted evalua-tion. 4. We demonstrate the applicability of both ErrorMap 

and ErrorAtlas for nuanced model comparison, benchmark analysis, and model debugging. 5. We publicly release the code, taxonomy and associated data. 

# 2. ErrorMap 

Our technique targets a common scenario: evaluating mul-tiple models on the same dataset examples, as typically 2ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

done in benchmark runs, or even reusing a benchmark run for deeper analysis. It leverages all available data in the benchmark, including inputs, reference answers, and model outputs, and produces comparative insights across models. The process is unsupervised and consists of two stages. First, incorrect predictions are analyzed on a per-example basis to form the leaf nodes of the taxonomy, creating the ini-tial layer. Second, the taxonomy is constructed top-down through recursive process. The method schema overview is shown in Figure 1, and additional details, including the specific prompts used, are provided in Appendix A. 

Stage 1: Per-Instance Error Analysis Our goal at this stage is to create a structured summary of the resulted errors and provide interpretable analysis of it. To achieve this, we task an analyst LLM with performing a detailed, structured analysis for each incorrect prediction. This includes eval-uating a list of criteria with associated features, providing a summary of the failure, and assigning a short label for it. To support the judge’s evaluation, we provide the following information: the original instance, any available references and multiple Informative Correct Predictions (ICPs) if avail-able, i.e., correct predictions made by other models in the benchmark. ICPs have proven useful (Zelikman et al., 2022; LI et al., 2022; Creswell & Shanahan, 2022). In this context, they act as rich reference points that often approximate full solutions, helping judges compare correct and incorrect out-puts rather than diagnose root causes. This is particularly valuable when no gold reference exists or when the gold standard is limited to a final answer (e.g., in classification tasks). The judge is asked to construct a structured solution to the instance (see prompt in Appendix A.1.1.) The structure has several components, all of which the judge should fill. The judge is asked to break down the solution and specify 

criteria ; steps, evidence, or assumptions required to reach a correct answer, for example relying on formulas, a list of reasoning steps of extracting multiple facts to deduce and answer. For each criterion, the judge should assess its presence, quality, supporting evidence (a quote from the prediction), and may add comments if there is something additional to note about this criterion. Grounded in the step-by-step analysis, the judge should identify the first major error that caused the prediction to fail and create both a 

summary of a few sentences and an informative label that highlights the failed skill. We focus on the first major error because it often sets the trajectory for the rest of the reason-ing; once an initial mistake is made, subsequent steps are likely to be flawed as well. This label is a phrasal description of the identified error. Finally, the judge outputs a JSON object that includes the necessary detailed criteria, along with the error summary and label. Note that, while only the error label is used in the next stage, the criteria and summary are helpful for interpreting each specific wrong prediction. 

Stage 2: Taxonomy Hierarchy Building This stage orga-nizes the errors analyzed in the previous step into a multi-level taxonomy that enables systematic navigation across different resolution paths. The hierarchy is constructed in a top-down manner through iterative cycles of two substages: (a) extracting categories from the analyzed errors (Stage 2.a) and (b) classifying errors into these categories (Stage 2.b). The initial iteration considers all analyzed errors; thereafter, for each category produced in the preceding Stage 2.a, child categories are derived based on the errors assigned to it in the corresponding Stage 2.b. This process can continue until no further meaningful subdivisions can be identified, but it may also be stopped earlier. 

Substage 2.a: Error Categorization This stage consolidates all or selected instance-level results from the previous analy-sis into a set of common error types by iteratively grouping unique error labels into broader categories. Each category is assigned a clear description to minimize ambiguity. To construct the categories, we adopted the data mining approach proposed by Wan et al. (2024), which iteratively employs an LLM to generate categories from input data, in our case, from the unique error labels and their prevalence. We summarize its 3 stages below (c.f., Wan et al., 2024), and provide in Appendix their prompts (§A.1.2, §A.1.3, §A.1.4) and configuration (Table 4). 1. Category Generation – The initial stage, where the LLM receives the first batch (a list of error labels with their frequencies) and generates categories and category de-scriptions based on it. Note that since error labels from stage 1 were created in free-form, label repetitions were not guaranteed, though we observed frequent overlaps. 2. Iterative Refinement – Multiple iterations (depending on data size), where the LLM receives the previously generated categories along with a new sampled batch and incrementally updates and improves the categories. 3. Final Review – A concluding iteration where the LLM reviews the final taxonomy to ensure coherence and compliance with the instructions (e.g., no ambiguity). The output of this stage is a finalized list of categories, each accompanied by a clear description, derived from the corresponding group of instance-level errors. 

Substage 2.b: Error Assignment This stage integrates the instance-level analysis performed in the previous step and assigns each error, along with its criteria analysis, summary, and label to the most appropriate category, based on the classification of its error label. This integration is done using a simple batched LLM call (the prompt is provided in App. §A.1.5). We provide the model with the error categories and a batch of error labels (see parameters in Appendix, Table 4), and ask the model to 3ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

assign each error to the most appropriate category. The outcome of this stage is a set of categories, each popu-lated with instance-level errors relevant to it. 

Final Outcome The final outcome of this stage is a layered taxonomy, where multiple category levels each contain a list of errors grouped by their respective labels. 

# 3. Constructing ErrorAtlas 

Where ErrorMap supplies a flexible way to acquire a dedi-cated taxonomy for a nuanced issue, such as specific models or task data, a static taxonomy is often preferred in cases where the replicability and broad comparisons are required. 

ErrorAtlas is built to accommodate such error analysis use-cases. We describe ErrorAtlas , a taxonomy that categorizes failure modes commonly shown by current pop-ular models, built using gpt-oss-120b . We detail the process of constructing ErrorAtlas , including practical decisions made, such as the identity of the datasets. We refer to experimental details that are general to all our ex-periments, from building the taxonomy to validating it in Appendix B. 

Coverage To create ErrorAtlas , we select a diverse group of 35 datasets spanning a wide range of tasks, do-mains, and capabilities, and extract an ErrorMap taxon-omy across all available model predictions. In total, we include predictions from 83 models. We cover the scope of LLM evaluations with the following benchmarks: from HELM leaderboards (Liang et al., 2023), Capabilities for general capabilities (Xu et al., 2024), MedHELM for medi-cal domain (Bedi et al., 2025), and ToRR for tables (Ashury-Tahan et al., 2025), and BFCL-v4 (Patil et al., 2025) for func-tion calling, and for code HumanEval (Chen et al., 2021), HumanEval Plus (Liu et al., 2023b), MBPP (Austin et al., 2021) and MBPP Plus (Liu et al., 2023b). For both ToRR and MedHELM, we selected partial subsets of the datasets they contain. 2

Scaling ErrorMap through Sampling The flexible na-ture of ErrorMap allows it to be applied to any number of models, datasets, and incorrect predictions. However, constructing ErrorAtlas , a unified taxonomy across all mentioned benchmarks, is far more demanding than apply-ing ErrorMap in narrow settings (e.g., a single benchmark or a small set of models). The number of errors to analyze scales with the dataset sizes and number of models, resulting in significant computational demands, a common obstacle   

> 2We focused on tasks where model outputs include inter-pretable content, as ErrorMap goal is to analyze predictions that reveal failure modes. Many benchmark tasks, like classifica-tion or entity extraction do not provide explanations (or CoT) and lack the necessary context for such analysis. The full details of selected datasets are provided in App. Table 5.

for large scale evaluations (Perlitz et al., 2024). To manage the data volume, we employed relative sampling: for each model-dataset pair, we sampled approximately 10% of the instances where the model was evaluated as having failed, i.e., a proportionate subset based on the model’s error rate. This resulted in a sample of approximately 7,000 failures that was then used to run ErrorMap . Interestingly, despite the limited sample size and its variability, the categories in the resulting taxonomy were unique, and no overlaps were observed. This may suggest that the iterative refinement ef-fectively consolidates similar error types and is not sensitive to sample size. 

Usage Now that we have extracted ErrorAtlas , its pri-mary value is in clearly surfacing common LLM error types (see §4). This can support future model develop-ment and real-world improvements, particularly as we un-cover previously unreported error categories. Moreover, 

ErrorAtlas categories can be practically applied at low cost to reflect general model failure modes. This can be done by running only Stage 1 and Stage 2.b, while skipping Stage 2.a (Error Categorization). 3

# 4. ErrorAtlas Applicability 

ErrorAtlas Reveals the Error Topography of Models 

Running ErrorMap on 35 datasets results in the construc-tion of ErrorAtlas (see §3). The main outcome is a set of 17 high-level taxonomy categories describing common model errors, presented in Table 1. ErrorAtlas statis-tics are available in App. Table 6, and examples of error categories with their subcategories can be found in App. Table 8. The resulting error categories span a wide spectrum, re-flecting diverse dimensions of model performance. These include reasoning-related errors, such as Logical Reasoning Errors, Incorrect Identification, Specification Misinterpre-tation; instruction-following issues, including Missing Re-quired Element, Irrelevant/Extraneous Content; procedural errors, such as Computation Error, Counting/Enumeration Error, Unit Conversion Error; and technical and linguistic issues, including Output Formatting Error, Inappropriate Refusal, Tool/API Usage Error. Additionally, there are cat-egories that fall outside these dimensions, such as False Positive Detection. While the areas of failure described above (e.g., instruc-tion following and reasoning) are generally well-known and researched within the community, ErrorAtlas enables the identification of more precise weaknesses within these broader categories. Moreover, the resulted taxonomy un-

> 3Running stage 2.a on specific data may be less representative from a model’s general failure mode perspective, as it depends on data collection that may be biased.

4ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Category Description Logical Reasoning Error Fails in logical inference, deduction, or applying correct reasoning steps. Missing Required Element Omits mandatory sections, fields, identifiers, or other specified content. Computation Error Produces incorrect numerical, algebraic, or geometric results, including miscalculations and faulty derivations. Incorrect Identification Mislabels or misidentifies objects, concepts, attributes, or entities. Specification Misinterpretation Misunderstands task requirements, output type, or provides incorrectly formatted parameters and inputs. Output Formatting Error Violates required structure, markup, punctuation, case, or other formatting rules. Irrelevant/Extraneous Content Generates off-topic, unrelated, or unnecessary information beyond the required answer. Counting/Enumeration Error Over-counts, under-counts, double-counts, or omits cases in combinatorial reasoning. Answer Selection Error Chooses wrong answer option, label, or maps solution to an incorrect choice. Incomplete Reasoning Lacks essential explanation, proof steps, or logical justification. Factual Error Provides inaccurate or fabricated factual information or domain knowledge. Tool/API Usage Error Missing, wrong, or extraneous tool/API calls; misuse of tool capabilities. Naming/Symbol Error Incorrect function/variable names, symbols, or identifier usage. Inappropriate Refusal Unjustified or overly restrictive refusal to answer the query. Unit Conversion Error Incorrect conversion between units, percentages, temperature, or similar measures. False Positive Detection Incorrectly flags an error or anomaly that does not actually exist. Error Detection Failure Fails to recognize existing mistakes, leaving errors undetected. 

Table 1. ErrorAtlas : High-level error categories and category descriptions. The categories are sorted most to least prevalent. 

Figure 2. Error distributions for models across the 10 most preva-lent ErrorAtlas error categories on the Capabilities benchmark. The total number of errors for each model appears at the center of its corresponding donuts chart. Each error count represents roughly 10% of the model’s total errors on Capabilities. 

derscores a key limitation of benchmark scores: although benchmarks provide useful indicators of model performance on specific tasks or domains, they often lack the granularity required to uncover detailed failure patterns. Understanding these patterns is essential for diagnosing concrete limitations in model behavior and guiding targeted improvements. 

Surfacing Frequent but Overlooked Model Failures 

While some error types are more actively studied, such as reasoning errors (Zheng et al., 2025; Xu et al., 2025; Liu et al., 2023a) and hallucinations (Cattan et al., 2025; Zhao et al., 2024) and others can be mitigated through tech-niques like tool use (e.g., resolving calculation mistakes), 

ErrorAtlas reveals additional error patterns that have received limited attention in the community, despite their prevalence not justifying such disproportionate neglect. One such pattern, is labeled in ErrorAtlas as Missing Required Element . Surprisingly, despite being a prevalent er-ror in our analysis, this error type is under-discussed 4. Upon manual inspection of the results, we found that this pattern usually involves missing details with respect to the context, such as not fully answering the question, omitting specific nuances requested, or ignoring certain instructions and con-straints. Examples provided in Appendix C.1.1 show such cases where the model produces a partially correct solution. This tendency to overlook contextual cues can significantly impact the reliability of AI systems. For instance, while a set of symptoms may typically suggest a particular diagnosis, subtle nuances in a specific case could point to a completely different one. Another error shown in Tables 1,6, with notable prevalence across datasets, is Specification Misinterpretation . Examin-ing specific instances in this category reveals various cases where models fail to adequately consider context or respond with the required expertise. These include instances of mis-alignment between surface cues and deeper context, as well as failures to interpret the information provided in the con-text (see examples in Appendix C.1.2). This underscores the need for improved contextual understanding in model development, particularly for nuance-sensitive tasks. 

> 4

Similar issues were hardly mentioned in related work search and existing taxonomies, with the exception or retrieval literature. 

5ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models Use Case Persona Goal Example 

Model Debugging Model Developer Identify regressions and behavioral changes Compare model versions (e.g., v1 vs. v2) to detect reductions in specific error types, such as reasoning failures, especially when targeting improvements in those areas Benchmark Analysis Benchmark Creator Reveal model capabilities , pro-vide error distributions and de-bug dataset validity Run ErrorMap on benchmark results to char-acterize model error distribution and error types across tasks Model Selection Product Team Choose the most suitable model for deployment Select the best model on domain-specific tasks based on stakeholder preferences (e.g., priori-tizing fewer hallucinations in medical applica-tions). Domain-Specific Evaluation Domain Expert / Analyst Identify failure modes in special-ized contexts Use ErrorMap to analyze model responses in high-stakes domains (e.g., legal, medical) and surface common failure patterns 

Table 2. Summary of key use cases using ErrorMap .4 2 0 2 4 6 8 10 12 

> Difference in Number of Errors
> Computation Error
> Incomplete Reasoning
> Specification Misinterpretation
> Answer Selection Error
> Incorrect Identification
> Error Detection Failure
> Other
> Logical Reasoning Error
> Output Formatting Error
> Inappropriate Refusal
> Naming/Symbol Error
> Tool/API Usage Error
> Factual Error
> Counting/Enumeration Error
> Missing Required Element

Figure 3. Differences in number of errors per category between 

Gemini 1.5 flash and Pro on the capabilities benchmark in HELM. X-axis represents the change in error frequency, highlight-ing areas of improvement or regression. 

Error Patterns Vary Between Models. 

Employing ErrorAtlas , we observe distinct model-specific patterns that reveal nuanced variations in error be-havior. To quantify these differences, we analyzed the er-ror distributions of models. To ensure a fair comparison, we selected models that appear in the same benchmark, HELM Capabilities, and measured their error distributions within it. Figure 2 illustrates the top 10 error categories across several models. Notably, all models tend to make errors related to incomplete content, with Gemini 2.0 Flash Lite exhibiting the highest frequency in this cate-gory, while showing the fewest errors in incorrect format-ting. Claude 3.5 Haiku shows a higher tendency to-ward logical reasoning errros compared to others, whereas 

Mixtral 8x22b Instruct v0.1 is more prone to computation errors. 

Model Error Does not Always Reflect Failure in the Benchmark Targeted Skill. We motivated our reliance on model outputs by the discrepancy between what a ques-tion aims to test and what eventually trips the model. We indeed find such cases in our analysis. For example, con-sider capability-focused datasets like MMLU-Pro, Omni-MATH, and GPQA, which are considered challenging due to their reasoning demands. While Omni-MATH empha-sizes math reasoning, GPQA focuses on general reasoning, and MMLU-Pro primarily tests knowledge along with a reasoning depth. However, approximately 44% of model errors in these benchmarks have a weak reasoning orienta-tion, and seem more technical challenges, e.g., computation error, missing required element, or counting error (see App. Table 7). Overall, we have shown the usefulness of ErrorAtlas for comparing model providers, models, and gaining insight in a new domain. In §5, we discuss the cases where a dedicated taxonomy is helpful and showcase it. 

# 5. ErrorMap Applicability 

In Section 4 we saw ErrorAtlas enables evaluations that help track model weaknesses and monitor progress over time when rerun. However, many diagnostic users are inter-ested in their specific failure modes rather than general ones. Table 2 summarizes five key use cases, each illustrating how 

ErrorMap can aid decision-making, debugging, and eval-uation in various contexts. In this section we demonstrate the first and second use-cases, aimed at supporting model developers and benchmark persona. The experimental setup is provided in App. B. 

5.1. Model Developers 

For model developers, ErrorMap provides a structured way to assess behavioral changes between iterations (similar to a behavioral model-diff; Mishra-Sharma et al., 2025; 6ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models       

> ErrorMap Categories MMLU-Pro Paper Categories
> Logical Reasoning Error (44%) Reasoning Errors (39%) Mathematical Mistake (24%) Calculation Errors (12%) Incomplete Answer (13%) Factual Error (12%) Lack of Specific Knowledge (35%) Prompt Misinterpretation (5%) Question Understanding Errors (4%) Other (10%)

Table 3. Comparison of MMLU-Pro error categories and GPT-4o distribution between manual annotations from the original paper and our method. 

Lindsey et al., 2024; Aranguri & McGrath, 2025). For example, it can help answer questions like: What common errors in my setting did my improved version address? or Did integration with external tools reduce hallucinations? By surfacing such differences, ErrorMap supports more informed and targeted improvements. To test this, we compared Gemini 1.5 flash and 

Gemini 1.5 pro using capabilities benchmark data from HELM. While the pro version outperforms the flash version by a mean score of 4.8% on the benchmark, one may wonder what are the differences between them. Our analysis (shown in Figure 3) presents the differences in number of errors. It is evident that the pro model performs significantly less computation and incomplete reasoning errors. A model developer can use such analysis to determine whether the changes made in the pro version were focused on that as-pect, or are those unexpected changes that call for more developmental efforts. We also note that this analysis can be valuable for other model stakeholders, helping them make more informed deci-sions . For example, if one has the budget to utilize Gemini models but seeks to optimize costs, this evaluation can serve as a valuable guide. In scenarios where the task involves calculations the pro version may be preferable due to its en-hanced capabilities. Conversely, for tasks primarily focused on generation, where such precision is less critical, paying more for the pro version might not be cost-effective. 

5.2. Benchmark Creators 

Diagnosis is also important for benchmark curators to vali-date what key challenges it poses for models and highlight unexpected errors. To demonstrate usability, we use MMLU-Pro as a case study and apply ErrorMap to generate its taxonomy, demonstrating several key capabilities of our approach: 

ErrorMap closely approximates manual analysis in MMLU-Pro. Running ErrorMap on MMLU-Pro dataset provided us with 5 error categories. We compared the man-ual analysis the paper reported for GPT-4o with ours and got a similar error distribution (see Table 3), with the ex-ception of two categories in ErrorMap that map to one manual one and no “other” category. 0 20 40 60 80 100 

> Num. of Model Errors
> Logical Reasoning Error
> Mathematical Mistake
> Incomplete Answer
> Factual Error
> Prompt Misinterpretation
> *
> *
> *
> *
> *
> *
> *
> *
> *
> *
> *
> deepseek-ai/deepseek-v3
> google/gemini-1.5-pro-002
> openai/gpt-4o-2024-11-20

Figure 4. Differences in error category distributions among three leading models on the MMLU-Pro dataset. Asterisks (*) indicate bars compared in statistical significance test (see App. Table 9) 

Comparing Dataset Parts for Richer Interpretive In-sights While ErrorMap separates analysis bottom up by the errors, integrating high-level data dimensions can yield more nuanced results. To demonstrate this, we analyzed model errors in relation to the domain categories of the dataset, as shown in App. Figure 5. Some patterns appear intuitive, for example, mathematics and physics exhibit sim-ilar error distributions. However, other findings are less expected, such as the disproportionately high number of factual errors in the health domain, even exceeding those in history. We further note that benchmark users can benefit from the model comparisons presented in benchmarks to distinguish between models in a more granular fashion. To demonstrate this, we compare three models on their MMLU-Pro error distributions in Figure 4. For instance, we find that GPT-4o 

exhibits a higher proportion of reasoning errors compared to Gemini 1.5 Pro , which, in contrast, makes signifi-cantly more factual errors. In conclusion, ErrorMap enhances understanding of benchmark datasets beyond overall metrics. By revealing task-specific insights and taxonomy, it helps users interpret model behavior and complements leaderboard reporting with deeper context. 

# 6. Validating ErrorMap 

In §5, we previously demonstrated the utility of ErrorMap 

outcome; we now evaluate whether it functions as expected. The key contribution of this work is the resulting taxonomy. We present in this section multiple measures of it and at-tach an additional evaluation of the first stage (per-instance error analysis) and extra details in Appendix E. The eval-uations include mainly quantitative ones, conducted using 

Qwen2.5-72B-Instruct as a meta-judge, along with qualitative ones performed manually. 7ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Building on prior work (Wan et al., 2024; Shah et al., 2023), we evaluate the taxonomy using three main criteria: cov-erage , how comprehensively the taxonomy captures error types; accuracy , how reliably it categorizes them, and use-fulness , how well it aligns with the intended application. Usefulness is central to the taxonomy’s practical value, re-flecting its support for downstream tasks; we therefore dedi-cate Section §5 to this aspect. Additionally, we introduce robustness as a fourth criterion, motivated by concerns in the literature about the reliability of LLM-based evaluations (Mizrahi et al., 2024; Siska et al., 2024; Lior et al., 2025). Robustness measures how stable the taxonomy remains un-der varying configuration setups. 

Coverage The coverage is evaluated using the approach proposed by Wan et al. (2024). For each sampled error instance, we attempt to automatically map it into the tax-onomy. If no suitable category is found, we assign it to an “other” category. We further added “hard to analyze” category for cases where there is not enough information to analyze the example. As shown in Table 6, only 1 example was classified into the “other” category, and 48 were classi-fied into the “hard to analyze”. We further include rare or uninformative categories, those not part of ErrorAtlas ,as part of this group, totaling 295 errors. We analyzed 7,049 

wrong predictions, and obtained a coverage score of 95.2% .

Accuracy We measure taxonomy accuracy using the ap-proach of Wan et al. (2024). In each evaluation round, we sample an error instance from the taxonomy and present it to a judge model. The judge is given the instance’s assigned label from the taxonomy, along with an alternative negative label from it at random. Based on this information, the judge is asked to determine which label better fits the error. The results of this evaluation are shown in App. Table 10 and indicate a high level of agreement with the assigned labels, averaging an accuracy score of 92% .

Robustness We evaluated the robustness of the taxon-omy construction process by manually comparing the origi-nal taxonomy against two configurations: an upper bound (larger sample and prompt variations) and a lower bound (smaller sample and smaller model). Overall, results sug-gest that our sampling strategy and approach are robust, and different models can capture common error patterns without significant gaps. More details can be found in Appendix E.2. 

# 7. Related Work 

While various works studied common errors in a specific setups, such as errors within a particular subdomain (Dou et al., 2024; Wang et al., 2024b; Ramprasad et al., 2024; Deshpande et al., 2025) or errors specific to a single model (Yehudai et al., 2025), or have defined challenges through the lens of question difficulty (Bradley, 2024; Baldock et al., 2021; Hacohen et al., 2020; Choshen et al., 2022; Habba et al., 2025), we are aware of no work that presents a general LLM error taxonomy or explores the use of global model er-ror signals. Although taxonomy construction can be difficult in domains where phrase meanings are ambiguous (Giglou et al., 2023; Funk et al., 2023; Ashury-Tahan et al., 2024), our setting is closer to data mining, where data is organized according to semantic relationships. Moreover, taxonomies based on latent skills were used in contexts such as scaling laws (Polo et al., 2024) and self-specialization (Kang et al., 2023) to find, possibly non-interpretable, dimensions that describe shared model performance. Maimon et al. (2025) utilize latent skills for diagnosis, and create a static ded-icated leaderboard to act as an IQ test for LLMs. Some manual efforts split input data to highlight instances posing a shared challenge (e.g., Magnusson et al., 2023), and others automate such practices (Choshen & Abend, 2019; Tjuatja & Neubig, 2025). Other works extract what each input tests to analyze model outputs (Zeng et al., 2025) or predict their skills (Zhou et al., 2025; Ruan et al., 2024; Polo et al., 2024). We see great value in such works. While we explain the benefits of analyzing actual errors and relying on model outputs rather than the challenges in the inputs, we believe these works to be useful for different needs. We also note that a recurring part driving the decisions made in ErrorMap is efficiency. Currently, as it runs only on failed examples and batches it takes a similar amount of inference as the evaluation, in ErrorAtlas we also sample. A large body of works suggested ways of efficient sampling for evaluation, they also consistently find that for most cases a random sample is a strong baseline and most alternatives introduce a bias (Choshen et al., 2024; Zhuang et al., 2025; Wang et al., 2025; Polo et al., a;b; Maia Polo et al., 2024; Perlitz et al., 2024). Since none of those methods aim for a distribution of errors and analysis, we sampled randomly. 

# 8. Conclusions 

In this work, we presented ErrorMap , a diagnosis method, to efficiently produce a summary of the errors mod-els perform on a given benchmark, enabling more inter-pretable comparisons between models. We also introduced 

ErrorAtlas , a general taxonomy emphasizing current LLM errors, contributing to a deeper understanding of their behaviors. While obtaining a complete picture is an inher-ently challenging goal, ErrorAtlas offers a first glimpse into current model errors, based on a diverse set of bench-marks spanning multiple domains and skills. Our approach lays the foundation for more detailed model evaluation. We refer to the limitations of our work to App. F. 8ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

# Impact Statement 

We see diagnosis work as a net good. There are little ob-vious harms from better understanding where systems that we currently develop fail, except in aiding development of systems that shouldn’t be developed in the same way it aids regular development. Importantly, our method supports re-sponsible development, deployment, and auditing of models (see Table 2, particularly the domain expert persona). 

# References 

Aranguri, S. and McGrath, T. Discovering unde-sired rare behaviors via model diff amplification. 

Goodfire Research , 2025. Published August 21, 2025, https://www.goodfire.ai/research/ model-diff-amplification .Ashury-Tahan, S., Cohen, A. D. N., Cohen, N., Louzoun, Y., and Goldberg, Y. Data-driven coreference-based ontology building, 2024. URL https://arxiv.org/abs/ 2410.17051 .Ashury-Tahan, S., Mai, Y., C, R., Gera, A., Perlitz, Y., Yehu-dai, A., Bandel, E., Choshen, L., Shnarch, E., Liang, P., and Shmueli-Scheuer, M. The mighty torr: A bench-mark for table reasoning and robustness, 2025. URL 

https://arxiv.org/abs/2502.19412 .Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 , 2021. Baldock, R., Maennel, H., and Neyshabur, B. Deep learning through the lens of example difficulty. Advances in Neural Information Processing Systems , 34:10876–10889, 2021. Bedi, S., Cui, H., Fuentes, M., Unell, A., Wornow, M., Banda, J. M., Kotecha, N., Keyes, T., Mai, Y., Oez, M., Qiu, H., Jain, S., Schettini, L., Kashyap, M., Fries, J. A., Swaminathan, A., Chung, P., Nateghi, F., Aali, A., Nayak, A., Vedak, S., Jain, S. S., Patel, B., Fayanju, O., Shah, S., Goh, E., han Yao, D., Soetikno, B., Reis, E., Ga-tidis, S., Divi, V., Capasso, R., Saralkar, R., Chiang, C.-C., Jindal, J., Pham, T., Ghoddusi, F., Lin, S., Chiou, A. S., Hong, C., Roy, M., Gensheimer, M. F., Patel, H., Schulman, K., Dash, D., Char, D., Downing, L., Grol-leau, F., Black, K., Mieso, B., Zahedivash, A., wai Yim, W., Sharma, H., Lee, T., Kirsch, H., Lee, J., Ambers, N., Lugtu, C., Sharma, A., Mawji, B., Alekseyev, A., Zhou, V., Kakkar, V., Helzer, J., Revri, A., Bannett, Y., Daneshjou, R., Chen, J., Alsentzer, E., Morse, K., Ravi, N., Aghaeepour, N., Kennedy, V., Chaudhari, A., Wang, T., Koyejo, S., Lungren, M. P., Horvitz, E., Liang, P., Pf-effer, M., and Shah, N. H. Medhelm: Holistic evaluation of large language models for medical tasks, 2025. URL 

https://arxiv.org/abs/2505.23802 .Biderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow, J., Abbasi, B., Aji, A. F., Ammanamanchi, P. S., Black, S., Clive, J., DiPofi, A., Etxaniz, J., Fattori, B., Forde, J. Z., Foster, C., Jaiswal, M., Lee, W. Y., Li, H., Lovering, C., Muennighoff, N., Pavlick, E., Phang, J., Skowron, A., Tan, S., Tang, X., Wang, K. A., Winata, G. I., Yvon, F., and Zou, A. Lessons from the trenches on reproducible evaluation of language models. ArXiv , abs/2405.14782, 2024. URL https://api.semanticscholar. org/CorpusID:269982020 .Bradley, W. F. Llms and the madness of crowds. 

ArXiv , abs/2411.01539, 2024. URL https: //api.semanticscholar.org/CorpusID: 273811305 .Cattan, A., Roit, P., Zhang, S., Wan, D., Aharoni, R., Szpek-tor, I., Bansal, M., and Dagan, I. Localizing factual in-consistencies in attributable text generation, 2025. URL 

https://arxiv.org/abs/2410.07473 .Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-ian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saun-ders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., Mc-Grew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/ 2107.03374 .Choshen, L. and Abend, O. Automatically extracting chal-lenge sets for non local phenomena in neural machine translation. arXiv preprint arXiv:1909.06814 , 2019. Choshen, L., Hacohen, G., Weinshall, D., and Abend, O. The grammar-learning trajectories of neural language models. In Muresan, S., Nakov, P., and Villavicen-cio, A. (eds.), Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 8281–8297, Dublin, Ire-land, May 2022. Association for Computational Linguis-tics. doi: 10.18653/v1/2022.acl-long.568. URL https: //aclanthology.org/2022.acl-long.568/ .Choshen, L., Gera, A., Perlitz, Y., Shmueli-Scheuer, M., and Stanovsky, G. Navigating the modern evaluation 9ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

landscape: Considerations in benchmarks and frame-works for large language models (LLMs). In Klinger, R., Okazaki, N., Calzolari, N., and Kan, M.-Y. (eds.), 

Proceedings of the 2024 Joint International Confer-ence on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024): Tutorial Sum-maries , pp. 19–25, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024. lrec-tutorials.4/ .Creswell, A. and Shanahan, M. Faithful reasoning us-ing large language models. ArXiv , abs/2208.14271, 2022. URL https://api.semanticscholar. org/CorpusID:251929296 .Deshpande, D., Gangal, V., Mehta, H., Krishnan, J., Kan-nappan, A., and Qian, R. Trail: Trace reasoning and agen-tic issue localization. arXiv preprint arXiv:2505.08638 ,2025. Dou, S., Jia, H., Wu, S., Zheng, H., Zhou, W., Wu, M., Chai, M., Fan, J., Huang, C., Tao, Y., Liu, Y., Zhou, E., Zhang, M., Zhou, Y., Wu, Y. Z., Zheng, R., bo Wen, M., Weng, R., Wang, J., Cai, X., Gui, T., Qiu, X., Zhang, Q., and Huang, X. What’s wrong with your code generated by large language models? an extensive study. ArXiv , abs/2407.06153, 2024. URL https://api.semanticscholar. org/CorpusID:271050610 .Errica, F., Siracusano, G., Sanvito, D., and Bifulco, R. What did i do wrong? quantifying llms’ sensitivity and consis-tency to prompt engineering. ArXiv , abs/2406.12334, 2024. URL https://api.semanticscholar. org/CorpusID:270562829 .Funk, M., Hosemann, S., Jung, J. C., and Lutz, C. Towards ontology construction with language mod-els. ArXiv , abs/2309.09898, 2023. URL https: //api.semanticscholar.org/CorpusID: 262044094 .Giglou, H. B., D’Souza, J., and Auer, S. Llms4ol: Large language models for ontology learning. 

ArXiv , abs/2307.16648, 2023. URL https: //api.semanticscholar.org/CorpusID: 260334867 .Habba, E., Arviv, O., Itzhak, I., Perlitz, Y., Bandel, E., Choshen, L., Shmueli-Scheuer, M., and Stanovsky, G. DOVE: A large-scale multi-dimensional predictions dataset towards meaningful LLM evaluation. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), 

Findings of the Association for Computational Linguis-tics: ACL 2025 , pp. 11744–11763, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl. 611. URL https://aclanthology.org/2025. findings-acl.611/ .Hacohen, G., Choshen, L., and Weinshall, D. Let’s agree to agree: Neural networks share classification order on real datasets. In International Conference on Machine Learning , pp. 3950–3960. PMLR, 2020. Honovich, O., Aharoni, R., Herzig, J., Taitelbaum, H., Kuk-liansy, D., Cohen, V., Scialom, T., Szpektor, I., Has-sidim, A., and Matias, Y. True: Re-evaluating fac-tual consistency evaluation. ArXiv , abs/2204.04991, 2022. URL https://api.semanticscholar. org/CorpusID:247694170 .Kamoi, R., Das, S. S. S., Lou, R., Ahn, J. J., Zhao, Y., Lu, X., Zhang, N., Zhang, Y., Zhang, R. H., Vummanthala, S. R., Dave, S., Qin, S., Cohan, A., Yin, W., and Zhang, R. Evaluating llms at detect-ing errors in llm responses. ArXiv , abs/2404.03602, 2024. URL https://api.semanticscholar. org/CorpusID:268889318 .Kang, J., Luo, H., Zhu, Y., Glass, J. R., Cox, D., Ritter, A., Feris, R., and Karlinsky, L. Self-specialization: Uncovering latent expertise within large language models. In Annual Meeting of the Association for Computational Linguistics , 2023. URL https: //api.semanticscholar.org/CorpusId: 263334580 .Kryscinski, W., McCann, B., Xiong, C., and Socher, R. Evaluating the factual consistency of abstrac-tive text summarization. In Conference on Em-pirical Methods in Natural Language Processing ,2019. URL https://api.semanticscholar. org/CorpusID:204976362 .LI, S., Chen, J., Shen, Y., Chen, Z., Zhang, X., Li, Z., Wang, H., Qian, J., Peng, B., Mao, Y., Chen, W., and Yan, X. Explanations from large language models make small reasoners better. ArXiv , abs/2210.06726, 2022. URL https://api.semanticscholar. org/CorpusID:252873123 .Li, X., Wang, W., Li, M., Guo, J., Zhang, Y., and Feng, F. Evaluating mathematical reason-ing of large language models: A focus on er-ror identification and correction. In Annual Meet-ing of the Association for Computational Linguistics ,2024. URL https://api.semanticscholar. org/CorpusID:270213098 .Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., R ´e, C., Acosta-Navas, D., Hudson, 10 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

D. A., Zelikman, E., Durmus, E., Ladhak, F., Rong, F., Ren, H., Yao, H., Wang, J., Santhanam, K., Orr, L., Zheng, L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha, N., Chatterji, N., Khattab, O., Henderson, P., Huang, Q., Chi, R., Xie, S. M., Santurkar, S., Ganguli, S., Hashimoto, T., Icard, T., Zhang, T., Chaudhary, V., Wang, W., Li, X., Mai, Y., Zhang, Y., and Koreeda, Y. Holistic evaluation of language models, 2023. URL https://arxiv.org/ abs/2211.09110 .Lin, B. Y., Deng, Y., Chandu, K. R., Brahman, F., Ravichan-der, A., Pyatkin, V., Dziri, N., Bras, R. L., and Choi, Y. Wildbench: Benchmarking llms with challenging tasks from real users in the wild. ArXiv , abs/2406.04770, 2024. URL https://api.semanticscholar. org/CorpusID:270357771 .Lindsey, J., Templeton, A., Marcus, J., Conerly, T., Batson, J., and Olah, C. Sparse crosscoders for cross-layer features and model diffing. Techni-cal report, Transformer Circuits Thread, Anthropic, 2024. https://transformer-circuits.pub/ 2024/crosscoders/index.html .Lior, G., Habba, E., Levy, S., Caciularu, A., and Stanovsky, G. Reliableeval: A recipe for stochastic llm evaluation via method of moments. arXiv preprint arXiv:2505.22169 ,2025. Liu, B., Ash, J., Goel, S., Krishnamurthy, A., and Zhang, C. Exposing attention glitches with flip-flop language modeling. Advances in Neural Information Processing Systems , 36:25549–25583, 2023a. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023b. URL https://arxiv.org/abs/2305.01210 .Magnusson, I., Bhagia, A., Hofmann, V., Soldaini, L., Jha, A., Tafjord, O., Schwenk, D., Walsh, P., Elazar, Y., Lo, K., Groeneveld, D., Beltagy, I., Hajishirzi, H., Smith, N. A., Richardson, K., and Dodge, J. Paloma: A benchmark for evaluating language model fit. ArXiv , abs/2312.10523, 2023. URL https://api.semanticscholar. org/CorpusID:266348815 .Maia Polo, F., Xu, R., Weber, L., Silva, M., Bhardwaj, O., Choshen, L., de Oliveira, A., Sun, Y., and Yurochkin, M. Efficient multi-prompt evaluation of llms. Advances in Neural Information Processing Systems , 37:22483– 22512, 2024. Maimon, A., Cohen, A. D., Vishne, G., Ravfogel, S., and Tsarfaty, R. Iq test for llms: An evaluation framework for uncovering core skills in llms, 2025. URL https: //arxiv.org/abs/2507.20208 .Mishra-Sharma, S., Bricken, T., Lindsey, J., Jermyn, A., Marcus, J., Rivoire, K., Olah, C., and Henighan, T. Insights on crosscoder model diffing. Tech-nical report, Transformer Circuits Thread, An-thropic, 2025. Preliminary report, https: //transformer-circuits.pub/2025/ crosscoder-diffing-update/index.html .Mizrahi, M., Kaplan, G., Malkin, D., Dror, R., Shahaf, D., and Stanovsky, G. State of what art? a call for multi-prompt llm evaluation. Transactions of the Asso-ciation for Computational Linguistics , 12:933–949, 08 2024. ISSN 2307-387X. doi: 10.1162/tacl a 00681. URL 

https://doi.org/10.1162/tacl_a_00681 .Mukherjee, S., Chinta, A., Kim, T., Sharma, T. A., and Hakkani-T ¨ur, D. Premise-augmented reasoning chains improve error identification in math reasoning with llms, 2025. URL https://arxiv.org/abs/ 2502.02362 .OpenAI. gpt-oss-120b and gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925 .Pan, Z., Li, Y., Lin, H., Pei, Q., Tang, Z., Wu, W., Ming, C., Zhao, H. V., He, C., and Wu, L. Lemma: Learning from errors for mathemat-ical advancement in llms. ArXiv , abs/2503.17439, 2025. URL https://api.semanticscholar. org/CorpusID:277272182 .Pang, J.-C., Wang, P., Li, K., Chen, X.-H., Xu, J., Zhang, Z., and Yu, Y. Language model self-improvement by reinforcement learning contem-plation. ArXiv , abs/2305.14483, 2023. URL https: //api.semanticscholar.org/CorpusID: 258865735 .Patil, S. G., Mao, H., Cheng-Jie Ji, C., Yan, F., Suresh, V., Stoica, I., and E. Gonzalez, J. The berkeley func-tion calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning , 2025. Perlitz, Y., Bandel, E., Gera, A., Arviv, O., Ein-Dor, L., Shnarch, E., Slonim, N., Shmueli-Scheuer, M., and Choshen, L. Efficient benchmarking (of lan-guage models). In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies (Volume 1: Long Papers) , pp. 2519–2536, Mexico City, Mexico, June 2024. Association for Computa-tional Linguistics. doi: 10.18653/v1/2024.naacl-long. 139. URL https://aclanthology.org/2024. naacl-long.139/ .11 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Pezeshkpour, P. and Hruschka, E. Large lan-guage models sensitivity to the order of options in multiple-choice questions. In NAACL-HLT ,2023. URL https://api.semanticscholar. org/CorpusID:261064970 .Polo, F. M., Weber, L., Choshen, L., Sun, Y., Xu, G., and Yurochkin, M. tinybenchmarks: evaluating llms with fewer examples. In Forty-first International Conference on Machine Learning , a. Polo, F. M., Xu, R., Weber, L., Silva, M., Bhardwaj, O., Choshen, L., de Oliveira, A. F. M., Sun, Y., and Yurochkin, M. Efficient multi-prompt evaluation of llms. In The Thirty-eighth Annual Conference on Neural Infor-mation Processing Systems , b. Polo, F. M., Somerstep, S., Choshen, L., Sun, Y., and Yurochkin, M. Sloth: scaling laws for llm skills to pre-dict multi-benchmark performance across families. arXiv preprint arXiv:2412.06540 , 2024. Ramprasad, S., Ferracane, E., and Lipton, Z. C. Analyz-ing llm behavior in dialogue summarization: Unveiling circumstantial hallucination trends. In Annual Meet-ing of the Association for Computational Linguistics ,2024. URL https://api.semanticscholar. org/CorpusID:270257966 .Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks, 2019. URL 

https://arxiv.org/abs/1908.10084 .Ruan, Y., Maddison, C. J., and Hashimoto, T. B. Obser-vational scaling laws and the predictability of langauge model performance. Advances in Neural Information Processing Systems , 37:15841–15892, 2024. Shah, C., White, R. W., Andersen, R., Buscher, G., Counts, S., Das, S. S. S., Montazer, A., Manivannan, S., Neville, J., Ni, X., Rangan, N., Safavi, T., Suri, S., Wan, M., Wang, L., and Yang, L. Using large language models to generate, validate, and apply user intent taxonomies. ACM Transactions on the Web ,2023. URL https://api.semanticscholar. org/CorpusID:262459084 .Simonds, T., Lopez, K., Yoshiyama, A., and Garmier, D. Rlsr: Reinforcement learning from self reward. 2025. URL https://api.semanticscholar. org/CorpusID:278602710 .Siska, C., Marazopoulou, K., Ailem, M., and Bono, J. Ex-amining the robustness of LLM evaluation to the distribu-tional assumptions of benchmarks. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 10406–10421, Bangkok, Thailand, August 2024. Association for Com-putational Linguistics. doi: 10.18653/v1/2024.acl-long. 560. URL https://aclanthology.org/2024. acl-long.560/ .Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understand-ing across millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024. Tjuatja, L. and Neubig, G. Behaviorbox: Automated dis-covery of fine-grained performance differences between language models, 2025. URL https://arxiv.org/ abs/2506.02204 .Wan, M., Safavi, T., Jauhar, S. K., Kim, Y., Counts, S., Neville, J., Suri, S., Shah, C., White, R. W., Yang, L., Andersen, R., Buscher, G., Joshi, D., and Ran-gan, N. Tnt-llm: Text mining at scale with large lan-guage models. Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ,2024. URL https://api.semanticscholar. org/CorpusID:268532413 .Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: A more robust and challenging multi-task language un-derstanding benchmark. Advances in Neural Information Processing Systems , 37:95266–95290, 2024a. Wang, Y., Ying, J., Cao, Y., Ma, Y., and Jiang, Y. Effieval: Efficient and generalizable model evaluation via capability coverage maximization. arXiv preprint arXiv:2508.09662 , 2025. Wang, Z., Zhou, Z., Song, D., Huang, Y., Chen, S., Ma, L., and Zhang, T. Towards understanding the characteris-tics of code generation errors made by large language models. 2025 IEEE/ACM 47th International Confer-ence on Software Engineering (ICSE) , pp. 2587–2599, 2024b. URL https://api.semanticscholar. org/CorpusID:270440414 .Xu, F., Lin, Q., Han, J., Zhao, T., Liu, J., and Cambria, E. Are large language models really good logical rea-soners? a comprehensive evaluation and beyond. IEEE Transactions on Knowledge and Data Engineering , 2025. Xu, J., Mai, Y., and Liang, P. Helm capabili-ties: Evaluating lms capability by capability. 

https://crfm.stanford.edu/2025/03/ 20/helm-capabilities.html , 2024. 12 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Yehudai, A., Eden, L., Perlitz, Y., Bar-Haim, R., and Shmueli-Scheuer, M. Clear: Error analysis via llm-as-a-judge made easy, 2025. URL https://arxiv.org/ abs/2507.18392 .Zelikman, E., Wu, Y., Mu, J., and Goodman, N. D. Star: Bootstrapping reasoning with reasoning, 2022. URL 

https://arxiv.org/abs/2203.14465 .Zeng, Z., Wang, Y., Hajishirzi, H., and Koh, P. W. Evaltree: Profiling language model weaknesses via hierarchical capability trees, 2025. URL https://arxiv.org/ abs/2503.08893 .Zhao, W., Goyal, T., Chiu, Y. Y., Jiang, L., Newman, B., Ravichander, A., Chandu, K., Bras, R. L., Cardie, C., Deng, Y., and Choi, Y. Wildhallucinations: Evaluat-ing long-form factuality in llms with real-world entity queries, 2024. URL https://arxiv.org/abs/ 2407.17468 .Zheng, C., Zhang, Z., Zhang, B., Lin, R., Lu, K., Yu, B., Liu, D., Zhou, J., and Lin, J. ProcessBench: Identifying process errors in mathematical reasoning. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Pro-ceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,pp. 1009–1024, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.50. URL https: //aclanthology.org/2025.acl-long.50/ .Zhou, L., Pacchiardi, L., Mart ´ınez-Plumed, F., Collins, K. M., Moros-Daval, Y., Zhang, S., Zhao, Q., Huang, Y., Sun, L., Prunty, J. E., et al. General scales unlock ai evaluation with explanatory and predictive power. arXiv preprint arXiv:2503.06378 , 2025. Zhuang, Y., Yu, J., Liu, Q., Sun, Y., Li, J., Huang, Z., and Chen, E. Efficient benchmarking via bias-bounded subset selection. IEEE Transactions on Pattern Analysis and Machine Intelligence , pp. 1–18, 2025. doi: 10.1109/ TPAMI.2025.3598031. Zhuo, J., Zhang, S., Fang, X., Duan, H., Lin, D., and Chen, K. Prosa: Assessing and understanding the prompt sensitivity of llms. ArXiv , abs/2410.12405, 2024. URL https://api.semanticscholar. org/CorpusID:273375563 .13 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

# A. ErrorMap : Additional Details 

Field Name Default Value Description 

batch size 500 Size of minibatches for data processing. 

classify batch size 50 Size of minibatches for item classification. 

cluster name length 5 Maximum length for cluster names. 

cluster description length 30 Maximum length for cluster descriptions. 

max num of clusters 25 Maximum number of clusters allowed. 

max layers 3 Maximum number of category layers.  

> Table 4. Taxonomy Configuration Parameters.

A.1. Prompts 

A.1.1. P ER -I NSTANCE ERROR ANALYSIS 

You a r e an e x p e r t a n a l y s t . Your j o b i s t o e v a l u a t e e v i d e n c e s t e p by s t e p , c o n s i d e r a l t e r n a t i v e s , and r e a c h a j u s t i f i e d c o n c l u s i o n . R e a s o n i n g : h i g h . You a r e g i v e n t h e f o l l o w i n g : − A c o n t e x t − A model r e s p o n s e t h a t was l a b e l e d i n c o r r e c t 

{% i f c o r r e c t a n s w e r %}

− A r e f e r e n c e 

{% e n d i f %}{% i f c o r r e c t o u t p u t s %}

− A l i s t o f s o l u t i o n s t h a t were l a b e l e d a s c o r r e c t 

{% e n d i f %}

Your t a s k : 1 . S t r u c t u r e d C o r r e c t S o l u t i o n : A n a l y z e t h e c o r r e c t r e s p o n s e s and e x t r a c t from them t h e main r e q u i r e d c r i t e r i a o r r e a s o n i n g s t e p s f o r t h e c o n t e x t . 2 . S t e p −by − s t e p E v a l u a t i o n : E v a l u a t e t h e i n c o r r e c t r e s p o n s e a g a i n s t e a c h o f t h e r e q u i r e d c r i t e r i a . F or e a c h c r i t e r i o n , p r o v i d e t h e f o l l o w i n g f i e l d s : p r e s e n t i n w r o n g : Whether i t i s p r e s e n t i n t h e i n c o r r e c t r e s p o n s e q u a l i t y : The q u a l i t y o f i t s e x e c u t i o n ( c o r r e c t , p a r t i a l l y c o r r e c t , i n c o r r e c t , o r n u l l i f m i s s i n g ) e v i d e n c e : S u p p o r t i n g e v i d e n c e from t h e i n c o r r e c t r e s p o n s e ( q u o t e ) comment : Any r e l e v a n t comments 3 . E r r o r D i a g n o s i s : I d e n t i f y t h e f i r s t m a j o r e r r o r i n t h e i n c o r r e c t r e s p o n s e t h a t l e d t o t h e i n c o r r e c t answer , and p r o v i d e t h e f o l l o w i n g f i e l d s i n f i n a l a n s w e r : e r r o r s u m m a r y : I f s u c h an e r r o r e x i s t s , summarize t h e model ’ s r e a s o n i n g w e a k n e s s i n e r r o r s u m m a r y . T h i s s h o u l d f o c u s on model t h i n k i n g ( e . g . , ’ t h e model f a i l e d t o r e c o g n i z e f a c t X’ ) r a t h e r t h a n t e c h n i c a l e x e c u t i o n ( e . g . , ’ t h e model s e l e c t e d t h e wrong answer ’ ) . t i t l e : P r o v i d e a s h o r t , f r e e − form t i t l e t h a t d e s c r i b e s t h e s p e c i f i c t y p e o f e r r o r .* I f you d i d n ’ t f i n d any e r r o r i n t h e i n c o r r e c t r e s p o n s e l e a v e a l l t h e f i e l d s o f f i n a l a n s w e r w i t h an empty s t r i n g . 14 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

* I f t h e whole s o l u t i o n i s i n c o r r e c t , w r i t e ’ whole s o l u t i o n i n c o r r e c t ’ i n f i n a l a n s w e r f i e l d s . * Avoid ambiguous t i t l e s o r o n e s t h a t c a n n o t be mapped t o a s p e c i f i c s k i l l . F o r example , i n s t e a d o f u s i n g ” Wrong m u l t i p l e c h o i c e s e l e c t i o n ” , i d e n t i f y t h e u n d e r l y i n g r e a s o n i n g e r r o r s u c h a s ” M i s i n t e r p r e t a t i o n o f c o n c e p t ” . Use a s many s t e p s and t h i n k i n g p r o c e s s a s you n ee d . F i n a l l y , o u t p u t t h e f i n a l r e s u l t i n t h e f o l l o w i n g f o r m a t : 

{

” r e q u i r e d c r i t e r i a ” : [

{

” c r i t e r i o n ” : ” D e s c r i b e t h e r e l a t i o n s h i p b e t w e e n A and B” , ” p r e s e n t i n w r o n g ” : t r u e , ” q u a l i t y ” : ” i n c o r r e c t ” , ” e v i d e n c e ” : ” B e c a u s e A i n c r e a s e d when B i n c r e a s e d , A must be c a u s e d by B . ” , ” comment ” : ” C o n f u s e s c o r r e l a t i o n w i t h c a u s a t i o n ” 

} ,

{

” c r i t e r i o n ” : ” E x p l a i n t h e mechanism o f a c t i o n ” , ” p r e s e n t i n w r o n g ” : t r u e , ” q u a l i t y ” : ” c o r r e c t ” , ” e v i d e n c e ” : ” t h e b i o c h e m i c a l pathway . . . ” , ” comment ” : ” A c c u r a t e and c o m p l e t e ” 

}

] , ” f i n a l a n s w e r ” : {

” e r r o r s u m m a r y ” : ” The i n c o r r e c t r e s p o n s e a s s u m e s c a u s a t i o n from c o r r e l a t i o n , l e a d i n g t o a f l a w e d c o n c l u s i o n a b o u t t h e r e l a t i o n s h i p b e t w e e n A and B . ” , ” e r r o r t i t l e ” : ” C a u s a l M i s i n t e r p r e t a t i o n ” 

}}

Use t h e f o l l o w i n g i n p u t s : C o n t e x t : 

{{ i n p u t t e x t }} {% i f c o r r e c t a n s w e r %}

R e f e r e n c e s : 

{{ c o r r e c t a n s w e r }} {% e n d i f %}{% i f c o r r e c t o u t p u t s %}

C o r r e c t R e s p o n s e s : 

{{ c o r r e c t o u t p u t s }} {% e n d i f %}

i n c o r r e c t p r e d i c t i o n : 

{{ o u t p u t t e x t }} 

Keeping t h e e v a l u a t i o n c r i t e r i a i n mind , do n o t p r o v i d e a g e n e r a l a s s e s s m e n t . Be s p e c i f i c , s t r u c t u r e d , and e v i d e n c e − b a s e d . 15 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

A s s e s s m e n t : A.1.2. T AXONOMY GENERATION PROMPT 

You a r e an e x p e r t a n a l y s t . Your j o b i s t o e v a l u a t e e v i d e n c e s t e p by s t e p , c o n s i d e r a l t e r n a t i v e s , and r e a c h a j u s t i f i e d c o n c l u s i o n . R e a s o n i n g : h i g h . # I n s t r u c t i o n ## C o n t e x t − ** Goal * * : Your g o a l i s t o c l u s t e r t h e i n p u t d a t a i n t o m e a n i n g f u l c a t e g o r i e s f o r t h e g i v e n u s e c a s e . − ** Da ta * * : The i n p u t d a t a w i l l be a l i s t o f {{ d a t a t y p e }} t u p l e s , i n c l u d i n g t h e f o l l o w i n g e l e m e n t s : − ** t e x t * * : {{ d a t a t y p e }} a s t h e f i r s t t u p l e e l e m e n t . − **num o f o c c u r r e n c e s * * : number a s t h e s e c o n d t u p l e e l e m e n t . − ** Use c a s e * * : G e n e r a t e a taxonomy t h a t c a t e g o r i z e s model e r r o r s b a s e d on t h e s p e c i f i c s k i l l s t h e model f a i l e d t o d e m o n s t r a t e i n e a c h example . 

{% i f p a r e n t c a t e g o r y %}

A l l o f t h e s e e r r o r s ha ve a l r e a d y b een l a b e l e d u n d e r t h e c a t e g o r y : *{{ 

p a r e n t c a t e g o r y } } * . P l e a s e e n s u r e t h a t you a s s i g n e a c h e r r o r t o more s p e c i f i c and i n f o r m a t i v e sub − c a t e g o r i e s t h a t go beyond t h e g e n e r a l l a b e l ”{{ p a r e n t c a t e g o r y } } ” . F ocus on i d e n t i f y i n g t h e u n d e r l y i n g s k i l l s o r e r r o r t y p e s t h a t p r o v i d e d e e p e r i n s i g h t . 

{% e n d i f %}

## R e q u i r e m e n t s #### Format − O u t p u t c l u s t e r s i n **JSON f o r m a t ** w i t h e a c h c l u s t e r a s an o b j e c t i n a ‘c l u s t e r s ‘ a r r a y . − Each c l u s t e r must i n c l u d e : − ‘ ” i d ” ‘ : c a t e g o r y number s t a r t i n g from 1 . − ‘ ” name ” ‘ : c a t e g o r y name ( max {{ c l u s t e r n a m e l e n g t h }} words ) , e i t h e r a v e r b o r noun p h r a s e . − ‘ ” d e s c r i p t i o n ” ‘ : c a t e g o r y d e s c r i p t i o n ( max {{ c l u s t e r d e s c r i p t i o n l e n g t h }} 

words ) . − Example : 

{

” c l u s t e r s ” : [

{

” i d ” : 1 , ” name ” : ” F a c t u a l E r r o r ” , ” d e s c r i p t i o n ” : ” Model f a i l s t o r e t r i e v e o r g e n e r a t e a c c u r a t e i n f o r m a t i o n . ” 

}

]

}

− T o t a l number o f c a t e g o r i e s s h o u l d be ** no more t h a n {{ m a x n u m c l u s t e r s } } * * . − O u t p u t s h o u l d be i n ** E n g l i s h ** o n l y . ### Q u a l i t y 16 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

− **No o v e r l a p o r c o n t r a d i c t i o n ** among t h e c a t e g o r i e s . − **Name** i s a c o n c i s e and c l e a r l a b e l f o r t h e c a t e g o r y , i d e n t i f i e s ** one s p e c i f i c s k i l l o r a b i l i t y o n l y * * . Use o n l y p h r a s e s t h a t a r e s p e c i f i c t o e a c h c a t e g o r y and a v o i d t h o s e t h a t a r e common t o a l l c a t e g o r i e s . − **Name** r e f l e c t s c o r e c a p a b i l i t i e s , n o t domain − s p e c i f i c c o n t e x t s , o r t e c h n i c a l c h o i c e s . Example : n o t ” I n c o r r e c t A n a t o m i c a l Knowledge ” b u t ” F a c t u a l E r r o r ” ( The i s s u e i s a b o u t f a c t u a l a c c u r a c y , n o t b i o l o g y s p e c i f i c a l l y ) . I f t h e i s s u e d o e s n o t c l e a r l y map t o a s p e c i f i c s k i l l , c l a s s i f y i t a s ” Hard t o A n a l y z e ” − t h i s a p p l i e s when t h e e r r o r i s ambiguous , s u b j e c t i v e , o r l a c k s s u f f i c i e n t c o n t e x t t o d e t e r m i n e i t s n a t u r e c o n f i d e n t l y . − ** D e s c r i p t i o n ** d i f f e r e n t i a t e s one c a t e g o r y from a n o t h e r . − **Name** and ** d e s c r i p t i o n ** c a n ** a c c u r a t e l y ** and ** c o n s i s t e n t l y ** c l a s s i f y new d a t a p o i n t s ** w i t h o u t a m b i g u i t y * * . − **Name** and ** d e s c r i p t i o n ** a r e * c o n s i s t e n t w i t h e a c h o t h e r * . − O u t p u t c l u s t e r s match t h e d a t a a s c l o s e l y a s p o s s i b l e , w i t h o u t m i s s i n g i m p o r t a n t c a t e g o r i e s o r a d d i n g u n n e c e s s a r y o n e s . − O u t p u t c l u s t e r s s h o u l d s t r i v e t o be o r t h o g o n a l , p r o v i d i n g s o l i d c o v e r a g e o f t h e t a r g e t domain . − O u t p u t c l u s t e r s s e r v e t h e g i v e n u s e c a s e w e l l . − O u t p u t c l u s t e r s s h o u l d be s p e c i f i c and m e a n i n g f u l . Do n o t i n v e n t c a t e g o r i e s t h a t a r e n o t i n t h e d a t a . # D at a 

{{ d a t a }} 

# Q u e s t i o n s ## Q1 . P l e a s e g e n e r a t e a c l u s t e r l i s t from t h e i n p u t d a t a t h a t m e e t s t h e r e q u i r e m e n t s . T i p s − ** Us er F e e d b a c k i s MANDATORY* * : You MUST a d d r e s s any p r e v i o u s u s e r f e e d b a c k i n y o u r c l u s t e r i n g − I f u s e r f e e d b a c k was p r o v i d e d , e x p l i c i t l y e x p l a i n how you ’ ve i n c o r p o r a t e d t h e i r s p e c i f i c c o n c e r n s and s u g g e s t i o n s − The c l u s t e r l i s t s h o u l d be a ** f l a t l i s t ** o f ** m u t u a l l y e x c l u s i v e ** c a t e g o r i e s . S o r t them b a s e d on t h e i r s e m a n t i c r e l a t e d n e s s . − Though you s h o u l d aim f o r {{ m a x n u m c l u s t e r s }} c a t e g o r i e s , you c a n ha ve *f e w e r t h a n {{ m a x n u m c l u s t e r s }} c a t e g o r i e s * i n t h e c l u s t e r l i s t ; b u t ** do n o t e x c e e d t h e l i m i t . * * − Be ** s p e c i f i c ** a b o u t e a c h c a t e g o r y . **Do n o t i n c l u d e vague c a t e g o r i e s ** s u c h a s ” O t h e r ” , ” G e n e r a l ” , ” U n c l e a r ” , ” M i s c e l l a n e o u s ” o r ” U n d e f i n e d ” i n t h e c l u s t e r l i s t . − You c a n i g n o r e low q u a l i t y o r ambiguous d a t a p o i n t s . ## Q2 . Why d i d you c l u s t e r t h e d a t a t h e way you d i d ? E x p l a i n y o u r r e a s o n i n g ** w i t h i n {{ e x p l a n a t i o n l e n g t h }} words * * . I n c l u d e how you a d d r e s s e d any u s e r f e e d b a c k . ## P r o v i d e y o u r a n s w e r s i n t h e t a g s : ” c l u s t e r s ” − y o u r g e n e r a t e d c l u s t e r l i s t w i t h no more t h a n {{ m a x n u m c l u s t e r s }} c a t e g o r i e s , ” e x p l a n a t i o n ” −e x p l a n a t i o n o f y o u r r e a s o n i n g p r o c e s s w i t h i n {{ e x p l a n a t i o n l e n g t h }} words . 17 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

# O u t p u t A.1.3. T AXONOMY UPDATE PROMPT 

You a r e an e x p e r t a n a l y s t . Your j o b i s t o e v a l u a t e e v i d e n c e s t e p by s t e p , c o n s i d e r a l t e r n a t i v e s , and r e a c h a j u s t i f i e d c o n c l u s i o n . R e a s o n i n g : h i g h . # I n s t r u c t i o n ## C o n t e x t − ** Goal * * : You g o a l i s t o r e v i e w t h e g i v e n r e f e r e n c e l i s t b a s e d on t h e i n p u t d a t a f o r t h e s p e c i f i e d u s e c a s e , t h e n u p d a t e t h e r e f e r e n c e l i s t i f n e e d e d . − You w i l l be g i v e n a r e f e r e n c e c l u s t e r l i s t , which i s b u i l t on e x i s t i n g d a t a . The r e f e r e n c e l i s t w i l l be u s e d t o c l a s s i f y new d a t a p o i n t s . − You w i l l compare t h e i n p u t d a t a w i t h t h e r e f e r e n c e l i s t , o u t p u t a r a t i n g s c o r e o f t h e q u a l i t y o f t h e r e f e r e n c e l i s t , s u g g e s t p o t e n t i a l e d i t s , and u p d a t e t h e r e f e r e n c e l i s t i f n e e d e d . − ** R e f e r e n c e c l u s t e r l i s t * * : The i n p u t c l u s t e r l i s t i s i n JSON f o r m a t w i t h e a c h c l u s t e r a s a ” c l u s t e r ” e l e m e n t , c o n t a i n i n g t h e f o l l o w i n g sub − e l e m e n t s : − ** i d * * : c a t e g o r y i n d e x . − ** name * * : c a t e g o r y name . − ** d e s c r i p t i o n * * : c a t e g o r y d e s c r i p t i o n u s e d t o c l a s s i f y d a t a p o i n t s . − ** Da ta * * : The i n p u t d a t a w i l l be a l i s t o f {{ d a t a t y p e }} t u p l e s , i n c l u d i n g t h e f o l l o w i n g e l e m e n t s : − ** t e x t * * : {{ d a t a t y p e }} a s t h e f i r s t t u p l e e l e m e n t . − **num o f o c c u r r e n c e s * * : number a s t h e s e c o n d t u p l e e l e m e n t . − ** Use c a s e * * : Update t h e taxonomy t h a t c a t e g o r i z e s model e r r o r s b a s e d on t h e s p e c i f i c s k i l l s t h e model f a i l e d t o d e m o n s t r a t e i n e a c h example . 

{% i f p a r e n t c a t e g o r y %}

A l l o f t h e s e e r r o r s ha ve a l r e a d y b een l a b e l e d u n d e r t h e c a t e g o r y : *{{ 

p a r e n t c a t e g o r y } } * . P l e a s e e n s u r e t h a t you a s s i g n e a c h e r r o r t o more s p e c i f i c and i n f o r m a t i v e sub − c a t e g o r i e s t h a t go beyond t h e g e n e r a l l a b e l ”{{ p a r e n t c a t e g o r y } } ” . F ocus on i d e n t i f y i n g t h e u n d e r l y i n g s k i l l s o r e r r o r t y p e s t h a t p r o v i d e d e e p e r i n s i g h t . 

{% e n d i f %}

## R e q u i r e m e n t s ### Format − O u t p u t c l u s t e r s i n **JSON f o r m a t ** w i t h e a c h c l u s t e r a s an o b j e c t i n a ‘c l u s t e r s ‘ a r r a y . − Each c l u s t e r must i n c l u d e : − ‘ ” i d ” ‘ : c a t e g o r y number s t a r t i n g from 1 . − ‘ ” name ” ‘ : c a t e g o r y name ( max {{ c l u s t e r n a m e l e n g t h }} words ) , e i t h e r a v e r b o r noun p h r a s e . − ‘ ” d e s c r i p t i o n ” ‘ : c a t e g o r y d e s c r i p t i o n ( max {{ c l u s t e r d e s c r i p t i o n l e n g t h }} 

words ) . − Example : 

{

” c l u s t e r s ” : [

{

” i d ” : 1 , 18 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

” name ” : ” F a c t u a l E r r o r ” , ” d e s c r i p t i o n ” : ” Model f a i l s t o r e t r i e v e o r g e n e r a t e a c c u r a t e i n f o r m a t i o n . ” 

}

]

}

− T o t a l number o f c a t e g o r i e s s h o u l d be ** no more t h a n {{ m a x n u m c l u s t e r s } } * * . − O u t p u t s h o u l d be i n ** E n g l i s h ** o n l y . ### Q u a l i t y − **No o v e r l a p o r c o n t r a d i c t i o n ** among t h e c a t e g o r i e s . − **Name** i s a c o n c i s e and c l e a r l a b e l f o r t h e c a t e g o r y , i d e n t i f i e s ** one s p e c i f i c s k i l l o r a b i l i t y o n l y * * . Use o n l y p h r a s e s t h a t a r e s p e c i f i c t o e a c h c a t e g o r y and a v o i d t h o s e t h a t a r e common t o a l l c a t e g o r i e s . − **Name** r e f l e c t s c o r e c a p a b i l i t i e s , n o t domain − s p e c i f i c c o n t e x t s , o r t e c h n i c a l c h o i c e s . Example : n o t ” I n c o r r e c t A n a t o m i c a l Knowledge ” b u t ” F a c t u a l E r r o r ” ( The i s s u e i s a b o u t f a c t u a l a c c u r a c y , n o t b i o l o g y s p e c i f i c a l l y ) . I f t h e i s s u e d o e s n o t c l e a r l y map t o a s p e c i f i c s k i l l , c l a s s i f y i t a s ” Hard t o A n a l y z e ” − t h i s a p p l i e s when t h e e r r o r i s ambiguous , s u b j e c t i v e , o r l a c k s s u f f i c i e n t c o n t e x t t o d e t e r m i n e i t s n a t u r e c o n f i d e n t l y . − ** D e s c r i p t i o n ** d i f f e r e n t i a t e s one c a t e g o r y from a n o t h e r . − **Name** and ** d e s c r i p t i o n ** c a n ** a c c u r a t e l y ** and ** c o n s i s t e n t l y ** c l a s s i f y new d a t a p o i n t s ** w i t h o u t a m b i g u i t y * * . − **Name** and ** d e s c r i p t i o n ** a r e * c o n s i s t e n t w i t h e a c h o t h e r * . − O u t p u t c l u s t e r s match t h e d a t a a s c l o s e l y a s p o s s i b l e , w i t h o u t m i s s i n g i m p o r t a n t c a t e g o r i e s o r a d d i n g u n n e c e s s a r y o n e s . − O u t p u t c l u s t e r s s h o u l d s t r i v e t o be o r t h o g o n a l , p r o v i d i n g s o l i d c o v e r a g e o f t h e t a r g e t domain . − O u t p u t c l u s t e r s s e r v e t h e g i v e n u s e c a s e w e l l . − O u t p u t c l u s t e r s s h o u l d be s p e c i f i c and m e a n i n g f u l . Do n o t i n v e n t c a t e g o r i e s t h a t a r e n o t i n t h e d a t a . # R e f e r e n c e c l u s t e r l i s t 

{{ c l u s t e r l i s t }} 

# D at a 

{{ d a t a }} 

# Q u e s t i o n s ## Q1 : Review t h e g i v e n r e f e r e n c e l i s t and t h e i n p u t d a t a and p r o v i d e a r a t i n g s c o r e o f t h e r e f e r e n c e l i s t . The r a t i n g s c o r e s h o u l d be an i n t e g e r b e t w e e n 0and 1 0 0 , h i g h e r r a t i n g s c o r e means b e t t e r q u a l i t y . You s h o u l d c o n s i d e r t h e f o l l o w i n g f a c t o r s when r a t i n g t h e r e f e r e n c e c l u s t e r l i s t : − ** I n t r i n s i c q u a l i t y * * : − 1 ) i f t h e c l u s t e r l i s t m e e t s t h e * R e q u i r e m e n t s * s e c t i o n , w i t h c l e a r and c o n s i s t e n t c a t e g o r y names and d e s c r i p t i o n s , and no o v e r l a p o r c o n t r a d i c t i o n among t h e c a t e g o r i e s ; − 2 ) i f t h e c a t e g o r i e s i n t h e c l u s t e r l i s t a r e r e l e v a n t t o t h e t h e g i v e n u s e c a s e ; − 3 ) i f t h e c l u s t e r l i s t i n c l u d e s any vague c a t e g o r i e s s u c h a s ” O t h e r ” , ”G e n e r a l ” , ” U n c l e a r ” , ” M i s c e l l a n e o u s ” o r ” U n d e f i n e d ” . − ** E x t r i n s i c q u a l i t y * * : 19 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

− 1 ) i f t h e c l u s t e r l i s t c a n a c c u r a t e l y and c o n s i s t e n t l y c l a s s i f y t h e i n p u t d a t a w i t h o u t a m b i g u i t y ; − 2 ) i f t h e r e a r e m i s s i n g c a t e g o r i e s i n t h e c l u s t e r l i s t b u t a p p e a r i n t h e i n p u t d a t a ; − 3 ) i f t h e r e a r e u n n e c e s s a r y c a t e g o r i e s i n t h e c l u s t e r l i s t t h a t do n o t a p p e a r i n t h e i n p u t d a t a . ## Q2 : E x p l a i n y o u r r a t i n g s c o r e i n Q1 ** w i t h i n {{ e x p l a n a t i o n l e n g t h }} words * * . ## Q3 : Based on y o u r r e v i e w , d e c i d e i f you ne ed t o e d i t t h e r e f e r e n c e l i s t t o i m p r o v e i t s q u a l i t y . I f yes , s u g g e s t p o t e n t i a l e d i t s ** w i t h i n {{ 

s u g g e s t i o n l e n g t h }} words * * . I f no , p l e a s e o u t p u t t h e o r i g i n a l r e f e r e n c e l i s t . T i p s : − You c a n e d i t t h e c a t e g o r y name , d e s c r i p t i o n , o r remove a c a t e g o r y . You c a n a l s o merge o r add new c a t e g o r i e s i f n e e d e d . Your e d i t s s h o u l d meet t h e *R e q u i r e m e n t s * s e c t i o n . − The c l u s t e r l i s t s h o u l d be a ** f l a t l i s t ** o f ** m u t u a l l y e x c l u s i v e ** c a t e g o r i e s . S o r t them b a s e d on t h e i r s e m a n t i c r e l a t e d n e s s . − You c a n h av e * f e w e r t h a n {{ m a x n u m c l u s t e r s }} c a t e g o r i e s * i n t h e c l u s t e r l i s t , b u t ** do n o t e x c e e d t h e l i m i t . * * − Be ** s p e c i f i c ** a b o u t e a c h c a t e g o r y . **Do n o t i n c l u d e vague c a t e g o r i e s ** s u c h a s ” O t h e r ” , ” G e n e r a l ” , ” U n c l e a r ” , ” M i s c e l l a n e o u s ” o r ” U n d e f i n e d ” i n t h e c l u s t e r l i s t . − You c a n i g n o r e low q u a l i t y o r ambiguous d a t a p o i n t s . ## Q4 : I f you d e c i d e t o e d i t t h e r e f e r e n c e l i s t , p l e a s e p r o v i d e y o u r u p d a t e d r e f e r e n c e l i s t . I f you d e c i d e n o t t o e d i t t h e r e f e r e n c e l i s t , p l e a s e o u t p u t t h e o r i g i n a l r e f e r e n c e l i s t . ## P r o v i d e y o u r a n s w e r s b e t w e e n t h e f o l l o w i n g t a g s : ” r a t i n g s c o r e ” − i n t e g e r b e t w e e n 0 and 100 ” e x p l a n a t i o n ” − e x p l a n a t i o n o f y o u r r a t i n g s c o r e w i t h i n {{ e x p l a n a t i o n l e n g t h }} 

words ” s u g g e s t i o n s ” − s u g g e s t e d e d i t s w i t h i n {{ s u g g e s t i o n l e n g t h }} words , o r ”N/A” i f no e d i t s n e e d e d ” c l u s t e r s ” − y o u r u p d a t e d c l u s t e r l i s t i f you d e c i d e d t o e d i t t h e r e f e r e n c e l i s t , o r t h e o r i g i n a l r e f e r e n c e l i s t i f no e d i t s made # O u t p u t A.1.4. T AXONOMY REVIEW PROMPT 

You a r e an e x p e r t a n a l y s t . Your j o b i s t o e v a l u a t e e v i d e n c e s t e p by s t e p , c o n s i d e r a l t e r n a t i v e s , and r e a c h a j u s t i f i e d c o n c l u s i o n . R e a s o n i n g : h i g h . # I n s t r u c t i o n ## C o n t e x t − ** Goal * * : Your g o a l i s t o r e v i e w t h e g i v e n r e f e r e n c e l i s t b a s e d on t h e r e q u i r e m e n t s and t h e s p e c i f i e d u s e c a s e , t h e n u p d a t e t h e r e f e r e n c e l i s t i f n e e d e d . − You w i l l be g i v e n a r e f e r e n c e c l u s t e r l i s t , which i s b u i l t on e x i s t i n g d a t a . The r e f e r e n c e l i s t w i l l be u s e d t o c l a s s i f y new d a t a p o i n t s . 20 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

− You w i l l compare t h e r e f e r e n c e l i s t w i t h t h e r e q u i r e m e n t s , o u t p u t a r a t i n g s c o r e o f t h e q u a l i t y o f t h e r e f e r e n c e l i s t , s u g g e s t p o t e n t i a l e d i t s , and u p d a t e t h e r e f e r e n c e l i s t i f n e e d e d . − ** R e f e r e n c e c l u s t e r l i s t * * : The i n p u t c l u s t e r l i s t i s i n JSON f o r m a t w i t h e a c h c l u s t e r a s a ” c l u s t e r ” e l e m e n t , c o n t a i n i n g t h e f o l l o w i n g sub − e l e m e n t s : − ** i d * * : c a t e g o r y i n d e x . − ** name * * : c a t e g o r y name . − ** d e s c r i p t i o n * * : c a t e g o r y d e s c r i p t i o n u s e d t o c l a s s i f y d a t a p o i n t s . − ** Use c a s e * * : Review t h e taxonomy t h a t c a t e g o r i z e s model e r r o r s b a s e d on t h e s p e c i f i c s k i l l s t h e model f a i l e d t o d e m o n s t r a t e i n e a c h example . 

{% i f p a r e n t c a t e g o r y %}

A l l o f t h e s e e r r o r s ha ve a l r e a d y b een l a b e l e d u n d e r t h e c a t e g o r y : *{{ 

p a r e n t c a t e g o r y } } * . P l e a s e e n s u r e t h a t you a s s i g n e a c h e r r o r t o more s p e c i f i c and i n f o r m a t i v e sub − c a t e g o r i e s t h a t go beyond t h e g e n e r a l l a b e l ”{{ p a r e n t c a t e g o r y } } ” . F ocus on i d e n t i f y i n g t h e u n d e r l y i n g s k i l l s o r e r r o r t y p e s t h a t p r o v i d e d e e p e r i n s i g h t . 

{% e n d i f %}

## R e q u i r e m e n t s ### Format − O u t p u t c l u s t e r s i n **JSON f o r m a t ** w i t h e a c h c l u s t e r a s an o b j e c t i n a ‘c l u s t e r s ‘ a r r a y . − Each c l u s t e r must i n c l u d e : − ‘ ” i d ” ‘ : c a t e g o r y number s t a r t i n g from 1 . − ‘ ” name ” ‘ : c a t e g o r y name ( max {{ c l u s t e r n a m e l e n g t h }} words ) , e i t h e r a v e r b o r noun p h r a s e . − ‘ ” d e s c r i p t i o n ” ‘ : c a t e g o r y d e s c r i p t i o n ( max {{ c l u s t e r d e s c r i p t i o n l e n g t h }} 

words ) . − Example : 

{

” c l u s t e r s ” : [

{

” i d ” : 1 , ” name ” : ” F a c t u a l E r r o r ” , ” d e s c r i p t i o n ” : ” Model f a i l s t o r e t r i e v e o r g e n e r a t e a c c u r a t e i n f o r m a t i o n . ” 

}

]

}

− T o t a l number o f c a t e g o r i e s s h o u l d be ** no more t h a n {{ m a x n u m c l u s t e r s } } * * . − O u t p u t s h o u l d be i n ** E n g l i s h ** o n l y . ### Q u a l i t y − **No o v e r l a p o r c o n t r a d i c t i o n ** among t h e c a t e g o r i e s . − **Name** i s a c o n c i s e and c l e a r l a b e l f o r t h e c a t e g o r y , i d e n t i f i e s ** one s p e c i f i c s k i l l o r a b i l i t y o n l y * * . Use o n l y p h r a s e s t h a t a r e s p e c i f i c t o e a c h c a t e g o r y and a v o i d t h o s e t h a t a r e common t o a l l c a t e g o r i e s . − **Name** r e f l e c t s c o r e c a p a b i l i t i e s , n o t domain − s p e c i f i c c o n t e x t s , o r t e c h n i c a l c h o i c e s . 21 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Example : n o t ” I n c o r r e c t A n a t o m i c a l Knowledge ” b u t ” F a c t u a l E r r o r ” ( The i s s u e i s a b o u t f a c t u a l a c c u r a c y , n o t b i o l o g y s p e c i f i c a l l y ) . I f t h e i s s u e d o e s n o t c l e a r l y map t o a s p e c i f i c s k i l l , c l a s s i f y i t a s ” Hard t o A n a l y z e ” − t h i s a p p l i e s when t h e e r r o r i s ambiguous , s u b j e c t i v e , o r l a c k s s u f f i c i e n t c o n t e x t t o d e t e r m i n e i t s n a t u r e c o n f i d e n t l y . − ** D e s c r i p t i o n ** d i f f e r e n t i a t e s one c a t e g o r y from a n o t h e r . − **Name** and ** d e s c r i p t i o n ** c a n ** a c c u r a t e l y ** and ** c o n s i s t e n t l y ** c l a s s i f y new d a t a p o i n t s ** w i t h o u t a m b i g u i t y * * . − **Name** and ** d e s c r i p t i o n ** a r e * c o n s i s t e n t w i t h e a c h o t h e r * . − O u t p u t c l u s t e r s match t h e d a t a a s c l o s e l y a s p o s s i b l e , w i t h o u t m i s s i n g i m p o r t a n t c a t e g o r i e s o r a d d i n g u n n e c e s s a r y o n e s . − O u t p u t c l u s t e r s s h o u l d s t r i v e t o be o r t h o g o n a l , p r o v i d i n g s o l i d c o v e r a g e o f t h e t a r g e t domain . − O u t p u t c l u s t e r s s e r v e t h e g i v e n u s e c a s e w e l l . − O u t p u t c l u s t e r s s h o u l d be s p e c i f i c and m e a n i n g f u l . Do n o t i n v e n t c a t e g o r i e s t h a t a r e n o t i n t h e d a t a . # R e f e r e n c e c l u s t e r l i s t 

{{ c l u s t e r l i s t }} 

# Q u e s t i o n s ## Q1 : Review t h e g i v e n r e f e r e n c e l i s t and p r o v i d e a r a t i n g s c o r e . The r a t i n g s c o r e s h o u l d be an i n t e g e r b e t w e e n 0 and 1 0 0 , h i g h e r r a t i n g s c o r e means b e t t e r q u a l i t y . You s h o u l d c o n s i d e r t h e f o l l o w i n g f a c t o r s when r a t i n g t h e r e f e r e n c e c l u s t e r l i s t : − ** I n t r i n s i c q u a l i t y * * : − I f t h e c l u s t e r l i s t m e e t s t h e r e q u i r e d q u a l i t y w i t h c l e a r and c o n s i s t e n t c a t e g o r y names and d e s c r i p t i o n s , and no o v e r l a p o r c o n t r a d i c t i o n among t h e c a t e g o r i e s . − I f t h e c a t e g o r i e s i n t h e c l u s t e r l i s t a r e r e l e v a n t t o t h e s p e c i f i e d u s e c a s e . − I f t h e c l u s t e r l i s t d o e s n o t i n c l u d e any vague c a t e g o r i e s s u c h a s ”O t h e r ” , ” G e n e r a l ” , ” U n c l e a r ” , ” M i s c e l l a n e o u s ” o r ” U n d e f i n e d ” . − ** E x t r i n s i c q u a l i t y * * : − I f t h e c l u s t e r l i s t c a n a c c u r a t e l y and c o n s i s t e n t l y c l a s s i f y t h e i n p u t d a t a w i t h o u t a m b i g u i t y . − I f t h e r e a r e m i s s i n g c a t e g o r i e s i n t h e c l u s t e r l i s t t h a t a p p e a r i n t h e i n p u t d a t a . − I f t h e r e a r e u n n e c e s s a r y c a t e g o r i e s i n t h e c l u s t e r l i s t t h a t do n o t a p p e a r i n t h e i n p u t d a t a . ## Q2 : E x p l a i n y o u r r a t i n g s c o r e i n Q1 [ The e x p l a n a t i o n s h o u l d be c o n c i s e , b a s e d on t h e i n t r i n s i c and e x t r i n s i c q u a l i t i e s e v a l u a t e d i n Q1 ] . ## Q3 : Based on y o u r r e v i e w , d e c i d e i f you ne ed t o e d i t t h e r e f e r e n c e l i s t t o i m p r o v e i t s q u a l i t y . I f yes , s u g g e s t p o t e n t i a l e d i t s [ S u g g e s t i o n s s h o u l d be s p e c i f i c , a c t i o n a b l e , and w i t h i n t h e c o n s t r a i n t s o f t h e maximum number o f c a t e g o r i e s and u s e c a s e s p e c i f i c i t y ] . ## Q4 : I f you d e c i d e t o e d i t t h e r e f e r e n c e l i s t , p r o v i d e y o u r u p d a t e d r e f e r e n c e l i s t . I f you d e c i d e n o t t o e d i t t h e r e f e r e n c e l i s t , p l e a s e o u t p u t t h e o r i g i n a l r e f e r e n c e l i s t . ## P r o v i d e y o u r a n s w e r s b e t w e e n t h e f o l l o w i n g t a g s : 22 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

” r a t i n g s c o r e ” − i n t e g e r b e t w e e n 0 and 100 ” e x p l a n a t i o n ” − e x p l a n a t i o n o f y o u r r a t i n g s c o r e w i t h i n {{ e x p l a n a t i o n l e n g t h }} 

words ” s u g g e s t i o n s ” − s u g g e s t e d e d i t s w i t h i n {{ s u g g e s t i o n l e n g t h }} words , o r ”N/A” i f no e d i t s n e e d e d ” c l u s t e r s ” − y o u r u p d a t e d c l u s t e r l i s t i f you d e c i d e d t o e d i t t h e r e f e r e n c e l i s t , o r t h e o r i g i n a l r e f e r e n c e l i s t i f no e d i t s made # O u t p u t A.1.5. E RROR LABEL CLASSIFICATION PROMPT 

You a r e an e x p e r t a n a l y s t . Your j o b i s t o e v a l u a t e e v i d e n c e s t e p by s t e p , c o n s i d e r a l t e r n a t i v e s , and r e a c h a j u s t i f i e d c o n c l u s i o n . R e a s o n i n g : h i g h . Your t a s k i s t o u s e t h e p r o v i d e d taxonomy t o c a t e g o r i z e t h e o v e r a l l t o p i c o r i n t e n t o f e a c h e r r o r g e n e r a t e d by LLMs . F i r s t , h e r e i s t h e taxonomy t o u s e : 

{{ taxonomy }} 

To c o m p l e t e t h e t a s k : 1 . C a r e f u l l y r e a d t h r o u g h t h e e n t i r e {{ d a t a t y p e } } , which c o n t a i n s a l i s t o f e r r o r s . 2 . F or e a c h e r r o r , c o n s u l t t h e taxonomy and i d e n t i f y t h e ** s i n g l e most r e l e v a n t c a t e g o r y ** t h a t b e s t c a p t u r e s t h e o v e r a l l t o p i c o r i n t e n t o f t h a t s p e c i f i c e r r o r . 3 . I f no c a t e g o r y f i t s w e l l , u s e t h e c a t e g o r y ’ O t h e r ’ . 4 . O u t p u t t h e r e s u l t i n a JSON f o r m a t , where e a c h t u p l e c o n t a i n s t h e e r r o r t e x t and i t s a s s i g n e d c a t e g o r y . Use t h e f o l l o w i n g f o r m a t : 

{

” c l a s s i f i e d e r r o r s ” : [

{

” e r r o r t e x t ” : ” e r r o r t e x t 1 ” , ” c a t e g o r y ” : ” c a t e g o r y name 1” 

} ,. . . ]

}

5 . Do n o t a s s i g n m u l t i p l e c a t e g o r i e s t o a s i n g l e e r r o r . Choose o n l y one t h a t b e s t f i t s . That ’ s i t ! Think c a r e f u l l y and e x p l a i n y o u r r e a s o n i n g b e f o r e g i v i n g y o u r f i n a l c a t e g o r y c h o i c e f o r e a c h e r r o r . A s s i g n a s i n g l e c a t e g o r y t o e a c h o f t h e f o l l o w i n g e r r o r s : 

{{ d a t a }} 

23 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Respond w i t h y o u r c a t e g o r i e s w i t h i n j s o n f o r m a t , one p e r e r r o r . Do n o t i n c l u d e t h e number , j u s t t h e c a t e g o r y t e x t . 

A.2. Resulted Taxonomy Examples 

Example Num. 1 

Category: Missing Required Element (Omits mandatory sections, fields, identifiers, or other specified content.) 

Sub-category Missing Contextual Details (Key domain-specific or contextual information is not provided.) 

Subsub-category: Background Context Omission (Lacks overarching situational or background information necessary for understanding.) 

Label: Missing Historical Basis 

Error Summary: The response omits the essential historical facts (marriage to Queen Maria and subsequent regency) that explain how John of Brienne actually acquired the title of King of Jerusalem, relying instead on vague inspirational statements. Example Num. 2 

Category: Computation Error (Produces incorrect numerical, algebraic, or geometric results, including miscalculations and faulty derivations.) 

Sub-category Formula Misapplication Error (Using an incorrect or inappropriate formula for the problem domain.) 

Subsub-category: Spatial Geometry Error (Misuse of area, volume, perimeter, trigonometric, or other geometric relations.) 

Label: Misapplication of General Position Formula 

Error Summary: The model misapplied the general - position formula for the maximum number of regions created by n planes, assuming all nine planes are in general position, which they are not. This led to an inflated count of 166 instead of the correct 48. 24 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

# B. Experimental Setup 

We begin by introducing the conducted experiments, followed by a description of the general configuration shared across them, and conclude with a summary of the compute resources used for each experiment. We conducted three experiments, which provide examples for the flexible usage of our approach; (1) ErrorAtlas 

Construction (§3, §4): we sample from all selected data and models, (2) Model Comparison: we utilize the ErrorAtlas 

categories and run only stages 1 (Per-Instance Error Analysis) and 2.b (Error Assignment) in ErrorMap on all predictions of two Gemini models listed in the HELM Capabilities leaderboard. We then present a comparative evaluation between them in Section 5.1. (3) Dataset Taxonomy: We demonstrate the application of ErrorMap to generate a taxonomy tailored to a specific dataset, MMLU-Pro benchmark, in Section D.1. 

Failure Threshold ErrorMap relies on a distinction between failed and successful instances, in non-binary metrics we make this distinction through a threshold. For each benchmark, we rely on a single metric (the primary score in the benchmark if there are multiple), and define for each range of scores what is the threshold considered as error. 5 For datasets evaluated with a binary score, the selection is straightforward. For others, we found that using an approximation of 0.7% of the maximum score per instance yields good results. 

Taxonomy Parameters The error categorization had to be well-defined in each of its prompts to provide a specific output. As part of this stage, and following the approach described in Wan et al. (2024), we defined a set of parameters tailored to our case, such as error label batch size, maximum length for category names, and others. The complete list of parameters and their corresponding values is provided in Table 4. 

Selected Judge All experiments were conducted using the gpt-oss-120b model (OpenAI, 2025), chosen for its scale and relevance to current state-of-the-art systems. To better leverage its strong reasoning capabilities, we adapted the prompts accordingly. We add maximum 3 ICP (if any exist) to each prompt. 

Compute The required compute for ErrorMap depends on the number of incorrect predictions. ErrorAtlas creation required approximately 7,200 inferences. Since most of these can run in parallel, the process took approximately 3 hours. The Gemini model comparison required about 2,000 inferences. The MMLU-Pro experiment required approximately 3,500 

inferences. 

Reliability validation We used the sentence-transformers/all-MiniLM-L6-v2 model. Changing the em-bedder did not change results, maybe because our task goes beyond textual similarity and aims to capture the underlying skills implied by the labels. 

Statistical Significance Test To assess whether the differences in model distributions are statistically significant, we conducted pairwise comparisons between models. Specifically, we used binomial probability tests to evaluate the likelihood that the observed performance of a weaker model could occur under the distribution of a stronger one. 

> 5A higher threshold is preferred over a lower one, as it prevents false negatives, ensuring that genuine errors are not mistakenly excluded. False positives, on the other hand, are mostly filtered out during instance-level analysis.

25 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

# C. ErrorAtlas Details and Results 

Dataset Name Benchmark 

MMLU-Pro HELM Capabilities GPQA OmniMATH WildBench IFEval ACIBench MedHELM MedDialog (healthcare magic) MedDialog (icliniq) MEDEC MediQA MedicationQA MTSamples procedures MTSamples replicate Live Multiple BFCL Live Parallel Multiple Live Parallel Live Simple Multi Turn Base Multi Turn Long Context Multi Turn Miss Func Multi Turn Miss Param Multiple Parallel Multiple Parallel Simple JAVA Simple Javascript Simple Python QTSumm ToRR NumericNLG SciGen TableBench (data analysis) HumanEval —HumanEval+ MBPP MBPP+   

> Table 5. List of datasets used to create ErrorAtlas .

26 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Error Category Num. of Datasets Num. of Models Prevalence (%) Missing Required Element 31 82 15.56 Specification Misinterpretation 27 70 11.5 Logical Reasoning Error 25 56 9.09 Incorrect Identification 21 56 8.98 Computation Error 22 54 8.45 Output Formatting Error 23 76 7.7 Irrelevant/Extraneous Content 24 53 7.06 Factual Error 22 49 5.72 Incomplete Reasoning 17 44 5.03 Counting/Enumeration Error 11 44 4.65 Answer Selection Error 16 37 4.39 Naming/Symbol Error 18 29 3.16 Tool/API Usage Error 16 19 2.83 Unit Conversion Error 12 23 1.82 Inappropriate Refusal 8 25 1.39 False Positive Detection 8 16 1.02 Other 4 15 0.8 Error Detection Failure 6 11 0.7 Hard to Analyze 3 3 0.16    

> Table 6. ErrorAtlas Statistics: The original categories generated by gpt-oss-120b , with the number of models and datasets classified to each category, along with their prevalence percentages.

error category count percentage Logical Reasoning Error 947 35 Computation Error 613 22 Incorrect Identification 317 12 Missing Required Element 209 8Specification Misinterpretation 173 6Counting/Enumeration Error 148 5Answer Selection Error 114 4Factual Error 69 3Incomplete Reasoning 91 3Unit Conversion Error 34 1Error Detection Failure 4 0Irrelevant/Extraneous Content 10 0Naming/Symbol Error 7 0Other 4 0Output Formatting Error 2 0Tool/API Usage Error 2 0 

> Table 7. MMLU-Pro, GPQA and Omni MATH error category statistics.

27 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Depth 1 Category Depth 2 Subcategory Depth 3 Sub-subcategory Logical Reasoning Error Misapplication of Rule Legal/Regulatory Rule Misapplication; Inequality Misuse Faulty Logical Inference Incorrect Base/Inductive Step; Misinterpreted Problem Structure Missing Required Element Missing Output Generation Answer -Choice Mapping Failure; Empty or Irrele-vant Output Missing Prompt Repetition Exact Repetition Omission Computation Error Floating-Point Precision Errors –Parameter Value Assignment Errors Numeric Value Error; Parameter Omission Incorrect Identification Goal/Objective Identification Errors –Role/Function Identification Errors –Specification Misinterpreta-tion Clarification Request Misinterpreta-tion –Constraint Misinterpretation Constraint Meaning Misinterpretation; Hard to An-alyze Output Formatting Error Length and Count Constraint Viola-tions Character Count Violation; Formatting Structure Violation Paragraph and Section Structure Er-rors Document-level Structure Errors; Paragraph Boundary Errors Irrelevant/Extraneous Con-tent Topic Irrelevance Scope Overextension; Topic Misalignment Redundant Verbosity Overly Complex Language; Redundant Repetition Counting/Enumeration Error Aggregation Error –Constraint Ignorance Constraint Misapplication; Constraint Omission Answer Selection Error Answer Choice Formatting Error –Answer Choice Mapping Error Correct Reasoning, Wrong Choice; Result-to-Choice Mapping Incomplete Reasoning Absent Empirical Evidence –Assumption Omission –Factual Error Unsupported Speculation –Terminology Misuse –Tool/API Usage Error Missing Prerequisite Data Retrieval –Missing or Wrong Directory Navi-gation Incorrect Directory Path; Missing Directory Change/Creation Naming/Symbol Error Identifier Naming Mistakes Contextual Identifier Misuse; Naming Syntax Vio-lation Argument Type or Value Mismatch Argument Type Mismatch; Argument Value or Or-dering Mismatch Inappropriate Refusal Failure to Refuse Disallowed –Style/Tone Compliance Refusal –Unit Conversion Error Arithmetic/Formula Unit Error –Inconsistent Unit Usage –False Positive Detection Improper Content Filter Trigger –Wrong Entity Absence Assumption –Error Detection Failure Factual Error Detection –Policy Violation Detection – 

> Table 8. Hierarchy of error taxonomy: sampled examples of depth-2 and depth-3 per category in depth-1.

28 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

C.1. Examples 

C.1.1. I NCOMPLETE RESPONSE EXAMPLES 

C o n t e x t : \ \ 

S o l v e t h e e q u a t i o n $a ˆ 3 + b ˆ 3 + c ˆ 3 = 2001 $ i n p o s i t i v e i n t e g e r s . R e f e r e n c e s : \ \ 

$\ boxed {\ l e f t [ ( 1 0 , 1 0 , 1 ) , ( 1 0 , 1 , 1 0 ) , ( 1 , 1 0 , 1 0 ) \ r i g h t ] } $i n c o r r e c t p r e d i c t i o n : 

\\ We a r e a s k e d t o s o l v e t h e e q u a t i o n $a ˆ 3 + b ˆ 3 + c ˆ 3 = 2001 $ i n p o s i t i v e i n t e g e r s . 

\\ We c a n o b s e r v e t h a t $1 ˆ 3 = 1$ , $2 ˆ 3 = 8$ , $3 ˆ 3 = 27$ , $4 ˆ 3 = 64$ , $5 ˆ 3 = 125 $ , $6 ˆ 3 = 216 $ , $7 ˆ 3 = 343 $ , $8 ˆ 3 = 512 $ , $9 ˆ 3 = 729 $ , $10 ˆ 3 = 1000 $ , $11 ˆ 3 =1331 $ , $12 ˆ 3 = 1728 $ , $13 ˆ 3 = 2197 $ . 

\\ S i n c e $a , b , c$ a r e p o s i t i v e i n t e g e r s , we must h ave $a , b , c \ l e 12 $ . 

\\ Also , we c a n assume w i t h o u t l o s s o f g e n e r a l i t y t h a t $a \ ge b \ ge c \ ge 1$ . 

\\ I f $a =12$ , t h e n $a ˆ 3 = 1728 $ , s o $b ˆ 3 + c ˆ 3 = 2001 − 1728 = 273 $ . 

\\ I f $b =6$ , t h e n $b ˆ 3 = 216 $ , s o $c ˆ 3 = 273 − 216 = 57$ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =5$ , t h e n $b ˆ 3 = 125 $ , s o $c ˆ 3 = 273 − 125 = 148 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =4$ , t h e n $b ˆ 3 = 64$ , s o $c ˆ 3 = 273 − 64 = 209 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =3$ , t h e n $b ˆ 3 = 27$ , s o $c ˆ 3 = 273 − 27 = 246 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =2$ , t h e n $b ˆ 3 = 8$ , s o $c ˆ 3 = 273 − 8 = 265 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =1$ , t h e n $b ˆ 3 = 1$ , s o $c ˆ 3 = 273 − 1 = 272 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $a =11$ , t h e n $a ˆ 3 = 1331 $ , s o $b ˆ 3 + c ˆ 3 = 2001 − 1331 = 670 $ . 

\\ I f $b =8$ , t h e n $b ˆ 3 = 512 $ , s o $c ˆ 3 = 670 − 512 = 158 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =7$ , t h e n $b ˆ 3 = 343 $ , s o $c ˆ 3 = 670 − 343 = 327 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =6$ , t h e n $b ˆ 3 = 216 $ , s o $c ˆ 3 = 670 − 216 = 454 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =5$ , t h e n $b ˆ 3 = 125 $ , s o $c ˆ 3 = 670 − 125 = 545 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =4$ , t h e n $b ˆ 3 = 64$ , s o $c ˆ 3 = 670 − 64 = 606 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =3$ , t h e n $b ˆ 3 = 27$ , s o $c ˆ 3 = 670 − 27 = 643 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =2$ , t h e n $b ˆ 3 = 8$ , s o $c ˆ 3 = 670 − 8 = 662 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $b =1$ , t h e n $b ˆ 3 = 1$ , s o $c ˆ 3 = 670 − 1 = 669 $ , which i s n o t a p e r f e c t cu be . 

\\ I f $ a =10$ , t h e n $a ˆ 3 = 1000 $ , s o $b ˆ 3 + c ˆ 3 = 2001 − 1000 = 1001 $ . 

\\ I f $ b =10$ , t h e n $b ˆ 3 = 1000 $ , s o $c ˆ 3 = 1001 − 1000 = 1$ , s o $c =1$ . Thus , $ ( 1 0 , 1 0 , 1 ) $ i s a s o l u t i o n . 

\\ I f $ a =1$ , $b =10$ , $c =10$ , t h e n $1 ˆ 3 + 1 0 ˆ 3 + 1 0 ˆ 3 = 1 + 1000 + 1000 = 2001 $ . Thus , $ ( 1 , 1 0 , 1 0 ) $ i s a s o l u t i o n . 29 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

\\ I f $ a =10$ , $b =1$ , $c =10$ , t h e n $10 ˆ 3 + 1 ˆ 3 + 1 0 ˆ 3 = 1000 + 1 + 1000 = 2001 $ . Thus , $ ( 1 0 , 1 , 1 0 ) $ i s a s o l u t i o n . 

\\ I f $a =10$ , $b =10$ , $c =1$ , t h e n $10 ˆ 3 + 1 0 ˆ 3 + 1 ˆ 3 = 1000 + 1000 + 1 = 2001 $ . 

\\ Thus , $ ( 1 0 , 1 0 , 1 ) $ i s a s o l u t i o n . F i n a l Answer : 

\\ The f i n a l a n s w e r i s $\ boxed { ( 1 , 1 0 , 1 0 ) }$%

Listing 1. Sample JSON Output  

> 12

{ 

> 3

"required_criteria": [ 

> 4

{ 

> 5

"criterion": "Establish an upper bound for the variables (show that a, b, c $\leq$ 12).",  

> 6

"present_in_wrong": true,  

> 7

"quality": "correct",  

> 8

"evidence": "Since $a, b, c$ are positive integers, we must have $a, b, c \\le 12$ .",  

> 9

"comment": "The bound is correctly derived from $13ˆ3 > 2001$."  

> 10

},  

> 11

{ 

> 12

"criterion": "Assume without loss of generality an ordering (e.g., a $\geq$ b $\geq$ c) to reduce duplicate work.",  

> 13

"present_in_wrong": true,  

> 14

"quality": "correct",  

> 15

"evidence": "Also, we can assume without loss of generality that $a \\ge b \\ge c \\ ge 1$.",  

> 16

"comment": "Proper ordering is stated."  

> 17

},  

> 18

{ 

> 19

"criterion": "Systematically examine all feasible values of a (starting from the maximum) and for each check whether the remaining sum can be expressed as two cubes.",  

> 20

"present_in_wrong": true,  

> 21

"quality": "partially correct",  

> 22

"evidence": "The response checks a $\eq$ 12, 11, 10 but stops there; it does not examine a $\eq$ 9, 8, $\ldots$ down to the lower bound.",  

> 23

"comment": "Missing cases means the search is incomplete; the criterion is only partially satisfied."  

> 24

},  

> 25

{ 

> 26

"criterion": "Identify the concrete solution(s) when a = 10 (i.e., b = 10, c = 1).",  

> 27

"present_in_wrong": true,  

> 28

"quality": "correct",  

> 29

"evidence": "If $a=10$, then $bˆ3 + cˆ3 = 1001$ $\ldots$ $b=10$, $c=1$ $\ldots$ Thus , $(10,10,1)$ is a solution.",  

> 30

"comment": "Correctly finds the core solution."  

> 31

},  

> 32

{ 

> 33

"criterion": "Account for all permutations of the found unordered triple to list every ordered solution.",  

> 34

"present_in_wrong": true,  

> 35

"quality": "partially correct",  

> 36

"evidence": "The answer mentions $(1,10,10)$ and $(10,1,10)$ but the final boxed answer only shows $(1,10,10)‘. ",  

> 37

"comment": "Permutations are recognized, yet the final response lists only one of the three ordered triples."  

> 38

},  

> 39

{ 

> 40

"criterion": "Conclude that the listed permutations are the only positive-integer 

30 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models                                                                       

> solutions to
> 41 $aˆ3+bˆ3+cˆ3=2001$.",
> 42 "present_in_wrong": false,
> 43 "quality": "incorrect",
> 44 "evidence": "",
> 45 "comment": "The response never explicitly states that no other solutions exist."
> 46 }
> 47 ],
> 48 "final_answer": {
> 49 "error_summary": "The answer fails to present the complete set of solutions and does not demonstrate that no other triples exist; it only outputs one ordered triple while ignoring the other two permutations and omits afinal uniqueness argument.",
> 50 "error_title": "Incomplete solution set"
> 51 }
> 52 }
> 53 54 Example_id: id3293
> 55 Model: google/gemini-2.0-flash-lite-preview-02-05
> 56 Dataset: omni_math

C o n t e x t : what d o e s t a s m a r c o n t a i n R e f e r e n c e s : ACTIVE INGREDIENT / ACTIVE MOIETY T o l c a p o n e ( UNII : CIF6334OLY ) INACTIVE INGREDIENTS L a c t o s e Monohydrate ( UNII : EWQ57Q8I5X ) p o v i d o n e K30 ( UNII : U725QWY32X ) sodium s t a r c h g l y c o l a t e t y p e a p o t a t o ( UNII : 5856 J3G2A2 ) t a l c ( UNII : 7SEV7J4R1U ) magnesium s t e a r a t e ( UNII : 70097 M6I30 ) t i t a n i u m d i o x i d e ( UNII : 15 FIX9V2JP ) t r i a c e t i n ( UNII : XHX3C3X673 ) sodium l a u r y l s u l f a t e ( UNII : 368 GB5141J ) f e r r i c o x i d e y e l l o w ( UNII : EX438O2MRT ) f e r r i c o x i d e r e d ( UNII : 1K09F3G675 ) MICROCRYSTALLINE CELLULOSE ( UNII : OP1R32D61U ) ANHYDROUS DIBASIC CALCIUM PHOSPHATE ( UNII : L11K75P92J ) ETHYLCELLULOSE, UNSPECIFIED ( UNII : 7Z8S9VYZ4B ) i n c o r r e c t p r e d i c t i o n : Tasmar i s a b r a n d name f o r t h e d r u g t o l c a p o n e . T o l c a p o n e i s a m e d i c a t i o n u s e d i n t h e t r e a t m e n t o f P a r k i n s o n ’ s d i s e a s e . I t works a s a c a t e c h o l −O− m e t h y l t r a n s f e r a s e (COMT) i n h i b i t o r , which h e l p s t o i n c r e a s e t h e l e v e l s and d u r a t i o n o f a c t i o n o f l e v o d o p a , a common m e d i c a t i o n u s e d t o manage P a r k i n s o n ’ s symptoms . Tasmar i s t y p i c a l l y u s e d i n c o m b i n a t i o n w i t h l e v o d o p a / c a r b i d o p a t h e r a p y t o i m p r o v e i t s e f f e c t i v e n e s s . I t i s i m p o r t a n t t o n o t e t h a t Tasmar c a n ha ve s e r i o u s s i d e e f f e c t s , i n c l u d i n g l i v e r t o x i c i t y , s o i t s u s e i s c a r e f u l l y m o n i t o r e d by h e a l t h c a r e p r o v i d e r s .      

> Listing 2. Sample JSON Output
> 1{
> 2"required_criteria": [
> 3{

31 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models                                                                                                           

> 4"criterion": "State the active ingredient of Tasmar",
> 5"present_in_wrong": true,
> 6"quality": "correct",
> 7"evidence": "Tasmar is abrand name for the drug tolcapone.",
> 8"comment": "The response correctly identifies Tolcapone as the active ingredient."
> 9},
> 10 {
> 11 "criterion": "List all inactive ingredients of Tasmar as given in the reference",
> 12 "present_in_wrong": false,
> 13 "quality": "incorrect",
> 14 "evidence": "",
> 15 "comment": "The response does not provide any of the inactive ingredients; it instead discusses clinical use and safety."
> 16 }
> 17 ],
> 18 "final_answer": {
> 19 "error_summary": "The model focused on the pharmacological description of Tasmar rather than enumerating its ingredient composition, omitting the required list of inactive ingredients.",
> 20 "error_title": "Omission of Required Ingredient List"
> 21 }
> 22 }
> 23 24 example_id: id553
> 25 model: openai/gpt-4o-2024-05-13
> 26 dataset: medhelm_v2_medication_qa

C.1.2. Q UESTION MISINTERPRETATION EXAMPLES 

C o n t e x t : T h i s q u e s t i o n r e f e r s t o t h e f o l l o w i n g i n f o r m a t i o n . ” To s l a c k e n t h e tempo would mean f a l l i n g b e h i n d . And t h o s e who f a l l b e h i n d g e t b e a t e n . But we do n o t want t o be b e a t e n . No , we r e f u s e t o be b e a t e n ! One f e a t u r e o f t h e h i s t o r y o f o l d R u s s i a was t h e c o n t i n u a l b e a t i n g s s h e s u f f e r e d b e c a u s e o f h e r b a c k w a r d n e s s . She was b e a t e n by t h e Mongol k h a n s . She was b e a t e n by t h e T u r k i s h b e y s . She was b e a t e n by t h e Swedish f e u d a l l o r d s . She was b e a t e n by t h e P o l i s h and L i t h u a n i a n g e n t r y . She was b e a t e n by t h e B r i t i s h and F r e n c h c a p i t a l i s t s . She was b e a t e n by t h e J a p a n e s e b a r o n s . A l l b e a t h e r −− b e c a u s e o f h e r b a c k w a r d n e s s , b e c a u s e o f h e r m i l i t a r y b a c k w a r d n e s s , c u l t u r a l b a c k w a r d n e s s , p o l i t i c a l b a c k w a r d n e s s , i n d u s t r i a l b a c k w a r d n e s s , a g r i c u l t u r a l b a c k w a r d n e s s . They b e a t h e r b e c a u s e i t was p r o f i t a b l e and c o u l d be done w i t h i m p u n i t y . You remember t h e words o f t h e p r e − r e v o l u t i o n a r y p o e t : ”You a r e p o o r and a b u n d a n t , m i g h t y and i m p o t e n t , Mother R u s s i a . ” Those g e n t l e m e n were q u i t e f a m i l i a r w i t h t h e v e r s e s o f t h e o l d p o e t . They b e a t h e r , s a y i n g : ”You a r e a b u n d a n t , ” s o one c a n e n r i c h o n e s e l f a t y o u r e x p e n s e . They b e a t h e r , s a y i n g : ”You a r e p o o r and i m p o t e n t , ” s o you c a n be b e a t e n and p l u n d e r e d w i t h i m p u n i t y . Such i s t h e law o f t h e e x p l o i t e r s −− t o b e a t t h e backward and t h e weak . I t i s t h e j u n g l e law o f c a p i t a l i s m . You a r e backward , you a r e weak −− t h e r e f o r e you a r e wrong ; h e n c e you c a n be b e a t e n and e n s l a v e d . You a r e mighty −− t h e r e f o r e you a r e r i g h t ; h e n c e we must be wary o f you . T h a t i s why we must no l o n g e r l a g b e h i n d . ” J o s e p h S t a l i n , s p e e c h d e l i v e r e d a t t h e f i r s t A l l −Union C o n f e r e n c e o f L e a d i n g P e r s o n n e l o f S o c i a l i s t I n d u s t r y , F e b r u a r y 4 , 1931 32 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

The r e f e r e n c e t o t h e ” c o n t i n u a l b e a t i n g s ” most d i r e c t l y s u p p o r t s which o f t h e f o l l o w i n g d e c i s i o n s i n l a t e r S o v i e t f o r e i g n p o l i c y ? C a n d i d a t e Answers : A) The d e c l a r a t i o n o f war a g a i n s t J a p a n a t t h e end o f World War I I B) The b l o c k a d e o f B e r l i n C) The s u p p o r t f o r Cuban R e v o l u t i o n D) The d e c i s i o n t o b u i l d t h e B e r l i n Wall E ) The i m p o s i t i o n o f t h e B r e z h n e v D o c t r i n e a g a i n s t a n t i − S o v i e t p r o t e s t e r s i n Europe F ) The s i g n i n g o f t h e Molotov − R i b b e n t r o p P a c t G) The l a u n c h o f S p u t n i k H) The d i p l o m a t i c r e c o g n i t i o n o f t h e P e o p l e ’ s R e p u b l i c o f China I ) The i n v a s i o n o f A f g h a n i s t a n J ) The c r e a t i o n o f t h e Warsaw P a c t R e f e r e n c e s : Ji n c o r r e c t p r e d i c t i o n : The p a s s a g e e m p h a s i z e s t h e h i s t o r i c a l v u l n e r a b i l i t y and b a c k w a r d n e s s o f R u s s i a , which l e d t o e x p l o i t a t i o n and a g g r e s s i o n by f o r e i g n powers . S t a l i n ’ s s p e e c h h i g h l i g h t s t h e ne ed t o a v o i d l a g g i n g b e h i n d i n i n d u s t r i a l and m i l i t a r y s t r e n g t h t o p r e v e n t f u t u r e e x p l o i t a t i o n . T h i s s e n t i m e n t a l i g n s w i t h t h e S o v i e t Union ’ s d e c i s i o n t o a s s e r t i t s power and i n f l u e n c e t o a v o i d b e i n g p e r c e i v e d a s weak o r backward . Let ’ s e v a l u a t e t h e o p t i o n s : − **A . The d e c l a r a t i o n o f war a g a i n s t J a p a n a t t h e end o f World War I I : * * T h i s d e c i s i o n was d r i v e n by S o v i e t s t r a t e g i c i n t e r e s t s and t h e d e s i r e t o r e g a i n t e r r i t o r y , b u t i t i s n o t d i r e c t l y t i e d t o t h e theme o f a v o i d i n g b a c k w a r d n e s s . − **B . The b l o c k a d e o f B e r l i n : * * T h i s was a Cold War s t r a t e g y t o a s s e r t S o v i e t power i n Europe , b u t i t d o e s n o t d i r e c t l y r e l a t e t o t h e h i s t o r i c a l theme o f b a c k w a r d n e s s . − **C . The s u p p o r t f o r Cuban R e v o l u t i o n : * * S u p p o r t i n g r e v o l u t i o n s a b r o a d was p a r t o f S o v i e t e f f o r t s t o expand i n f l u e n c e , b u t i t i s n o t d i r e c t l y t i e d t o t h e theme o f b a c k w a r d n e s s . − **D . The d e c i s i o n t o b u i l d t h e B e r l i n Wall : * * T h i s was a m e a s u r e t o p r e v e n t E a s t Germans from f l e e i n g t o t h e West , b u t i t i s n o t d i r e c t l y t i e d t o t h e theme o f b a c k w a r d n e s s . − **E . The i m p o s i t i o n o f t h e B r e z h n e v D o c t r i n e a g a i n s t a n t i − S o v i e t p r o t e s t e r s i n Europe : * * T h i s was a p o l i c y t o m a i n t a i n S o v i e t c o n t r o l o v e r E a s t e r n Europe , b u t i t d o e s n o t d i r e c t l y a d d r e s s t h e theme o f b a c k w a r d n e s s . − **F . The s i g n i n g o f t h e Molotov − R i b b e n t r o p P a c t : * * T h i s non − a g g r e s s i o n p a c t w i t h Na zi Germany was a p r a g m a t i c d e c i s i o n t o buy t i m e and a v o i d c o n f l i c t , b u t i t d o e s n o t d i r e c t l y r e l a t e t o t h e theme o f b a c k w a r d n e s s . − **G . The l a u n c h o f S p u t n i k : * * T h i s was a d e m o n s t r a t i o n o f S o v i e t t e c h n o l o g i c a l a d v a n c e m e n t and s t r e n g t h , d i r e c t l y a d d r e s s i n g t h e n ee d t o a v o i d b e i n g p e r c e i v e d a s backward . − **H . The d i p l o m a t i c r e c o g n i t i o n o f t h e P e o p l e ’ s R e p u b l i c o f China : * * T h i s was ag e o p o l i t i c a l move , b u t i t d o e s n o t d i r e c t l y r e l a t e t o t h e theme o f b a c k w a r d n e s s . 33 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

− ** I . The i n v a s i o n o f A f g h a n i s t a n : * * T h i s was an a t t e m p t t o expand S o v i e t i n f l u e n c e , b u t i t d o e s n o t d i r e c t l y a d d r e s s t h e theme o f b a c k w a r d n e s s . − ** J . The c r e a t i o n o f t h e Warsaw P a c t : * * T h i s was a m i l i t a r y a l l i a n c e t o c o u n t e r NATO, a s s e r t i n g S o v i e t s t r e n g t h , b u t i t i s n o t d i r e c t l y t i e d t o t h e theme o f b a c k w a r d n e s s . The ** l a u n c h o f S p u t n i k ** (G) b e s t a l i g n s w i t h t h e theme o f S t a l i n ’ s s p e e c h , a s i t d e m o n s t r a t e d S o v i e t t e c h n o l o g i c a l and i n d u s t r i a l p r o g r e s s , c o u n t e r i n g t h e h i s t o r i c a l n a r r a t i v e o f b a c k w a r d n e s s and v u l n e r a b i l i t y . The c o r r e c t a n s w e r i s (G) . 

Listing 3. Sample JSON Output  

> 1

{ 

> 2

"required_criteria": [ 

> 3

{ 

> 4

"criterion": "Recognize that the passage stresses historical exploitation of Russia because of its backwardness and the need to avoid lagging behind militarily.",  

> 5

"present_in_wrong": true,  

> 6

"quality": "partially correct",  

> 7

"evidence": "The model notes: \"The passage emphasizes the historical vulnerability and backwardness of Russia, which led to exploitation and aggression by foreign powers.\"",  

> 8

"comment": "The model correctly identifies the theme of backwardness, but does not link it specifically to a security-oriented foreign-policy response."  

> 9

},  

> 10

{ 

> 11

"criterion": "Link the theme of avoiding backwardness to a foreign-policy decision that provides collective security against external powers.",  

> 12

"present_in_wrong": true,  

> 13

"quality": "incorrect",  

> 14

"evidence": "The model argues that the launch of Sputnik \"directly addresses the need to avoid being perceived as backward.\"",  

> 15

"comment": "Sputnik is a technological showcase, not a collective security measure; the passage calls for defending against exploitation, not a prestige project."  

> 16

},  

> 17

{ 

> 18

"criterion": "Exclude options that pertain to internal achievements or isolated diplomatic moves that do not create a defensive alliance.",  

> 19

"present_in_wrong": true,  

> 20

"quality": "incorrect",  

> 21

"evidence": "The model dismisses many options (e.g., Berlin Wall, Brezhnev Doctrine) as ‘‘not directly tied to the theme of backwardness,’’ yet incorrectly selects Sputnik, also an internal achievement.",  

> 22

"comment": "The exclusion reasoning is inconsistent; the correct exclusion should keep the Warsaw Pact and reject purely symbolic actions."  

> 23

},  

> 24

{ 

> 25

"criterion": "Select the Warsaw Pact (option J) as the decision that most directly embodies the passage’s call for collective defense against exploitation.",  

> 26

"present_in_wrong": false,  

> 27

"quality": "incorrect",  

> 28

"evidence": "The model never mentions the Warsaw Pact and instead chooses option G ( Sputnik).",  

> 29

"comment": "Failure to consider the Warsaw Pact indicates the model missed the central security implication of the passage."  

> 30

} 

> 31

],  

> 32

"final_answer": { 

> 33

"error_summary": "The model misinterpreted the passage’s emphasis on historical exploitation and the need for collective military security, treating it as a call 

34 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

for technological prestige and consequently selecting Sputnik instead of the Warsaw Pact.",  

> 34

"error_title": "Misinterpretation of Historical Context Leading to Wrong Policy Choice " 

> 35

} 

> 36

} 

> 37

model: deepseek-ai/deepseek-v3  

> 38

dataset: mmlu_pro_old  

> 39

example_id: id5031 

35 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

# D. ErrorMap Applicability 

D.1. Benchmark Stakeholders 0.0 0.2 0.4 0.6 0.8 1.0 

> Model Errors (%)
> health
> history
> biology
> psychology
> other
> law
> philosophy
> economics
> engineering
> business
> chemistry
> computer science
> physics
> math
> Factual Error
> Incomplete Answer
> Logical Reasoning Error
> Mathematical Mistake
> Prompt Misinterpretation

Figure 5. Differences in error category distributions across domain categories in MMLU-Pro. Error Category P-val ( ↓)Factual Error .000218 Incomplete Answer .000000 Logical Reasoning Error .000333 Mathematical Mistake .002563 Prompt Misinterpretation .074530 

Table 9. Significance testing between the best- and worst-performing models for each error category. The results show that differences between models are usually statistically significant. 

# E. ErrorMap Evaluation 

E.1. Per-Instance Error Analysis 

Two key aspects of the per-instance analysis stage are accuracy and robustness. While accuracy measures whether the judge assigned a correct label to the error, robustness can be evaluated in multiple ways (Habba et al., 2025). In this work, we adopt a commonly used approach to measure robustness by examining the model’s sensitivity to prompt variations (Pezeshkpour & Hruschka, 2023; Errica et al., 2024; Zhuo et al., 2024). To evaluate accuracy , we provided the meta-judge with all the information given to the original judge and its proposed analysis. Notably, the meta-judge’s task is significantly simpler than that of the analysis component. While the judge must generate a coherent explanation for the error, often requiring reasoning across multiple steps, the meta-judge only verifies whether the given analysis correctly explains the error. In other words, the judge performs a binary classification (correct/incorrect) based on a predefined context, without needing to produce or synthesize new information. The meta-judge accepted the vast majority of instances in each case with an average score of 91.1% (see Table 10). For assessing robustness , we followed the approach of Kamoi et al. (2024), executing the per-instance stage with 3 prompt variations. We then compared the consistency of the 2 resulting error analyses with the original prompt setup. This comparison is challenging to automate, as the error labels are free-form and may differ in non-relevant ways (e.g., style or level of generality). To approximate a quantitative measure, we computed pairwise cosine similarity between error labels 36 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

from the original and varied prompts using Sentence-BERT embeddings (Reimers & Gurevych, 2019)(details in App.B). The average similarity score was moderate 53% . we analyzed manually 100 examples, and found them to be divided into the following categories: 45% included the same underlying concept, but one phrase was partial to another in the text. Another 30% suffered from varying specificity in the labels and not disagreements. For instance, the labels “missing temporal specification for midnight setting” and “missing explicit time reference” received a similarity score of 0.21 . And 

25% included a different error, and this may stem from the soft nature of errors, that may be called in multiple ways. Stage Experiment Accuracy (%) Per-Instance 

ErrorAtlas .89 

Gemini comparison .90 

MMLU-Pro taxonomy .94 

Taxonomy 

ErrorAtlas .92 

Gemini comparison .93 

MMLU-Pro taxonomy .91     

> Table 10. ErrorMap and ErrorAtlas Evaluation results.

E.2. Robustness Analysis For ErrorAtlas 

To evaluate the robustness of the taxonomy construction stage, we used the high level categories aggregated in ErrorAtlas .We compared its high-level results created by gpt-oss-120b using 10% of the data to two interesting cases: 1. Upper bound: To test the robustness of our sampling approach, we ran the process with a rephrased prompt, different examples (a sample), and a larger sample of 15% .2. Lower bound: To test robustness with respect to the model and sample size, we attempted to create the taxonomy using qwen-30b-thinking with a smaller sample of 5% , to see how far the final result would deviate. The comparison was performed manually by examining the category descriptions and their resulting subcategories to ensure they were as similar as possible. The upper bound shows a high correlation with the original result, achieving 88% precision and 88% recall. We further observed that the differences in categories occur mainly in those that appear less frequently in the data (as the original taxonomy is ordered by frequency) and are also less informative than others (e.g., empty response). Thus, even the missing categories belong to the long tail and not the core. The lower bound demonstrates high precision and covers the majority of common categories ( 8 out of the top 10 ). Its recall with respect to the original is moderate, which suggests that the amount of data may be critical when the sample size is a bit small for representing the error distribution. In summary, our sampling choices appear reasonable, and different models can aggregate similar common errors without significant gaps. This strengthens our approach for identifying LLM error trends. 37 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

Original Results Results Config 1 Results Config 2 

Logical Reasoning Error Logical Reasoning Error Conceptual Misunderstanding, Causal Misinterpretation Missing Required Element Missing Required Content, Policy Violation, Tone/Style Mismatch Overlooking Constraints, Failure to Follow Instructions, Constraint Viola-tion Computation Error Numerical Computation Error, Syn-tax or Code Generation Error Calculation Error, Incorrect Formula Application Incorrect Identification Entity Misidentification Misidentification of Concept, Pattern Misidentification Specification Misinterpretation Prompt or Task Misinterpretation Misinterpretation of Problem, Data Misinterpretation, Causal Misinterpre-tation Output Formatting Error Output Formatting Violation, Lan-guage Accuracy Error Format Adherence Error, Boolean Syn-tax Error Irrelevant/Extraneous Content Redundant or Extraneous Output, Ir-relevant or Off-Topic Generation Counting/Enumeration Error Statistical or Probability Miscalcula-tion Answer Selection Error Answer Choice Selection Error Incorrect Category or Label Incomplete Reasoning Unjustified Assumption Unverified Assumption, Incomplete Re-sponse Factual Error Factual Inaccuracy Tool/API Usage Error Incorrect API/Parameter Usage, Edge-Case Oversight Naming/Symbol Error Data Retrieval or Lookup Error Inappropriate Refusal Inappropriate Refusal Unit Conversion Error Incorrect Data Handling Incorrect Data Handling False Positive Detection Error Detection Failure Domain Knowledge Misapplication Empty Response    

> Table 11. Comparative analysis of ErrorAtlas final error categories across different configurations as detailed in §6.

Metric Results Config 1 (%) Results Config 2 (%) 

Precision 0.88 1.00 Recall 0.88 0.52 F1-score 0.88 0.69  

> Table 12. Considering the original created taxonomy, we compared it to ones with other configurations (see Table 11).

# F. Limitations 

Predictions Signal While our method relies on model outputs, we acknowledge that the prediction signal can be partial compared to what actually happens inside the model (as opposed to white-box interpretability). 

Informative Prediction Dependence ErrorMap focuses on predictions as the primary basis for analysis. This approach technically depends on informative predictions. If a model cannot be run in generative mode and does not explain its response (CoT is also acceptable), then its responses cannot be analyzed. 

Error Category We acknowledge that error categories are inherently soft, that is, a single mistake may reflect multiple underlying issues. For example, a model incorrectly stating that “mRNA carries amino acids to the ribosome” could indicate either a factual error or confusion about molecular roles. 38 ErrorMap and ErrorAtlas : Charting the Failure Landscape of Large Language Models 

ErrorAtlas Generality While we have tried to create ErrorAtlas in the most varied way possible, there may be cases where it does not represent certain specific domains well. 

LLM-based technique This work makes use of LLMs to analyze mistakes made by LLMs themselves. While this approach is somewhat circular, verification and comparison are generally easier than generation (Simonds et al., 2025; Pang et al., 2023; Lin et al., 2024). However, this assumption may not always hold in practice. 

# G. Usage in AI 

In this work, we used AI models exclusively for language-related tasks, such as rephrasing and surface-level linguistic transformations. It was further used for minor improvements to code style across the repo. 39