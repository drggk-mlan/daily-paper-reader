# ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models
# ErrorMap 与 ErrorAtlas：绘制大语言模型的失败图谱

**Authors**: Shir Ashury-Tahan, Yifan Mai, Elron Bandel, Michal Shmueli-Scheuer, Leshem Choshen
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15812v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 9.0
**Evidence**: Evaluation framework for failure analysis in LLM benchmarks

---

## Abstract
Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

## 摘要
大语言模型（LLM）的基准

---

## 论文详细总结（自动生成）

这篇论文由 IBM Research、斯坦福大学和 MIT 的研究人员共同发表，旨在解决当前大语言模型（LLM）评估中“只知其然（是否失败），不知其所以然（为何失败）”的核心痛点。

以下是对该论文的结构化深入总结：

### 1. 论文的核心问题与整体含义（研究动机和背景）
*   **核心问题**：现有的 LLM 基准测试（Benchmarks）通常只提供一个最终分数（如准确率），但无法解释模型为何出错。例如，一个数学题做错，可能是因为计算失误、格式错误、误解题意，也可能是逻辑推理能力不足。
*   **研究动机**：如果不能解构失败的原因，开发者就无法针对性地改进模型，用户也难以根据具体需求选择最合适的模型。
*   **整体含义**：论文提出了 **ErrorMap**（一种自动生成错误分类体系的方法）和 **ErrorAtlas**（基于 83 个模型和 35 个数据集生成的通用错误图谱），旨在为 LLM 提供深度的“诊断性评估”。

### 2. 论文提出的方法论：ErrorMap
ErrorMap 是一个无监督的、基于 LLM 的两阶段分析框架：

*   **第一阶段：单实例错误分析（Per-Instance Analysis）**
    *   **核心思想**：利用一个“分析员 LLM”对每一个错误的预测进行深度剖析。
    *   **关键技术**：引入了**信息性正确预测（ICPs）**，即参考其他模型对同一问题的正确回答。分析员 LLM 会对比错误答案与正确答案，提取推理步骤、证据和假设，最终生成一个结构化的 JSON 对象，包含错误摘要和简短的错误标签（Label）。
*   **第二阶段：分类体系层级构建（Taxonomy Building）**
    *   **核心思想**：通过递归迭代将第一阶段生成的数千个错误标签组织成多层分类法。
    *   **流程**：
        1.  **错误分类（Categorization）**：将相似的错误标签聚类成更广泛的类别。
        2.  **错误分配（Assignment）**：将所有实例重新映射到这些新定义的类别中。
        3.  **递归细化**：对每个大类重复上述过程，直到达到预设的深度或无法进一步细分。

### 3. 实验设计
*   **数据集/场景**：涵盖了 35 个数据集，涉及通用能力（HELM）、医疗（MedHELM）、表格推理（ToRR）、函数调用（BFCL）和代码生成（HumanEval, MBPP）等领域。
*   **模型范围**：分析了 83 个主流模型（包括 GPT-4o, Gemini 系列, Claude 系列, Llama 系列等）的预测结果。
*   **对比与验证**：
    *   **ErrorAtlas 生成**：通过对约 7,000 个采样错误实例运行 ErrorMap，生成了包含 17 个顶级类别的通用错误分类法。
    *   **案例研究**：对比了 Gemini 1.5 Flash 与 Pro 的错误分布差异；在 MMLU-Pro 上对比了 ErrorMap 自动分类与人工分类的一致性。

### 4. 资源与算力
*   **使用的模型**：主要使用 **gpt-oss-120b** 作为分析员模型（Judge）。
*   **算力消耗**：
    *   **ErrorAtlas 构建**：约 7,200 次推理，耗时约 3 小时（支持并行）。
    *   **模型对比实验**：Gemini 对比约 2,000 次推理；MMLU-Pro 实验约 3,500 次推理。
*   **硬件细节**：文中未明确提及具体的 GPU 型号和数量，但强调了该方法由于只针对失败样本进行采样分析，因此具有较高的推理效率。

### 5. 实验数量与充分性
*   **实验规模**：分析了超过 7,000 个错误实例，覆盖 83 个模型和 35 个数据集，这在同类诊断性研究中属于规模极大的一次。
*   **充分性与客观性**：
    *   **覆盖率验证**：ErrorAtlas 对错误的覆盖率达到 **95.2%**。
    *   **准确性验证**：通过元判官（Meta-judge）验证，分类准确率达到 **92%**。
    *   **鲁棒性测试**：通过改变采样比例（5% 到 15%）和提示词（Prompt）变体，验证了分类体系的稳定性。
    *   **统计显著性**：对模型间的错误分布差异进行了二项式概率检验（P-value），确保发现具有统计学意义。

### 6. 论文的主要结论与发现
*   **揭示了被忽视的错误类型**：发现“**缺失必要元素**（Missing Required Element）”和“**规范误解**（Specification Misinterpretation）”是极其普遍但目前研究较少的错误。
*   **基准测试的局限性**：在 MMLU-Pro 等推理基准中，约 **44%** 的错误实际上是技术性挑战（如计算错误、格式错误），而非核心推理能力不足。
*   **模型家族的“失败签名”**：不同厂商的模型