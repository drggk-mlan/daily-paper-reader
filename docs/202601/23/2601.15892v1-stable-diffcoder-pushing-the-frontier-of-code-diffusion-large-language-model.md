# Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model
# Stable-DiffCoder：推动代码扩散大语言模型的前沿

**Authors**: Chenghao Fan, Wen Heng, Bo Li, Sichen Liu, Yuxuan Song, Jing Su, Xiaoye Qu, Kai Shen, Wei Wei
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15892v1
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 6.0
**Evidence**: diffusion based code large language model architecture and benchmark performance

---

## Abstract
Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.

## 摘要
与自回归（AR）模型相比，基于扩散

---

## 速览摘要（自动生成）

**问题**：现有代码扩散模型（DLLM）在性能上仍落后于同规模的自回归（AR）模型。

**方法**：提出 Stable-DiffCoder，通过块扩散持续预训练（CPT），并引入定制预热策略与块剪切噪声调度来优化训练。