# Improving Methodologies for LLM Evaluations Across Global Languages
# 改进全球语言的大语言模型评估方法

**Authors**: Akriti Vij, Benjamin Chua, Darshini Ramiah, En Qi Ng, Mahran Morsidi, Naga Nikshith Gangarapu, Sharmini Johnson, Vanessa Wilfred, Vikneswaran Kumaran, Wan Sie Lee, Wenzhuo Yang, Yongsen Zheng, Bill Black, Boming Xia, Frank Sun, Hao Zhang, Qinghua Lu, Suyu Ma, Yue Liu, Chi-kiu Lo, Fatemeh Azadi, Isar Nejadgholi, Sowmya Vajjala, Agnes Delaborde, Nicolas Rolin, Tom Seimandi, Akiko Murakami, Haruto Ishi, Satoshi Sekine, Takayuki Semitsu, Tasuku Sasaki, Angela Kinuthia, Jean Wangari, Michael Michie, Stephanie Kasaon, Hankyul Baek, Jaewon Noh, Kihyuk Nam, Sang Seo, Sungpil Shin, Taewhi Lee, Yongsu Kim, Daisy Newbold-Harrop, Jessica Wang, Mahmoud Ghanem, Vy Hong
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15706v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 8.0
**Evidence**: Describes multilingual evaluation benchmarks for frontier AI models

---

## Abstract
As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.   The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

## 摘要
随着前沿人工智能模型在全球范围内的部署，确保其在不同语言和文化

---

## 论文详细总结（自动生成）

这篇论文是由新加坡、日本、英国、美国、加拿大、法国、肯尼亚、韩国等多个国家的AI安全研究所（AISI）及相关政府机构联合开展的一项关于大语言模型（LLM）多语言安全评估的研究报告。

以下是对该论文的结构化深入总结：

### 1. 论文的核心问题与整体含义（研究动机和背景）
*   **核心问题**：随着前沿AI模型在全球范围内的部署，模型在非英语环境下的安全表现是否与英语一致？现有的评估方法（如LLM-as-a-judge）在处理不同语言和文化语境时的有效性如何？
*   **研究动机**：目前大多数AI安全评估集中在英语，而对于非英语（尤其是低资源语言）的安全防护研究不足。论文旨在建立一套通用的多语言安全评估方法论，并识别模型在不同语言中的安全漏洞。

### 2. 论文提出的方法论
核心思想是结合**自动化评估（LLM-as-a-judge）**与**人工标注（Human Annotation）**，对模型生成的响应进行双重验证。
*   **翻译与适配**：将现有的英语安全基准翻译成9种语言（粤语、波斯语、法语、日语、韩语、斯瓦希里语、马来语、中文、泰卢固语），并由母语者进行校验。
*   **评估流程**：
    1.  **提示词输入**：使用翻译后的提示词对测试模型进行推理。
    2.  **自动化评分**：使用一个独立的第三方LLM作为裁判，根据预设的“可接受性”和“拒绝率”标准进行评分。
    3.  **人工校验**：由各国AISI的专家对自动化评分结果进行人工审核，记录差异并分析原因。
*   **评估维度**：
    *   **可接受性（Acceptability）**：响应是否安全且相关。
    *   **拒绝质量（Quality of Refusal）**：模型拒绝执行有害指令时是否提供了合理的解释或合法的替代方案。
    *   **裁判有效性（Evaluator Effectiveness）**：LLM裁判与人工标注的一致程度。

### 3. 实验设计
*   **测试模型**：选择了两个开源权重模型：**Mistral Large** 和 **Gemma 2 (27B)**。
*   **数据集/基准（Benchmarks）**：
    *   **MLCommons AILuminate (v1.0)**：涵盖隐私、知识产权、暴力犯罪、非暴力犯罪。
    *   **AnswerCarefully**：由日本NII开发，专注于个人信息和敏感信息。
    *   **CyberSecEval (Purple Llama)**：专注于提示词注入（Jailbreaking）和网络安全漏洞。
*   **覆盖语言**：共10种语言，涵盖高资源（如英语、法语、中文、日语、韩语）和低资源（如斯瓦希里语、泰卢固语、波斯语）语言。
*   **实验规模**：总计使用了超过 **6,000条** 新翻译的提示词。

### 4. 资源与算力
*   **算力说明**：论文**未明确说明**具体的GPU型号、数量或推理时长。
*   **工具平台**：实验使用了新加坡AISI开发的开源工具包 **Moonshot** 进行测试执行和结果提取。由于是推理评估而非模型训练，其算力需求主要集中在模型推理和裁判模型的调用上。

### 5. 实验数量与充分性
*   **实验规模**：针对10种语言、2个模型、5大类伤害类别进行了全面测试。
*   **充分性**：
    *   **多维度验证**：不仅做了自动化测试，还引入了大规模的人工标注（Discrepancy Rate分析），这在多语言研究中较为罕见。
    *   **一致性检查**：在英语和日语上进行了多次运行（Multiple Runs）以验证结果的稳定性。
*   **客观性**：通过跨国合作，由不同国家的母语专家独立标注各自语言的结果，减少了单一机构的偏见。

### 6. 论文的主要结论与发现
*   **安全防护差距**：非英语语言的安全防护普遍落后于英语。尽管所有语言都保持了基准安全水平，但非英语语言在越狱（Jailbreak）防护上表现最弱。
*   **伤害类别差异**：知识产权保护最强，而越狱防护最差。
*   **LLM裁判的局限性**：LLM裁判在处理复杂或混合信号（如先给警告后给有害信息）时表现不佳。在日语、泰卢固语和中文中，LLM与人工的评估差异率最高（最高达18.9%）。
*   **语言特有行为**：
    *   **混合语言输出**：除英法外，其他语言常出现中途切换到英语或语意不明的现象。
    *   **拒绝风格**：法语、韩语、日语和波斯语倾向于委婉拒绝，以符合文化礼仪。
    *   **低资源语言幻觉**：波斯语、泰卢固语和斯瓦希里语中更容易出现胡言乱语或幻觉。

### 7. 优点（亮点）
*   **全球协作**：这是首次由多国AI安全研究所联合进行的大规模多语言