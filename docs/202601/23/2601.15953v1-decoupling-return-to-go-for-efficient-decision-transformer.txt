Title: Decoupling Return-to-Go for Efficient Decision Transformer

URL Source: https://arxiv.org/pdf/2601.15953v1

Published Time: Fri, 23 Jan 2026 01:47:20 GMT

Number of Pages: 9

Markdown Content:
# Decoupling Return-to-Go for Efficient Decision Transformer 

# Yongyi Wang 1 , Hanyu Liu 1 , Lingfeng Li 1 , Bozhou Chen 1 , Ang Li 1 , Qirui Zheng 1 , Xionghui Yang 1 , Wenxin Li 1,∗

> 1

# School of Computer Science, Peking University, Beijing, China. 

> ∗

# lwx@pku.edu.cn 

# Abstract 

The Decision Transformer (DT) has established a powerful sequence modeling approach to offline re-inforcement learning. It conditions its action pre-dictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this de-sign: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT’s perfor-mance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simpli-fies the architecture by processing only observation and action sequences through the Transformer, us-ing the latest RTG to guide the action prediction. This streamlined approach not only improves per-formance but also reduces computational cost. Our experiments show that DDT significantly outper-forms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks. 

# 1 Introduction 

Offline Reinforcement Learning (RL) aims to learn effec-tive policies from static, pre-collected datasets, eliminating the need for costly or risky online interaction during train-ing. This paradigm is especially valuable for real-world ap-plications that face the dual challenges of scarce data and high safety demands, including robotic manipulation and au-tonomous driving. However, offline RL faces fundamen-tal challenges, such as overcoming the distributional shift between the dataset and the learned policy, and effectively extracting high-quality behavioral patterns from potentially suboptimal data. Decision Transformer (DT) [Chen et al. , 2021] has emerged as a novel and promising paradigm that departs from traditional approaches based on offline Dynamic Program-ming (DP). The DT leverages the Transformer [Vaswani et al. , 2017] architecture to model trajectories as sequences of (Return-to-Go (RTG), observation, action) tokens, predicting each action autoregressively based on past observations, ac-tions, and RTGs. This sequence modeling paradigm circum-vents the Markov assumption and the deadly triad problem [Peng et al. , 2024], delivering robust performance across of-fline RL benchmarks. Central to DT is the use of RTG, which specifies the cumu-lative reward desired from a given timestep onward. During training, the RTG sequence serves as a crucial feature to dis-tinguish trajectory quality. During inference, it acts as an ad-justable, user-specified target to guide the action generation. The standard DT design follows an intuitive autoregressive scheme: it feeds the entire sequence of RTGs, observations, and actions into the Transformer to predict the next action. In this work, we challenge this established design. Through a combination of theoretical analysis and empiri-cal evidence, we identify a critical redundancy in the standard DT implementation: conditioning the Transformer on the en-tire historical sequence of RTGs is theoretically unnecessary for optimal action prediction under the trajectory modeling objective, as only the most recent RTG provides essential fu-ture information. More importantly, our experiments reveal that this redundancy is not benign; it can introduce unnec-essary complexity, thereby degrading the model’s computa-tional efficiency and final performance, a particularly unde-sirable trait for applications such as robotic manipulation. To address this limitation, we propose a simple but highly effective architecture: Decoupled DT (DDT) 1. As demon-strated in Fig. 1, the core insight is to decouple the condition-ing of RTG from the Transformer’s input sequence. Specifi-cally, DDT takes only the sequences of observations and ac-tions as input to the Transformer. The conditioning is simpli-fied: only the last RTG is used to directly modulate the out-put token corresponding to the predicted action via Adaptive Layer Normalization (adaLN) [Peebles and Xie, 2023]. This elegant design drastically reduces input redundancy, allow-ing the Transformer to focus on learning essential dynamics and policy patterns from observation-action histories, while remaining goal-directed via a focused RTG signal. Extensive experiments on widely-adopted offline RL D4RL benchmarks [Fu et al. , 2020] demonstrate that DDT not only consistently outperforms the original DT and its vari-ants, but also achieves competitive results compared to ad-      

> 1Code in the supplementary, will be online upon acceptance
> arXiv:2601.15953v1 [cs.AI] 22 Jan 2026 st-1 at-1 stat
> Rt-1 Rt
> at-1 at
> emb. + pos.
> encoder
> linear decoder
> adaLN adaLN
> causal transformer Figure 1: DDT architecture. States/observations and actions are first projected via modality-specific linear embeddings, followed by a po-sitional encoding. A GPT backbone with causal self-attention pro-cesses the resulting tokens. RTGs are integrated via adaLN to mod-ulate the output token for action prediction.

vanced model-free and model-based offline algorithms RL. Furthermore, by eliminating the need to process lengthy RTG sequences, DDT enjoys reduced computational overhead and a smaller memory footprint, making it a more efficient and scalable solution, a property that is advantageous for deploy-ment in computationally cost-aware real-world scenarios. Our principal contributions are: • Exposing the fundamental redundancy of conditioning on the full RTG history in DT; • Introducing the DDT, a simplified architecture that conditions only on the current RTG with adaLN; • Empirically showing that DDT is competitive with state-of-the-art DT variants on D4RL datasets. The remainder of this paper is organized as follows: Sec-tion 2 reviews related work. Section 3 provides necessary background. Section 4 details our theoretical motivation. Section 5 demonstrates our proposed DDT architecture. Sec-tion 6 presents experimental results and analysis. Section 7 explores possible extensions of the DDT framework. Finally, Section 8 concludes the paper. 

# 2 Related Work 

2.1 Offline RL 

Offline RL learns policies from static datasets, offering a safe and cost-effective paradigm for applications like robotics and autonomous driving [Levine et al. , 2020]. Predominant methods in offline RL are built upon the framework of DP and value function learning. However, learning from a static dataset introduces the core challenge of distributional shift. To address this, several key algorithmic families have been developed, including policy constraint methods (e.g., BCQ [Fujimoto et al. , 2019]), value regularization methods (e.g., CQL [Kumar et al. , 2020]), and expectile-regression-based approaches (e.g., IQL [Kostrikov et al. , 2021]). Despite their success, these methods, due to their reliance on bootstrap-ping within the DP framework, remain susceptible to error accumulation and sensitivity to hyperparameters. 

2.2 Offline RL via Sequence Modeling 

Departing from offline RL algorithms based on DP, the DT [Chen et al. , 2021] frames RL as sequence modeling. By leveraging a Transformer to autoregressively predict actions conditioned on past observations, actions, and RTGs, DT avoids bootstrapping and the Markov assumption, establish-ing a robust, trajectory modeling paradigm. Generalized DT [Furuta et al. , 2022] later provided a unifying theoretical framework of Hindsight Information Matching (HIM). Sub-sequent research has targeted specific limitations of DT. Sub-optimal Trajectory Stitching : Enhanced by generating way-points (WT [Badrinath et al. , 2023]), adaptive context lengths (EDT [Wu et al. , 2023]), or Q-learning relabeling (QDT [Ya-magata et al. , 2023]). Stochasticity & Over-optimism : Ad-dressed by clustering returns (ESPER [Paster et al. , 2022]) or disentangling latent variables (DoC [Yang et al. , 2023]), ad-vantage conditioning (ACT [Gao et al. , 2024]). Online Train-ing & Adaptation : Explored via online fine-tuning (ODT [Zheng et al. , 2022]), auxiliary critics (CGDT [Wang et al. ,2024]), RL gradient infusion (TD3+ODT [Yan et al. , 2024]), value function guidance (VDT [Zheng et al. , 2025]). Mod-els other than Transformer have also been explored, includ-ing convolutional models (DC [Kim et al. , 2023]), Mamba [Gu and Dao, 2024] (Decision Mamba [Lv et al. , 2024; Huang et al. , 2024]), and hybrid designs (LSDT [Wang et al. , 2025]). 

2.3 Conditioning via Adaptive Normalization 

Layer Normalization (LN) [Ba et al. , 2016] was introduced to address the issue of internal covariate shift in recurrent neu-ral networks, thereby stabilizing the training process. This layer-wise normalization proved particularly effective for se-quential and Transformer-based architectures, where batch statistics are often unstable. Building upon LN and inspired by conditional normalization techniques from earlier gener-ative models [De Vries et al. , 2017; Karras et al. , 2019; Dhariwal and Nichol, 2021], the Diffusion Transformer (DiT) [Peebles and Xie, 2023] formally established and popularized the modern adaLN module. In this design, a shared Multi-Layer Perceptron (MLP) maps conditioning vectors, such as timestep or class embeddings, into layer-specific modulation parameters (scale γ and bias β). This mechanism dynam-ically fuses conditional information into the network, effec-tively replacing the fixed, learnable affine transformations tra-ditionally used in each LN layer of a Transformer block with adaptive, condition-dependent scaling and shifting. 

# 3 Preliminaries 

3.1 MDP and POMDP 

In RL, the environment is commonly formulated as a Markov Decision Process (MDP), denoted by M = ⟨S, A, T, r, ρ ⟩,where S is the state space, A is the action space, T : S ×A →

∆S is the transition probability, r : S × A → R is the reward function, ρ ∈ ∆S is the initial state distribution. A Partially Observable MDP (POMDP) generalizes the MDP framework to partially observable environments, de-noted as N = ⟨S, A, T, r, ρ, Ω, O ⟩, where ⟨S, A, T, r, ρ ⟩ is an MDP, Ω is the set of observations, and O : S × A → ∆Ω 

is the observation probability function. The objective in MDP and POMDP is to learn a policy that maximizes the cumulative return R1 = PTt=1 rt. For an MDP, a policy specifies a distribution over actions given a state. In a POMDP, however, the state is unobservable, necessitating the extraction of a belief state [ ˚ Astr¨ om, 1965; Smallwood and Sondik, 1973; Subramanian et al. , 2022] b ∈

∆S from the trajectory to inform decisions, where 

bt+1 (st+1 ) = O(ot | st+1 , a t) P 

> st∈S

T (st+1 | st, a t)bt(st)

P (ot | at, b t)

(1) It is noteworthy that reward information is not utilized in computing the belief state in Eq. 1, as per the standard as-sumption in a POMDP. The belief state serves as a sufficient statistic [Kaelbling et al. , 1998], implying that optimal deci-sions can be made without other information within the tra-jectory when it is given. 

3.2 Decision Transformer 

The DT is trained with a Mean Squared Error (MSE) Behavior Cloning (BC) loss to output an action given a trajec-tory as context. At step t, the model takes as input a trajectory 

τt = ( ˆRt−k+1 , o t−k+1 , a t−k+1 , ..., ˆRt−1, o t−1, a t−1, ˆRt, o t)

where k denotes the specified context length and ˆRt =

PTt′=t rt′ is the RTG at step t. Thus, the DT policy takes the form 

π(at | τt) = π(at | ˆRt−k+1: t, o t−k+1: t, a t−k+1: t−1) (2) where the action at is conditioned on the sequence of RTGs, observations, and actions within the context window of length 

k. Such a sequence modeling paradigm of DT makes it well-suited for solving POMDPs as well as MDPs. 

3.3 Adaptive Layer Normalization 

AdaLN [Peebles and Xie, 2023] injects conditional informa-tion by dynamically adjusting the LN affine parameters, re-placing static γ, β with learned functions γ(z), β (z) of the condition vector z. The calculation process of adaLN is as follows: 

γ(z), β (z) = MLP (z)

adaLN (x, z ) = γ(z) ⊙ x − μ(x)

pσ2(x) + ϵ + β(z) (3) where μ is the mean value of each dimension of x, σ2 is the variance, and ⊙ denote the Hadamard product. The number of layers of the MLP in adaLN scales with the complexity of the condition z, though in practice it is kept small; a single layer suffices for many cases. 

# 4 Redundancy of RTG Sequence as Condition 

This section presents the theoretical rationale for the re-dundancy of conditioning on the entire RTG sequence in the original architecture of DT. In brief, in DT pol-icy Eq. 2, the set of conditioning random variables 

(Rt−k+1: t, Ot−k+1: t, At−k+1: t−1) can be grouped into two parts: the trajectory history Ht := ( Ot−k+1: t, At−k+1: t−1)

which refers to the events that have already occurred and been observed during the reasoning process, including executed r1

r2

r3

Rt

# ... 

r2

r3

Rt

# ...  r3

Rt

# ...  ...  rt-1 

Rt Rt

R1 R2 R3 Rt-1  Rt

Figure 2: In the RTG sequence, the difference between successive terms is the reward. These rewards do not contribute to the belief state. Therefore, they are redundant information in history. 

actions and received observations and the future condition 

Rt−k+1: t. which primarily aims to guide the current action through anticipated future returns. Let rt denote the random variable representing the reward obtained at time t. Then the random event {Rt−k+1: t =

Rt−k+1: t} can be expressed as follows: 

{Rt−k+1: t = Rt−k+1: t} =

> t

\

> i=t−k+1

{Ri = Ri}

= {Rt = Rt} ∩ 

> t−1

\

> i=t−k+1

{Ri+1 − Ri = Ri+1 − Ri}

= {Rt = Rt} ∩ { rt−k+1: t−1 = rt−k+1: t−1}

(4) As shown in Eq. 4 and illustrated in Fig. 2, the future condition of DT comprises the intersection of two terms: 

{Rt = Rt}, representing the expected future returns after step t, and {rt−k+1: t−1 = rt−k+1: t−1}, which corresponds to already observed past rewards and therefore contains no information about the future. In POMDP, since the belief state has already extracted all decision-relevant information from the history and, according to Eq. 1, is independent of the rewards in the history, {rt−k+1: t−1 = rt−k+1: t−1} can-not provide any additional information for decision-making. Therefore, Rt−k+1: t−1 is redundant information. Based on the above theoretical results, we conclude that under the standard POMDP framework [Kaelbling et al. ,1998], the policy formulation of DT contains redundant RTG conditioning and should be simplified to: 

π(at | τt) = π(at | ˆRt, o t−k+1: t, a t−k+1: t−1) (5) which takes only the latest RTG ˆRt as future condition. 

# 5 DDT: A Simplified Architecture 

Based on the conclusions in Section 4, significant redundancy exists when conditioning the policy of the original DT on RTG sequence. Therefore, the required new architecture must correspond to a policy of the form given in Eq. 5, one that can leverage the powerful sequence modeling capabilities of the Transformer while effectively harnessing the guidance of the latest RTG. This section details the implementation of our proposed DDT framework. Alg. 1 and Fig. 3 outline the action prediction process of DDT. Its key distinctions from original DT lie in: st-1  at-1  st       

> Rt
> at
> emb. + pos.
> encoder
> linear decoder
> adaLN
> causal transformer Figure 3: During action prediction, DDT takes only the most recent RTG ˆRtas condition. This condition applies to the hidden state corresponding to otvia adaLN, and the result is subsequently fed into Pred A(an MLP) to output the action at.

Algorithm 1 Decoupled Decision Transformer Prediction 

Input : Obs: ot−k+1: t, Act: at−k+1: t−1, RTG: ˆRt.

Parameter : Transformer, adaLN, Embed O , Embed A, Pred A.

Output : Predicted action at.

▷ MLP & Embeddings per-timestep.  

> 1:

o′ ← Embed O (ot−k+1: t, t ) 

> 2:

a′ ← Embed A(at−k+1: t−1, t )

▷ Interleave tokens as: (o1, a 1, o 2, a 2, . . . ). 

> 3:

Trajectory ← stack (o′, a ′) 

> 4:

HiddenStates ← Transformer (Trajectory )

▷ Get the hidden state of the last action at. 

> 5:

ActionHidden ← unstack (HiddenStates ).actions [−1] 

▷ Incorporate RTG conditioning ˆRt. 

> 6:

ConditionedHidden ← adaLN (ActionHidden , ˆRt) 

> 7:

return Pred A(ConditionedHidden )

• Line 3, where the input sequence to the Transformer is constructed solely from observations and actions, ex-cluding the RTG sequence; • Lines 5–6, where the RTG condition ˆRt modulate only the hidden state corresponding to the last action at.Thus, the design of DDT circumvents the need of DT to rely on the entire RTG sequence, ˆRt−k+1: t, as input to the Transformer during action prediction. With adaLN imple-mented as a single linear layer adding negligible overhead, DDT cuts computation by reducing the input sequence length from 3k to 2k, leveraging the quadratic scaling of Trans-former inference cost. As shown in Fig 1, during training, the forward pass of DDT differs from its action prediction pipeline (Fig. 3, Alg. 1): at line 5, hidden states are extracted for the entire ac-tion sequence at−k+1: t rather than only for the last action 

at; and at line 6, the complete RTG sequence ˆRt−k+1: t is used to modulate the corresponding hidden states via adaLN. Our DDT design preserves the same forward pass and train-ing loop as the original DT. The loss function also remains identical as DT’s, i.e., the MSE loss for BC. 

# 6 Experiments 

This section evaluates the performance of our proposed sim-plified DT framework, DDT, on the widely adopted D4RL [Fu et al. , 2020] benchmark for continuous action space con-trol problems. The results show that DDT not only reduces the input sequence length and consequently the inference cost compared to the original DT, but also substantially sur-passes its performance. Remarkably, DDT remains competi-tive with recently introduced state-of-the-art DT variants that enhance performance by integrating value-based offline RL techniques or using more complex architectures. In addition to the main results presented in Tab. 1, we in-vestigate the following questions: 

What explains DDT’s superior performance over DT? 

To address this, we visualize the attention scores of DT and DDT in Section 6.1, which reveals that DDT’s attention dis-tribution aligns more closely with the Markovian nature of the problem. We therefore infer that by removing the redun-dant RTG sequence, DDT enables a more efficient allocation of attention scores. 

Is the adaLN module necessary for achieving the policy form in Eq. 5, or can simply modifying the attention mask to block irrelevant RTGs suffice? 

We examine this in Section 6.2 by comparing DDT to a ”blocked-DT” variant, which only modifies the attention mask. While blocked-DT shows a minor improvement over the original DT, its performance remains significantly lower than that of DDT. This demonstrates that using adaLN to in-tegrate the return condition is a more effective approach. 

Can DDT be applied to tasks with discrete action spaces, high stochasticity, and sparse rewards? 

We validate this in Section 6.3 through experiments in the 2048 game. The results confirm that DDT generalizes effec-tively to such tasks without any performance loss compared to the original DT. 

Does adding more linear layers to adaLN help? 

In Section 6.4, we examine this question. The results demonstrate that for the experimental tasks, a single-layer, activation-free adaLN is sufficient for conditioning. In fact, adding more layers and introducing nonlinearity degrades performance. 

Datasets 

We evaluate DDT in Hopper, Walker2d, and HalfCheetah (Fig. 4a, from [Tassa et al. , 2018]) with dense rewards. The datasets for each environment consist of three types: medium (generated by a medium-level policy), medium-replay (sam-pled from the replay buffer of an agent trained to a medium performance level), and medium-expert ( 1 : 1 mixture of medium and expert trajectories). All datasets are obtained from the official D4RL benchmark implementation, and all reported scores are normalized using the standard evaluation interface provided by the D4RL library. To validate the applicability of DDT on discrete and highly stochastic problems, we employed the game 2048, a sliding puzzle game (Fig. 4b, from [Cirulli, 2014]) where tiles with identical numbers can be combined to form a tile of greater value. In this environment, the agent receives a reward of 1

only when a tile of value 128 is created, and no reward other-wise. The offline dataset for this task consists of 5×10 6 steps collected using a mixture of a random agent and an expert (a) D4RL-HalfCheetah (b) 2048 Figure 4: Environments for evaluation. 

policy trained with PPO, as adopted from the experiments in ESPER [Paster et al. , 2022]. 

Baselines 

The baseline algorithms considered in our study fall into two categories: (1) classic offline RL and supervised learning ap-proaches, including BRAC-v [Wu et al. , 2019], TD3+BC [Fujimoto and Gu, 2021], IQL [Kostrikov et al. , 2021], and CQL [Kumar et al. , 2020]; and (2) DT [Chen et al. , 2021] and its recent variants, namely VDT [Zheng et al. , 2025] and LSDT [Wang et al. , 2025]. The performance scores presented in Tab. 1 are sourced from the best results reported in the original publications or from our own implementations using d3rlpy with consistent hyperparameters, ensuring a fair com-parison. 

Our DDT Implementation 

Our implementation builds upon d3rlpy [Seno and Imai, 2022], which provides a well-structured interface for imple-menting and extending various offline RL algorithms, in-cluding the original DT. Both DDT and DT employ the same GPT architecture [Radford et al. , 2018] as their net-work backbone. Their key distinction lies in the action prediction stage. DT takes a sequence that includes RTG, 

( ˆRt−k+1 , o t−k+1 , a t−k+1 , . . . , ˆRt, o t), as input to the GPT and uses the final token of the output sequence for action pre-diction. In contrast, DDT inputs a sequence without RTG, 

(ot−k+1 , a t−k+1 , . . . , o t), into the same GPT. It then takes the final output token and conditions it on the return-to-go ˆRt

via an adaLN module before predicting the action. We adopt a popular adaLN-Zero variant to implement our DDT, which initializes the weights of the final linear layer to zero (so that initially, γ(z) = β(z) = 0 ). Specifically, in Eq. 3, γ(z) is replaced with γ(z) + 1 . This design ensures that the network behaves similarly to a standard LN layer at the beginning of training, thereby stabilizing the training process. The comparative experimental results with other offline RL methods are presented in Tab. 1. 

6.1 Why Does DDT Perform Better? 

The core of the Transformer architecture lies in its atten-tion module. Therefore, when analyzing methods related to DT, visualizing the attention scores can provide a summary of how the model internally models temporal dependencies.     

> (a) hopper-medium, DT (b) hopper-medium, DDT
> (c) halfcheetah-medium-expert, DT
> (d) halfcheetah-medium-expert, DDT Figure 5: Average attention scores over the first 10 3inference steps for the learned DT and DDT policies during action prediction in the respective environment (hopper and halfcheetah).

Given that DDT achieves an improvement of approximately 

10% over DT in the tasks listed in Tab. 1, we hypothesize that this improvement may be related to DDT’s ability to allocate attention scores more efficiently and in a manner that better aligns with the inherent characteristics of these environments. The visualizations in this section present the evidence we pro-vide in support of this hypothesis. The attention matrix in Fig. 5 demonstrates the cap-tured associations during action prediction. These are lower triangular matrices because DT can only rely on histori-cal information to predict actions, with all future informa-tion masked out to ensure this causality. The horizontal and vertical axes in Fig. 5a, 5c (DT) correspond with 

( ˆR1, o 1, a 1, . . . , ˆR30 , o 30 , a 30 ), and those of Fig. 5b, 5d (DDT) correspond with (o1, a 1, . . . , o 30 , a 30 ).Since the hopper and halfcheetah environments are Marko-vian, theoretically, only the most recent state is required to predict actions. Therefore, ideally, the attention scores al-located in these environments should concentrate near the main diagonal, implying that the attention layer ought to fo-cus more on information from adjacent timesteps. However, as can be observed from Fig. 5a and 5c, the attention scores learned by DT do not exhibit such an ideal distribution; in-stead, they appear relatively scattered, which is also consis-tent with findings in existing literature [Wang et al. , 2025]. As shown in Fig. 5b and 5d, the attention score distribu-tion in DDT aligns more closely with the ideal pattern. By removing the unnecessary RTG sequences, we reduce the in-Dataset & Environment BRAC-v TD3+BC IQL CQL DT VDT LSDT DDT (Ours) 

hopper-medium-replay-v2 0.6 60 .9 94 .7 48 .6 83 .1 ± 0.7 96 .0 93 .9 92 .5 ± 0.6

walker-medium-replay-v2 0.9 81 .8 73 .9 26 .7 66 .9 ± 1.4 82 .3 74 .7 77 .6 ± 1.1

halfcheetah-medium-replay-v2 47 .7 44 .6 44 .2 46 .2 36 .2 ± 1.2 39 .4 42 .9 37 .8 ± 0.5

hopper-medium-v2 31 .1 59 .3 66 .3 58 .0 68 .3 ± 0.9 98 .3 87 .2 99 .4 ± 0.6

walker-medium-v2 81 .1 83 .7 78 .3 79 .2 74 .3 ± 0.3 81 .6 81 .0 78 .6 ± 0.3

halfcheetah-medium-v2 46 .3 48 .3 47 .4 44 .4 42 .5 ± 0.5 43 .9 43 .6 43 .0 ± 0.4

hopper-medium-expert-v2 0.8 98 .0 91 .5 111 .0 106 .8 ± 0.4 111 .5 111 .7 111 .1 ± 0.2

walker-medium-expert-v2 81 .6 110 .1 109 .6 98 .7 108 .6 ± 0.3 110 .4 109 .8 109 .5 ± 0.5

halfcheetah-medium-expert-v2 41 .9 90 .7 86 .7 62 .4 88 .1 ± 0.4 93 .9 93 .2 94 .2 ± 0.2       

> Table 1: Performance of DDT and SOTA baselines on D4RL tasks. For DDT, results are reported as the mean and standard error of normalized rewards over 4×100 rollouts ( 4independently trained models using different seeds with 100 trajectories), generally showing low variance.

put length to the Transformer and mitigate the interference of such redundant information on model learning, thereby al-lowing attention scores to be allocated more efficiently and effectively capturing short-term dependencies. This achieves an effectiveness analogous to that of LSDT [Wang et al. ,2025], which incorporates convolution to enhance local de-pendency modeling. 

6.2 Does Masking Out Redundant RTGs Suffice? 

Our DDT proposes a solution to the redundant RTGs, as iden-tified in Section 4. However, in pursuit of minimizing al-terations to the original DT architecture, this is not the most concise variant to address the issue. A more streamlined form can satisfy the formulation in Eq. 5 without introducing the adaLN module, merely requiring an adjustment to the causal mask to block all RTG tokens except ˆRt. Here, we contrast the performance of our method with that of blocked-DT, a variant that merely masks non-last RTG tokens. As shown in Tab. 2, our findings indicate that while blocked-DT might achieve a modest gain over DT, it is still inferior to DDT. This performance gap demonstrates that for the last RTG ˆRt to effectively guide the action generation, masking out redundant RTGs ˆRt−k+1: t−1 is insufficient com-pared to using an effective conditioning module like adaLN. 

Data & Env DT blocked-DT DDT (Ours) 

hop-mr 83 .1 82 .7( −0.4) 92 .5(+9 .4) 

wlk-mr 66 .9 67 .2(+0 .3) 77 .6(+10 .7) 

hal-mr 36 .2 36 .6(+0 .4) 37 .8(+1 .6) 

hop-m 68 .3 69 .1(+0 .8) 99 .4(+31 .1) 

wlk-m 74 .3 74 .1( −0.2) 78 .6(+4 .3) 

hal-m 42 .5 43 .2(+0 .7) 43 .0(+0 .5) 

hop-me 106 .8 107 .7(+0 .9) 111 .1(+4 .3) 

wlk-me 108 .6 107 .9( −0.7) 109 .5(+0 .9) 

hal-me 88 .1 89 .6(+1 .5) 94 .2(+6 .1) 

> Table 2: Performance comparison of DT, blocked-DT, and DDT. Numbers in parentheses indicate the performance difference relative to DT.

6.3 Does DDT Generalize to Discrete Problems? 

For discrete action spaces, we adopt the DT implementation from d3rlpy [Seno and Imai, 2022] and build our DDT variant on top of it. This discrete DT differs from the standard DT in three main aspects: (1) it uses a cross-entropy loss, (2) its out-put is a categorical distribution, and (3) actions are generated by selecting the one with the highest probability. 

Data & Env DT blocked-DT DDT (Ours) 

2048 0.93 ± 0.03 0.93 ± 0.02 0.93 ± 0.02 

> Table 3: Performance of DT, blocked-DT, and DDT on 2048.

Tab. 3 shows that DDT, DT and blocked-DT achieve simi-lar performance on 2048. This shows that DDT generalizes to discrete-action, high-stochasticity, sparse-reward tasks with-out performance degradation, thereby establishing its applica-bility beyond the continuous, relatively deterministic, dense-reward control problem settings in D4RL. 

6.4 Does Adding More Layers to adaLN Help? 

Since the conditioning variable ˆRt is a simple scalar, our im-plementation of DDT employs a single linear layer (input di-mension 1, output dimension 128 × 2, corresponding to γ

and β in Eq. 3) without an activation function. To investigate whether a more complex adaLN could enhance performance, we evaluate a variant that uses a 2-layer MLP with a ReLU activation in between. However, as Tab. 4 shows on several datasets, this added complexity leads to performance degra-dation rather than improvement. 

Data & Env DT DDT DDT(2) 

hop-med 68 .3 99 .4 90 .3

hal-med 42 .5 43 .0 42 .9  

> Table 4: Performance comparison. DDT(2) denotes the variant with an additional linear layer ( 128 →128 ) and a ReLU in the adaLN.

# 7 Further Discussion 

7.1 DDT with Generalized Condition 

Like DT, our DDT falls under the Reward Conditioned Su-pervised Learning (RCSL) paradigm [Kumar et al. , 2019; Brandfonbrener et al. , 2022] within the HIM [Furuta et al. , 2022] framework. The HIM offers a general theoreti-cal framework for modeling decision-making processes that leverage both historical information and predicted future out-comes. Beyond the RCSL paradigm, the HIM also encom-passes the Goal Conditioned Supervised Learning (GCSL) paradigm. In GCSL, the policy is conditioned on goals rather than RTGs. DT generalizes effectively to GCSL tasks by taking the en-tire goal-conditioned sequence as input; the following discus-sion explores how DDT can achieve a similar effect under the GCSL paradigm. We first consider a class of scenarios where the goal re-mains constant throughout an episode. In this case, condi-tioning on a single goal is obviously more reasonable than conditioning on a constant sequence. This is analogous to the case of sparse rewards in RCSL: since the RTG ˆRt stays unchanged until the episode ends, this constant RTG can be treated as the goal. Our experiments on 2048 also demon-strate that DDT can adapt effectively to this setting. We next consider a class of scenarios where the goal may change within a single episode during the decision-making process. The key question is whether an architecture like DDT, which conditions only on the final goal variable, would fail in such a setting. In GCSL, the agent typically employs a goal-setting mech-anism. Similar to the process in RCSL, where a high ini-tial RTG ˆR1 is specified and then updated by subtracting re-ceived rewards, the goal can be updated progressively. As analyzed in Section 4, in such an update scheme, past RTGs 

ˆRt−k+1: t−1 become redundant for current decision-making. This scheme has a critical characteristic: the genera-tion of the new condition (goal or RTG) depends entirely on the observable history, including observations, actions, and rewards. This information is either directly fed into the sequence model or can be reliably inferred by it (e.g., in a POMDP, the reward sequence can be deduced from the observation-action sequence, providing no additional decision-relevant information). In such cases, the DDT ar-chitecture remains effective. It does not need to rely on the full sequence of redundant goal conditions; conditioning on the latest goal is sufficient for action generation. However, if the goal update mechanism in GCSL does not possess this characteristic, for instance, if the goal at each step depends on information external to the observable his-tory, such as being specified by an oracle with an unknown internal state and transition dynamics, then our DDT frame-work would no longer be applicable. In this scenario, pro-viding the entire sequence of past goals to the model would convey strictly more information than providing only the lat-est one. One possible solution to this problem lies in aug-menting the observations provided to the model so that they encompass the information internal to the oracle. 

7.2 Role of RTG Sequence in DT 

Our analysis in Section 4 demonstrates that all but the final item in the RTG sequence are redundant. While this observa-tion might appear to suggest that paradigms like DT, which utilize the entire RTG sequence as input, are unnecessary, our findings indicate that such paradigms nevertheless hold value under certain circumstances, despite might not being the most efficient implementation. One scenario involves problems that do not conform to the standard POMDP assumption. In such cases, updating the belief state relies not only on the observation and action se-quences but also on the reward. Here, considering the en-tire RTG sequence effectively encodes the reward information from each step, which can provide gains for decision-making. The second scenario involves standard POMDPs. Al-though the RTG sequence is theoretically redundant, it can improve learning by enriching the feature representation. A typical case is when states with minimal visual differences yield vastly different immediate rewards. Providing per-step rewards enables the network to discern these critical sub-tleties and better predict high-return actions. While the RTG sequence can provide gains in the afore-mentioned two scenarios, we hold a different perspective from DT regarding how to utilize the per-step reward infor-mation it contains. We posit that when per-step reward or RTG information is necessary as model input, it should be in-corporated as a field within the observation, rather than being treated as an independent input token to the Transformer, par-allel to observations and actions. This preference is based on two considerations: From the perspective of computational cost, the quadratic complexity of the Transformer’s input se-quence favors shorter sequences. In terms of input dimen-sions and information content, per-step reward is a scalar that carries significantly less information compared to the higher-dimensional observation and action vectors, making it unsuit-able as an independent token. Even in traditional RNN mod-els, shorter sequences still play a crucial role in enabling effi-cient policy learning. 

# 8 Conclusion and Future Work 

In this work, we identify and address a fundamental redun-dancy in the DT framework: conditioning the policy on the full history of RTGs is theoretically unnecessary and empiri-cally harmful to performance, an issue previously overlooked in the literature. Our proposed DDT architecture resolves this by utilizing adaLN to condition solely on the current RTG. This principled simplification yields a model that is not only more computationally efficient but also achieves superior per-formance, establishing a strong and efficient baseline. Future work includes several promising directions: (1) Advanced Condition Fusion: Exploring the applica-tion of DiT’s sophisticated, layer-wise condition modulation paradigms to DT architectures. (2) Architectural Generalization: Applying the DDT paradigm, conditioning on minimal necessary information, to other DT variants to enhance their efficiency and perfor-mance. References 

[ ˚ Astr¨ om, 1965] Karl Johan ˚Astr¨ om. Optimal control of markov processes with incomplete state information i. Journal of mathematical analysis and applications ,10:174–205, 1965. [Ba et al. , 2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Ge-offrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. [Badrinath et al. , 2023] Anirudhan Badrinath, Yannis Flet-Berliac, Allen Nie, and Emma Brunskill. Waypoint trans-former: Reinforcement learning via supervised learning with intermediate targets. Advances in Neural Information Processing Systems , 36:78006–78027, 2023. [Brandfonbrener et al. , 2022] David Brandfonbrener, Al-berto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? Advances in Neu-ral Information Processing Systems , 35:1542–1553, 2022. [Chen et al. , 2021] Lili Chen, Kevin Lu, Aravind Ra-jeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence model-ing. Advances in neural information processing systems ,34:15084–15097, 2021. [Cirulli, 2014] Gabriele Cirulli. 2048. https://play2048.co/, March 2014. Accessed: 2026-01-18. [De Vries et al. , 2017] Harm De Vries, Florian Strub, J´ er´ emie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville. Modulating early visual processing by language. Advances in neural information processing systems , 30, 2017. [Dhariwal and Nichol, 2021] Prafulla Dhariwal and Alexan-der Nichol. Diffusion models beat gans on image synthe-sis. Advances in neural information processing systems ,34:8780–8794, 2021. [Fu et al. , 2020] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219 , 2020. [Fujimoto and Gu, 2021] Scott Fujimoto and Shixi-ang Shane Gu. A minimalist approach to offline reinforcement learning. Advances in neural information processing systems , 34:20132–20145, 2021. [Fujimoto et al. , 2019] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on ma-chine learning , pages 2052–2062. PMLR, 2019. [Furuta et al. , 2022] Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offline hindsight information matching. In International Conference on Learning Representations , 2022. [Gao et al. , 2024] Chen-Xiao Gao, Chenyang Wu, Mingjun Cao, Rui Kong, Zongzhang Zhang, and Yang Yu. Act: Empowering decision transformer with dynamic program-ming via advantage conditioning. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 12127–12135, 2024. [Gu and Dao, 2024] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In 

First conference on language modeling , 2024. [Huang et al. , 2024] Sili Huang, Jifeng Hu, Zhejian Yang, Liwei Yang, Tao Luo, Hechang Chen, Lichao Sun, and Bo Yang. Decision mamba: Reinforcement learning via hybrid selective sequence modeling. Advances in Neural Information Processing Systems , 37:72688–72709, 2024. [Kaelbling et al. , 1998] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Artificial intel-ligence , 101(1-2):99–134, 1998. [Karras et al. , 2019] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,pages 4401–4410, 2019. [Kim et al. , 2023] Jeonghye Kim, Suyoung Lee, Woojun Kim, and Youngchul Sung. Decision convformer: Local filtering in metaformer is sufficient for decision making. In NeurIPS 2023 Foundation Models for Decision Making Workshop , 2023. [Kostrikov et al. , 2021] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with im-plicit q-learning. arXiv preprint arXiv:2110.06169 , 2021. [Kumar et al. , 2019] Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint arXiv:1912.13465 , 2019. [Kumar et al. , 2020] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in neural infor-mation processing systems , 33:1179–1191, 2020. [Levine et al. , 2020] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tu-torial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020. [Lv et al. , 2024] Qi Lv, Xiang Deng, Gongwei Chen, Michael Yu Wang, and Liqiang Nie. Decision mamba: A multi-grained state space model with self-evolution reg-ularization for offline rl. Advances in Neural Information Processing Systems , 37:22827–22849, 2024. [Paster et al. , 2022] Keiran Paster, Sheila McIlraith, and Jimmy Ba. You can’t count on luck: Why decision trans-formers and rvs fail in stochastic environments. Advances in neural information processing systems , 35:38966– 38979, 2022. [Peebles and Xie, 2023] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceed-ings of the IEEE/CVF international conference on com-puter vision , pages 4195–4205, 2023. [Peng et al. , 2024] Zhiyong Peng, Yadong Liu, and Zongtan Zhou. Deadly triad matters for offline reinforcement learn-ing. Knowledge-Based Systems , 284:111341, 2024. [Radford et al. , 2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [Seno and Imai, 2022] Takuma Seno and Michita Imai. d3rlpy: An offline deep reinforcement learning library. 

Journal of Machine Learning Research , 23(315):1–20, 2022. [Smallwood and Sondik, 1973] Richard D Smallwood and Edward J Sondik. The optimal control of partially observ-able markov processes over a finite horizon. Operations research , 21(5):1071–1088, 1973. [Subramanian et al. , 2022] Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. Approximate information state for approximate planning and reinforce-ment learning in partially observed systems. Journal of Machine Learning Research , 23(12):1–83, 2022. [Tassa et al. , 2018] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690 , 2018. [Vaswani et al. , 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems ,30, 2017. [Wang et al. , 2024] Yuanfu Wang, Chao Yang, Ying Wen, Yu Liu, and Yu Qiao. Critic-guided decision transformer for offline reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 15706–15714, 2024. [Wang et al. , 2025] Jincheng Wang, Penny Karanasou, Pengyuan Wei, Elia Gatti, Diego Martinez Plasencia, and Dimitrios Kanoulas. Long-short decision transformer: Bridging global and local dependencies for generalized decision-making. In ICLR , pages 1–25. OpenReview. net, 2025. [Wu et al. , 2019] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361 , 2019. [Wu et al. , 2023] Yueh-Hua Wu, Xiaolong Wang, and Masashi Hamaya. Elastic decision transformer. Advances in neural information processing systems , 36:18532– 18550, 2023. [Yamagata et al. , 2023] Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez. Q-learning decision transformer: Leveraging dynamic programming for conditional se-quence modelling in offline rl. In International Confer-ence on Machine Learning , pages 38989–39007. PMLR, 2023. [Yan et al. , 2024] Kai Yan, Alex Schwing, and Yu-Xiong Wang. Reinforcement learning gradients as vitamin for on-line finetuning decision transformers. Advances in Neural Information Processing Systems , 37:38590–38628, 2024. [Yang et al. , 2023] Sherry Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Dichotomy of control: Sep-arating what you can control from what you cannot. In 

The Eleventh International Conference on Learning Rep-resentations , 2023. [Zheng et al. , 2022] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In inter-national conference on machine learning , pages 27042– 27059. PMLR, 2022. [Zheng et al. , 2025] Hongling Zheng, Li Shen, Yong Luo, Deheng Ye, Shuhan Xu, Bo Du, Jialie Shen, and Dacheng Tao. Value-guided decision transformer: A unified rein-forcement learning framework for online and offline set-tings. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025.