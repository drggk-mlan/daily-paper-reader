# Next Generation Active Learning: Mixture of LLMs in the Loop
# 下一代主动学习：循环中的大语言模型混合

**Authors**: Yuanyuan Qi, Xiaohao Yang, Jueqing Lu, Guoxiang Guo, Joanne Enticott, Gang Liu, Lan Du
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15773v1
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 6.0
**Evidence**: framework utilizing a mixture of large language models for annotation

---

## Abstract
With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

## 摘要
随着大语言模型（LLMs）的快速发展和强大的泛化

---

## 速览摘要（自动生成）

**问题**：LLM 标注质量不佳，难以在主动学习中完全替代人工。

**方法**：提出“混合 LLM 在环”框架，通过聚合多个轻量级 LLM 优势进行标注，并引入标注差异与负学习机制来识别和处理噪声标签。

**结论**：性能媲美人工标注，显著优于单模型及现有集成方法，且支持低成本本地部署。