Title: Structured Hints for Sample-Efficient Lean Theorem Proving

URL Source: https://arxiv.org/pdf/2601.16172v1

Published Time: Fri, 23 Jan 2026 02:09:18 GMT

Number of Pages: 9

Markdown Content:
# Structured Hints for Sample-Efficient Lean Theorem Proving 

## Zac Burton Massachusetts Institute of Technology January 23, 2026 

Abstract 

State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 com-bine large language models with reinforcement learning, achieving im-pressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention—a fixed prompt schedule over 15 common tactic skeletons—on the miniF2F benchmark. This sim-ple approach yields 21.7% pass@16 compared to 15.2% for standard sam-pling from the same model, a 43% relative improvement using the same number of samples ( k = 16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers under-utilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost. 

# 1 Introduction 

Recent advances in neural theorem proving have demonstrated remarkable suc-cess by combining Large Language Models (LLMs) with formal verification environments like Lean. Systems leveraging reinforcement learning (RL) and massive tree searches have achieved strong results on formal proof benchmarks [6, 2, 5]. The prevailing paradigm often assumes that with sufficient RL train-ing and inference-time compute, models will implicitly internalize the necessary structural and tactic-level priors required for formal proofs. However, despite these capabilities, neural provers remain surprisingly brit-tle. Even RL-trained models frequently fail due to low-level structural er-rors—generating invalid syntax, hallucinating identifiers, or getting “lost” in the proof state space—rather than a lack of mathematical insight. This discon-nect raises a critical question for resource-constrained settings: Has RL actually solved the structural problem, or can lightweight, fixed-schedule guidance still yield significant gains? 

In this work, we investigate whether enforcing structural priors at inference time improves performance under the same inference budget. Unlike recent 1

> arXiv:2601.16172v1 [cs.AI] 22 Jan 2026

approaches that rely on expensive stochastic search or massive sampling (e.g., 

k = 1024), we propose a lightweight Intermediate Representation (IR) 

with a fixed prompt schedule. By guiding the model with structural skeletons, we probe the behavioral limits of current RL-trained provers. Our results on the miniF2F benchmark show that this simple structural guidance yields a 43% relative improvement over the baseline (15.16% →

21.72%) under a fixed, low-inference budget ( k = 16). Our paired analysis shows strong asymmetry (19 wins vs. 3 losses), indicating that tactic skeletons provide a robust boost rather than merely trading off between problem types. 

## 1.1 Contributions 

Our primary contributions are as follows: 

• We introduce a Lean-aware Intermediate Representation (IR) with a fixed prompt schedule that provides structural guidance at inference time. 

• We demonstrate that this lightweight intervention significantly outper-forms standard sampling on miniF2F under identical inference budgets (k = 16, 1024 tokens). 

• We provide a failure-mode analysis showing that error distributions are similar between methods; gains appear to come from a higher hit-rate on successful completions rather than systematic avoidance of particular error types. 

# 2 Related Work 

Neural Theorem Proving The integration of Language Models with formal proof assistants has evolved rapidly. Early systems like GPT-f [4] and PACT [3] demonstrated that transformers could generate tactic scripts. More recent efforts, such as LeanDojo and its associated ReProver model [7], introduced retrieval-augmented generation to handle the vast library of Lean premises. The current state-of-the-art is dominated by models trained with massive Reinforce-ment Learning (RL) pipelines, notably DeepSeek-Prover-V1.5 [6] for Lean tactic generation and AlphaGeometry [5] for geometry-specific formal reason-ing, which rely on iterative self-improvement and synthetic data generation to achieve strong performance on formal benchmarks. 

Neuro-Symbolic Reasoning Recent work has moved towards hybrid sys-tems that interleave natural language reasoning with formal verification. Aris-totle [1] achieved state-of-the-art results on IMO problems by generating and formalizing semantic lemmas (intermediate mathematical statements) to bridge the gap between informal reasoning and formal proof. Our work targets a complementary, lower-level efficiency constraint. Rather than generating new mathematical lemmas (which requires heavy reasoning), we 2focus on syntactic skeletons —enforcing valid tactic structures (e.g., induction ,

cases ) at the start of generation, serving as a hint for the LLM’s approach to the problem. We demonstrate that this lightweight structural guidance captures significant performance gains in resource-constrained environments where full lemma synthesis is too costly. 

# 3 Method 

To evaluate the impact of structural guidance under strict inference constraints, we introduce an Intermediate Representation (IR) with a fixed prompt sched-ule. Unlike standard autoregressive generation which conditions solely on the theorem statement, our approach conditions the model on a structural tuple that explicitly separates proof planning from tactic execution. 

## 3.1 Structured Intermediate Representation 

Formally, let T be the space of theorem statements in Lean, and P be the space of valid tactic sequences. We define a Structured Query q as a tuple: 

q = ( x, s ) (1) where: 

• x ∈ T is the formal theorem statement (the goal). 

• s ∈ S is a tactic skeleton (a prefix of tactics representing the high-level proof strategy, e.g., simp , intro , constructor ). The generation process is modeled as Pθ (y | x, s ), where the model θ is constrained to complete the proof y given the enforced structural start s. This effectively “locks” the model into a specific proof strategy before inference be-gins, reducing the search space of valid next tokens. 

## 3.2 Fixed Prompt Schedule 

Instead of relying on stochastic temperature sampling alone, we enforce diversity through a fixed prompt schedule. For a given theorem x, we generate a set of 

k = 16 distinct queries Qx = {q1, . . . , q 16 } by iterating over a fixed set of 15 tactic skeletons (including the empty skeleton; full list in Appendix A). The 16th attempt reuses the empty skeleton, yielding one duplicate in the schedule. 1

This schedule ensures that the model attempts a diverse range of proof strategies regardless of the model’s internal probability distribution, which might otherwise collapse to a single, incorrect mode. Note that while the prompt schedule is fixed, the model still samples stochastically (temperature = 0 .6) to generate the proof completion.   

> 1Our implementation also supports optional natural-language goal hints; for this paper we set all hints to ∅except attempt 16, which includes a generic hint. Ablations isolating skeleton vs. hint contributions are left for future work.

33.3 Experimental Setup & Constraints 

To strictly evaluate the model’s structural efficiency without the confounding factor of massive search, we enforce strict computational constraints on all eval-uations. Specifically, we limit generation to a maximum of 1024 tokens and restrict sampling to k = 16 attempts per theorem. While recent benchmarks often utilize significantly larger budgets (e.g., k =1024, extended context windows), these approaches are computationally pro-hibitive for many practical applications. Our experimental design explicitly targets this resource-constrained regime to evaluate whether structural guid-ance can recover performance lost to limited compute. As such, our baseline results reflect this strict cutoff and should be interpreted as a low-latency per-formance floor , rather than a ceiling of model capability. 

## 3.4 Evaluation Protocol 

We evaluate on miniF2F-test (Lean 4 split; 244 theorems) using DeepSeek-Prover-V1.5-RL 2 in completion-style prompting. For each theorem, we run up to k = 16 attempts with a maximum of 1024 generated tokens per attempt under fixed decoding parameters (temperature = 0 .6, top-p = 0 .95; see Ap-pendix A for the full prompt template). Each attempt is assembled into a Lean 4 file containing import Mathlib followed by the full benchmark theorem state-ment and := by with the generated tactics. We compile using lake env lean --json inside a pinned Mathlib environment (Appendix B) with a per-attempt timeout of 60 seconds. A theorem is counted as solved if and only if Lean returns exit code 0; any output containing sorry is treated as failure. 

# 4 Results 

We evaluate the effectiveness of our approach on the miniF2F-test benchmark (Lean split), consisting of 244 formal theorems. We compare our Structured Intermediate Representation (IR) against the unguided Baseline under identical inference constraints ( k = 16, max tokens=1024). 

## 4.1 Main Performance 

Table 1 summarizes the pass rates. Our structured approach solves 53 theorems (21.72%), compared to 37 theorems (15.16%) for the baseline. This represents a net absolute gain of 6.56 percentage points and a relative improvement of 43.2% .

## 4.2 Paired Analysis & Overlap 

To understand whether our method simply solves a different subset of problems or strictly improves upon the baseline, we conducted a paired analysis of solved  

> 2deepseek-ai/DeepSeek-Prover-V1.5-RL from HuggingFace

4Method Solved (N) Pass@16 (%) Relative Gain 

Baseline (Unguided) 37 / 244 15.16% -

Structured IR (Ours) 53 / 244 21.72% +43.2% 

Table 1: Performance comparison on miniF2F-test. Both methods use DeepSeek-Prover-V1.5-RL with k = 16 samples and a strict 1024-token limit. 

Figure 1: Venn diagram detailing the overlap of solved problems theorems (Figure 1). 

• Intersection (Both Solved): 34 theorems were solved by both methods. 

• Structured-Only Wins: Our method solved 19 theorems that the base-line failed to solve. 

• Baseline-Only Wins: The baseline solved only 3 theorems that our method missed. This strong asymmetry (19 wins vs. 3 losses) indicates that structural guid-ance does not merely trade off one type of reasoning for another; rather, it strictly dominates the unguided approach in this resource-constrained regime. Using a paired McNemar test on per-theorem outcomes, the improvement is statistically significant ( p < 0.001, two-tailed). 

## 4.3 Failure Mode Analysis 

We analyzed the compilation logs for the failed attempts, classifying errors by the first diagnostic returned by Lean’s JSON output. Table 2 summarizes the distribution of error types across all failed attempts. Notably, the error distributions between methods are qualitatively similar. This suggests that the Structured IR’s performance gains do not come primarily 5Error Type Baseline (%) Structured IR (%) 

unsolved goals 32.9 30.4 syntax error 30.0 29.3 unknown identifier 18.4 19.9 other 14.5 15.2 typeclass error 2.4 2.6 type error 1.9 2.1 Table 2: Distribution of first-failure error types across failed attempts (Baseline: 207 failures, Structured IR: 191 failures). The error distributions are similar, suggesting the Structured IR’s gains come from successfully completing proofs that would otherwise fail, rather than from shifting the failure mode. from avoiding specific error types, but rather from guiding the model toward successful proof completions more often. By enforcing a valid tactic skeleton at the start of generation, the model may avoid early missteps that cascade into failures—the skeleton provides a “warm start” that increases the probability of reaching a valid proof, even though failed attempts still exhibit similar error patterns. The 16 additional theorems solved (53 vs. 37) represent cases where the skeleton-guided generation happened to align with the correct proof strategy. 

# 5 Discussion 

Our findings challenge the assumption that “more scale” is the only path to bet-ter theorem proving. By simply enforcing valid structural priors—specifically, valid tactic skeletons—we achieved a 43% relative improvement in pass rate. Interestingly, our error analysis (Table 2) shows that the failure mode distri-bution remains similar between methods. This suggests that the Structured IR’s gains come not from systematically avoiding particular error types, but from in-creasing the hit rate on successful proofs. The skeleton acts as a “seed” that, when aligned with the correct proof strategy, guides the model to completion; when misaligned, it fails in similar ways to unguided sampling. The key advan-tage is that by cycling through diverse skeletons, we increase the probability that at least one attempt aligns with a viable proof strategy. Furthermore, under the strict constraint ( k = 16, 1024 tokens), our method strictly dominates the unguided baseline (19 unique solves vs. 3). This sug-gests that for resource-constrained applications—where massive tree search is infeasible—structural guidance via a fixed prompt schedule is a more efficient lever than pure stochastic sampling. 

# 6 Limitations & Future Work 

The primary limitation of this study is the strict computational budget. Due to resource constraints, we limited generation to 1024 tokens, which likely trun-6cated valid proofs that required verbose Lean states. While this serves as a rigorous test of low-latency performance, it prevents a direct comparison with state-of-the-art leaderboards that utilize extended context windows and massive sampling ( k > 1000). Additionally, we report results from a single run with one seed and one decoding configuration; we did not sweep multiple random seeds or decoding settings, so run-to-run variability remains unmeasured. Future work will explore two directions: 

• Scaling the IR: Investigating whether this structural guidance continues to provide gains when scaled to larger budgets (e.g., k = 128) or stronger base models. 

• Dynamic Skeletons: Currently, our tactic skeletons are fixed. A learned “skeleton retriever” that dynamically predicts the best structure for a given theorem could further refine the query generation process. 

# A Prompts and Skeleton List 

## A.1 Prompt Template 

For DeepSeek-Prover-V1.5-RL, we use completion-style prompting (no chat tem-plate). The prompt format is: 

/- Lean 4 with Mathlib4 -/ import Mathlib open BigOperators Real Nat Topology <theorem statement> := by <tactic skeleton> 

The model then generates the proof completion after the skeleton prefix. 

## A.2 Tactic Skeletons 

We use the following 15 tactic skeletons, cycled through for each theorem. For 

k = 16, the 16th attempt reuses skeleton 1 (empty) paired with a generic goal hint. 1. (empty) — no prefix 2. simp 

3. intro 

4. intros 

75. constructor 

6. refine ? 

7. refine ⟨? , ? ⟩

8. aesop 

9. norm num 

10. linarith 

11. nlinarith 

12. ring 

13. ring nf 

14. simp → try aesop 

15. simp → try nlinarith 

The 16th attempt uses skeleton 1 with the hint: “Start by simplifying the goal and hypotheses using simp .” 

# B Environment and Evaluation Harness 

## B.1 Lean Environment 

• Lean version: Lean 4 v4.27.0-rc1 

• Mathlib commit: 3c327186024184e988ebbcae1b7d7795eaacdee3 

• Verification command: lake env lean --json <file.lean> 

## B.2 Model Configuration 

• Model: deepseek-ai/DeepSeek-Prover-V1.5-RL 

• Temperature: 0.6 (for pass@ k sampling) 

• Top-p: 0.95 

• Max tokens: 1024 

• Per-attempt timeout: 60 seconds 

## B.3 Pass Criteria 

A theorem is considered solved if: 1. Lean returns exit code 0 (successful compilation) 2. The generated proof does not contain sorry 

8References 

[1] Achim, T., Best, A., Bietti, A., Der, K., F´ ed´ erico, M., Gukov, S., Halpern-Leistner, D., Henningsgard, K., Kudryashov, Y., Meiburg, A., Michelsen, M., Patterson, R., Rodriguez, E., Scharff, L., Shanker, V., Sicca, V., Sowrirajan, H., Swope, A., Tamas, M., Tenev, V., Thomm, J., Williams, H., and Wu, L. Aris-totle: IMO-level Automated Theorem Proving, 2025. [2] Chen, L., Gu, J., Huang, L., Huang, W., Jiang, Z., Jie, A., Jin, X., Jin, X., Li, C., Ma, K., Ren, C., Shen, J., Shi, W., Sun, T., Sun, H., Wang, J., Wang, S., Wang, Z., Wei, C., Wei, S., Wu, Y., Wu, Y., Xia, Y., Xin, H., Yang, F., Ying, H., Yuan, H., Yuan, Z., Zhan, T., Zhang, C., Zhang, Y., Zhang, G., Zhao, T., Zhao, J., Zhou, Y., and Zhu, T. H. Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving, 2025. [3] Han, J. M., Rute, J., Wu, Y., Ayers, E., and Polu, S. Proof Arti-fact Co-training for Theorem Proving with Language Models. International Conference on Learning Representations (ICLR) (2022). [4] Polu, S., and Sutskever, I. Generative Language Modeling for Auto-mated Theorem Proving, 2020. [5] Trinh, T., Wu, Y., Le, Q. V., He, H., and Luong, T. Solving olympiad geometry without human demonstrations. Nature 625 (2024), 476–482. [6] Xin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan, C., Li, W., and Liang, X. DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data, 2024. [7] Yang, K., Swope, A., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, S., Prenger, R., and Anandkumar, A. LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. Advances in Neural Information Processing Systems 36 (2023), 10065–10081. 9