Title: Performance-guided Reinforced Active Learning for Object Detection

URL Source: https://arxiv.org/pdf/2601.15688v1

Published Time: Fri, 23 Jan 2026 01:25:50 GMT

Number of Pages: 6

Markdown Content:
PERFORMANCE-GUIDED REINFORCED ACTIVE LEARNING FOR OBJECT DETECTION 

Zhixuan Liang ‚ãÜ‚Ä° Xingyu Zeng ‚Ä† Rui Zhao ‚Ä† Ping Luo ‚ãÜ‚ãÜ The University of Hong Kong ‚Ä†SenseTime Research 

ABSTRACT 

Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data‚Äôs distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided ( i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel ap-proach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL‚Äôs active learning performance on detec-tion tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection. 

Index Terms ‚Äî Active learning, reinforcement learning, active object detection 

1. INTRODUCTION 

While recent years have seen significant progress in model architectures, attention is increasingly shifting towards more efficient data utilization strategies. Among them, Active Learning (AL) stands out by training high-performance mod-els with minimal labeling, particularly when field data arrives continuously and annotation is costly. By strategically se-lecting and annotating the most informative samples, active learning substantially improves data efficiency Therefore, the central question becomes how to identify most informative data. Prior methods select the data points that are complementary to the currently labeled set in fea-ture space, no matter characterized by distributional coverage 

‚Ä° This work is partially done during Zhixuan‚Äôs internship at SenseTime. (a) Data Sampling Agent Training Phase     

> (b) Data Selection with Trained Sampling Agent
> Data Sampling
> Agent
> Unlabeled image data pool
> Detector
> Unsupervised Detection Model
> Training with Selected Samples
> Perform inference
> after training
> Train Agent with
> ùúüùíéùë®ùë∑ as reward
> Selected
> Samples
> 45
> 50
> 55
> 60
> Sample list1 Sample list2
> Performance
> at last cycle
> ùúüùíéùë®ùë∑
> Performance Record
> Selected
> Samples
> Query
> Oracle
> Unlabeled image data pool
> Data Sampling
> Agent
> Unselected
> Labeled image data pool
> aeroplane
> New annotated

Fig. 1 : Overview of Performance-guided (mAP-guided) Reinforced Active Learning (MGRAL) for object detec-tion. The RL-based agent learns to select most informative samples directly using performance gains ( ‚àÜmAP) as reward. or intrinsic ‚Äúinformation‚Äù. Although various definitions of this measure have been proposed and utilized as query strate-gies [1‚Äì9], they do not align directly with downstream met-rics, such as mean Average Precision (mAP) in object de-tection. EMOC [10] addresses this by measuring expected model output change, which matches our objective, but it was developed for Gaussian process regression and is not readily applicable to deep learning architectures. In this paper, we propose Performance-guided (mAP-guided) Reinforced Active Learning (MGRAL) for object detection, directly using mAP changes to guide data sam-pling. However, different from EMOC not using deep learn-ing downstream task model, our optimization pipeline must handle discrete, non-differentiable batch selection from the unlabeled pool, which makes it unable to update the se-lected samples through gradient from downstream metrics straightly. We therefore employ a reinforcement learning-based sampling agent that uses mAP variation ( ‚àÜmAP) as the reward to enable optimizing the selection process through 

> arXiv:2601.15688v1 [cs.CV] 22 Jan 2026

policy gradient technique. This approach achieves efficient exploration of possible batch combinations, maximizing mAP improvement per selected batch. Because candidate samples are unlabeled when estimat-ing potential mAP changes, we substitute an unsupervised surrogate for the detector to approximate downstream perfor-mance. It is effective as we only need to capture the relative influence among candidate data batches instead of correct la-bels to guide the active learning process. To mitigate the high cost made by reinforcement learning (RL)-based agent of re-training the whole detector from scratch once per RL itera-tion, we introduce a fast lookup-table accelerator. Empiri-cal results demonstrate our AL method‚Äôs efficiency on both Pascal VOC [11] and MS COCO [12] across two detector backbones. Visualizations further show that selection crite-ria should vary across training stages, and only downstream performance provides a reliable signal. Our contributions can be summarized as: (1) We pro-pose MGRAL, extending the concept of expected model output changes for deep active learning, directly aligning the sampling strategy with mAP improvement. (2) We intro-duce a reinforcement learning-based agent for efficient batch selection, addressing the combinatorial explosion and non-differentiable challenge in optimizing from mAP improve-ment. (3) We develop practical techniques for MGRAL, and evaluate it on multiple downstream benchmarks and different backbones, revealing our method‚Äôs superior performance and the key effect that expected model output change played in it. 

2. RELATED WORKS 2.1. Active Learning for Object Detection 

Earliest attempts in active learning for object detection in-clude LL4AL [1] adapting instance loss predictions and AL4DeepDetection [13] combining uncertainty metrics for foreground objects and background pixels. CDAL [2] then enhances sample representativeness through spatial context, while MIAL [3, 14] employs adversarial classifiers and an unsupervised framework. EBAL [5] integrates uncertainty and diversity but faces challenges with computational com-plexity and class imbalance. More recent works including ComPAS [9], MEH+HUA [6], MSS [7] and PPAL [8] focus more on efficient sample uncertainty definition and provides new solutions to batch selection problem. However, these methods do not directly align with the task model‚Äôs performance. Our work, instead, directly utilizes mAP improvement to guide the selection process, addressing the limitations of previous approaches in balancing various metrics and handling batch selection effectively. 

2.2. Reinforcement Learning in Active Learning 

Reinforcement learning has been adopted to learn better query strategies in active learning, aiming to maximize task model performance. Techniques include imitation learning [15], bi-directional RNNs [16], Deep Q-Networks [17, 18], and pol-icy gradient for simultaneous learning of data representation and selection heuristics [19] have been utilized to address data representativeness and develop efficient sampling strate-gies. However, these methods struggle with integrating multi-instance uncertainty within a single image and batch sample selection for object detection tasks. Our approach addresses these challenges by leveraging ‚àÜmAP as the reward to opti-mize the batch sample selection strategy. 

3. METHODOLOGY 3.1. MGRAL Active Learning Pipeline 

As depicted in Fig. 1, to handle the non-differentiable link between sample selection and downstream performance changes, we embed a reinforcement learning-based sampling agent into classical pool-based active learning framework, optimizing selections for mAP improvement. 

Training. MGRAL uses a nested optimization scheme: the outer RL loop trains the sampler; the inner detector loop estimates mAP gains for each RL step. For each AL round (cycle t), [Inner Loop] we build a lookup table by train-ing multiple detector variants in parallel (Sec. 3.5). The re-sults are pre-recorded and serve for efficiently approximat-ing performance gains. [Outer Loop] The sampling agent (Sec. 3.2) then runs RL iterations ( i), selecting candidate sets from the unlabeled pool and receiving feedback from the es-timated mAP improvements (Sec. 3.3). This feedback up-dates the policy via our stabilized policy-gradient optimiza-tion (Sec. 3.4). 

Inference (active selection). The trained agent traverses the unlabeled pool with its sequence-model-based architec-ture, computing for each image a selection score from image features and historical selection context, and finally chooses the top-B (budget) images for annotation. This design enables MGRAL to efficiently bridge sample selection and detector performance gains. 

3.2. MGRAL Data Sampling Agent 

The sampling agent adopts a Neural Architecture Search-inspired design, utilizing Long Short-Term Memory (LSTM) [20] network for sequential sample selection. As the architecture of sampling agent shown in Fig. 2, each image Ik is first pro-cessed through a pre-trained detection encoder Œ¶( ¬∑), which derives a feature vector Œ¶( Ik). This architecture considers both new data features and previous selection patterns through combining each image‚Äôs embedding with the preceding unit‚Äôs decision vector. And then, the extracted representation flows through parameter-sharing LSTM modules that ensure se-quential modeling while preventing gradient vanishing. After LSTM, a 2-layer MLP decoder Œ®k(¬∑) (top in Fig. 2) outputs selection scores from LSTM hidden states for each sample. LSTM     

> Module
> LSTM
> Module
> LSTM
> Module
> img 1 embedding
> ‚Ñé!
> ùëê !
> Image Feature Encoder
> ‚Ñé"
> ‚Ä¶
> ‚Ä¶
> ‚Ñé#
> dummy
> head
> unlabeled image dataset
> img 2 embedding img nembedding
> ‚Ñé!
> Decoder 1
> Selection Score
> ‚Ñé"
> ùëê "
> ‚Ñé#$!
> ùëê #$!
> Decoder 2
> Selection Score
> Decoder n
> Selection Score

Fig. 2 : Data sampling agent architecture. 

After processing the whole sequence, our algorithm se-lects the top-B (budget) samples that achieves highest scores from the unlabeled pool. This selection mechanism forms the basis of our reward computation and policy updates. 

3.3. Performance-Driven Reward Design 

The reward mechanism is the cornerstone of our approach, which links the sample selection directly with detector perfor-mance improvement ( ‚àÜmAP). Detailedly, at active learning cycle t, for each RL training iteration i, MGRAL first selects one batch of sample candidates (without labels) and combines them with currently labeled dataset. Assuming we have labels of these selected candidates, we can train a new detector from scratch with the combined dataset to compute mAP ti. Then, 

‚àÜmAP ti = mAP ti ‚àí mAP ti‚àí1. (1) However, candidates are only annotated after selection by the trained RL agent Œ∏t

> agent

. To address this, we employ a semi-supervised detection model trained on labeled XtL and unlabeled XtS,i to approximate the performance of a fully-supervised detector trained on XtL +XtS,i . In practice, various semi-supervised methods ( e.g. ISD-SSD [21]) can be adopted depending on the detector backbone, following an objective: 

LTotal = LS + w(t) √ó LU , where LS denotes supervised loss on labeled data, LU represents unsupervised consistency losses on unlabeled data, and w(t) is a time-dependent weight that gradually increases during training. After that, we only need to replace mAP ti in Eq. 1 with this approximated term. This manner is sufficient as we only need to rank and identify the most potentially impactful samples for query strategy, not make exact performance predictions. 

3.4. Stabilized Policy-gradient Optimization 

As illustrated above, the training of MGRAL centers on itera-tive refinement of RL-based data sampling agent. Using pol-icy gradient , we enable model output changes to flow back to this discrete selection process, allowing effective update of the agent‚Äôs parameters. Moreover, to stabilize the policy gradient training, we employ a moving average baseline to normalize the reward signal and reduce variance in the policy gradient updates. Let mAP ti denote the mAP achieved at AL cycle t and RL iteration i as above. We additionally maintain a reference mAP ref baseline to track the performance trend and smooth the ‚àÜmAP ti signal as: mAP tref,i = Œª‚àómAP tref,i ‚àí1 +(1 ‚àíŒª)‚àó(mAP ti ‚àímAP tref,i ‚àí1),

(2) where Œª is the momentum hyper-parameter that controls the update rate. Then, we substitute this reference mAP tref,i ‚àí1 to the actual mAP ti‚àí1 in Eq. 1 and rewrite policy gradient loss: 

loss Œ∏agent ,i = ‚àí‚àÜmAP ti = ‚àí(mAP ti ‚àí mAP tref,i ‚àí1). (3) This normalized reward indicates whether the current batch selection is better or worse than the recent average, enabling effective optimization over the discrete selection process. 

3.5. Acceleration Technique 

Another critical challenge of our approach lies in the com-putational cost of our inner loop optimization. By default, MGRAL‚Äôs each RL iteration requires a complete retraining of the unsupervised detector to estimate mAP for one candidate batch selection, yielding a prohibitive computation of 2200 (VOC) and 800 (COCO) sequential trainings per AL cycle. We introduce a fast lookup-table (LUT) technique for ac-celeration. Before each cycle‚Äôs agent training, we train M un-supervised models in parallel and record their performance, each on the current labeled data XtL plus a randomly selected batch of unlabeled data ÀúXtS,l (l = 1 , 2, ..., M ) with the size 

| ÀúXtS,l | equal to AL budget B. Then during the agent‚Äôs RL training, instead of retraining the detector again and again, we compute the Wasserstein distance [22] (an optimal-transport metric) between visual representations of the agent-selected batch and LUT entries, and approximate mAP via a distance-weighted sum of the most similar records (weights inversely proportional to distance). If no entry meets a preset thresh-old ( e.g. , mean minus one standard deviation), we fall back to direct retraining. This acceleration converts thousands of se-quential trainings into one parallel phase per cycle, delivering 

> 1000 √ó speedup and making MGRAL practical. 

4. EXPERIMENTS 4.1. Experiment Details 

Datasets and protocol: VOC07+12 and COCO2017. VOC-1k labeled init, +1k/cycle for 10 cycles; COCO-2.0% init, +2.0%/cycle to 10.0%. Detectors: SSD [23] on VOC; Reti-naNet [24] on COCO. Agent: 257-dimension hidden (256 im-age embedding + 1 prev-score) with an LSTM of equal width; 1 2 3 4 5 6 7 8 9 10 

> Images in thousands
> 50
> 55
> 60
> 65
> 70
> 75
> mAP  Random
> Entropy
> Core-set (ICLR'18)
> LL4AL (CVPR'19)
> MIAL (TPAMI'23)
> EBAL (CVPR'22)
> MEH+HUA (ICLR'23)
> PPAL (CVPR'24)
> Ours

(a) SSD on Pascal VOC 2 4 6 8 10  

> Images %
> 6
> 8
> 10
> 12
> 14
> 16
> 18
> 20
> 22
> AP (0.5:0.95)
> Random
> Entropy
> Core-set (ICLR'18)
> LL4AL (CVPR'19)
> CDAL (ECCV'20)
> MIAL (TPAMI'23)
> EBAL (CVPR'22)
> MEH+HUA (ICLR'23)
> Ours

(b) RetinaNet on MS COCO 

Fig. 3 : Comparative performance of active learning methods. The mAP is plotted against the number of labeled images. 

Table 1 : Training time of MGRAL with and without lookup table on Pascal VOC. 

Method Time of 10 Iters Time for One Cycle 

w/o acceleration 4800 min unknown 

w/ lookup table 3 min 640 min 

Table 2 : Ablation on Efficiency analysis. All results are about models tested on VOC. 

Method Training Time of One Cycle Inference Time of One Cycle # of params 

Entropy 0 5 min 26.3 M MIAL [3] 7 h 13 min 43 min 31.9 M EBAL [5] 0 181 min 26.5 M 

Ours (9 h) + 7 h 8 min 0.5 min 33.0 M Pascal VOC Top #1     

> MIAL
> Pascal VOC Top #2
> MIAL
> Pascal VOC Top #3
> MIAL
> Pascal VOC Top #4
> MIAL
> Pascal VOC Top #1
> Mean Entropy=2.6054
> Pascal VOC Top #2
> Mean Entropy=2.5561
> Pascal VOC Top #3
> Mean Entropy=2.2469
> Pascal VOC Top #4
> Mean Entropy=2.1987
> Pascal VOC Top #1
> Sum Entropy=8.2586
> Pascal VOC Top #2
> Sum Entropy=7.7521
> Pascal VOC Top #3
> Sum Entropy=7.7081
> Pascal VOC Top #4
> Sum Entropy=7.3701
> Pascal VOC Top #1
> Ours
> Pascal VOC Top #2
> Ours
> Pascal VOC Top #4
> Ours
> Pascal VOC Top #3
> Ours
> (a) Mean Entropy (b) Sum Entropy
> (c) MIAL (d) MGRAL (Ours)

Fig. 4 : Visualizations of the selected samples with highest scores by different AL methods during first cycle on VOC. 

Adam (lr 3.5 √ó 10 ‚àí4). RL iters per cycle: 2,200 (VOC), 800 (COCO); baseline decay Œª = 0 .5 for mAP ref . LUT: per cycle, M =200 ISD-SSD runs on VOC; on COCO, 30 SED-SSOD runs (150 records in total). 

4.2. Overall Performance 

We compare MGRAL against random, entropy, Core-set [25], CDAL [2], LL4AL [1], MIAL [3, 14], EBAL ( aka Di-vProto) [5], MEH+HUA [6], and PPAL [8]. Results in Fig. 3 show that on VOC, MGRAL consistently outperforms all methods, validating our mAP-guided design. On COCO, MGRAL surpasses most baselines. Despite a slight early lag vs. EBAL, MGRAL exhibits the steepest growth and finishes ahead. We attribute this to EBAL‚Äôs feature learning benefiting from more labels (weaker early, stronger mid-stage), whereas our performance-guided sampling yields larger early gains and better label-cost efficiency-especially clear on VOC. On COCO (117k vs. VOC‚Äôs 16.5k), the larger unlabeled pool induces initial parity; MGRAL then accelerates and takes the lead. This indicates MGRAL excels at fine-grained selection; on very large pools, a hierarchical scheme ( e.g. , clustering to prune the search space) could further amplify gains. In summary, mAP-as-reward directly optimizes detection, and the RL formulation can easily generalize to other tasks by changing the reward metric beyond detection. 

4.3. Visualization Analysis 

We visualize selections at the first AL cycle on PASCAL VOC (Fig. 4). It shows mean-entropy sampling favors ex-posed/blurred images (large entropy drop), which can hin-der early training; sum-entropy sampling picks images with many instances but from few categories, causing redundancy. MIAL [3] and MGRAL both prefer single, centered ob-jects that provide clean supervision, while MGRAL further promotes category diversity, improving robustness across classes. These patterns explain MGRAL‚Äôs higher mAP and validate its performance-guided, diversity-aware selection for building balanced early datasets. 

4.4. Ablation on Efficiency 

Our lookup table (LUT) accelerator slashes time cost: for 10 iterations on four GTX 1080Ti GPUs, training drops from 4,800 minutes to 3 minutes (Tab. 1). Although MGRAL incurs a slightly higher one-time setup than MIAL [3] and EBAL [5], it is more efficient in subsequent cycles due to shorter inference (Tab. 2), which aligns with offline AL work-flows where collection/selection/labeling are batched rather than real time. Space-wise, MGRAL remains competitive (33 .0M params vs. EBAL 26 .5M, 31 .9M), and the LUT adds only ‚àº 12 KB while enabling the above acceleration. 

5. CONCLUSION 

We present MGRAL, a Performance-guided Reinforced Ac-tive Learning framework for object detection. By using 

‚àÜmAP as reward, our method effectively aligns batch selec-tion with detection performance improvement, addressing the non-differentiable nature of selection process through pol-icy gradient. The integration of unsupervised approximation and lookup table acceleration enables practical deployment while maintaining efficiency. MGRAL achieves consistent improvements over prior methods on both VOC and COCO across different backbones, establishing a new paradigm that combines performance-driven reinforcement learning and ef-ficient active learning for object detection. Exploring early stopping techniques for faster mAP estimation and replacing lookup tables with prediction networks for online reinforce-ment learning are promising directions for future work. 6. REFERENCES 

[1] Donggeun Yoo and In So Kweon, ‚ÄúLearning loss for ac-tive learning,‚Äù in Proceedings of the IEEE/CVF confer-ence on computer vision and pattern recognition , 2019. [2] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora, ‚ÄúContextual diversity for active learning,‚Äù in European Conference on Computer Vision , 2020. [3] Tianning Yuan, Fang Wan, Mengying Fu, Jianzhuang Liu, Songcen Xu, Xiangyang Ji, and Qixiang Ye, ‚ÄúMul-tiple instance active learning for object detection,‚Äù in 

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 5330‚Äì5339. [4] Xiaosong Zhang, Fang Wan, Chang Liu, Xiangyang Ji, and Qixiang Ye, ‚ÄúLearning to match anchors for visual object detection,‚Äù IEEE Transactions on Pattern Analy-sis and Machine Intelligence , 2021. [5] Jiaxi Wu, Jiaxin Chen, and Di Huang, ‚ÄúEntropy-based active learning for object detection with progressive di-versity constraint,‚Äù in IEEE/CVF Conference on Com-puter Vision and Pattern Recognition , 2022. [6] Younghyun Park, Wonjeong Choi, Soyeong Kim, Dong-Jun Han, and Jaekyun Moon, ‚ÄúActive learning for object detection with evidential deep learning and hierarchical uncertainty aggregation,‚Äù in The Eleventh International Conference on Learning Representations , 2023. [7] Jiaxiang Dong and Li Zhang, ‚ÄúMultibox sample selec-tion for active object detection,‚Äù in IEEE International Conference on Multimedia and Expo (ICME) , 2023. [8] Chenhongyi Yang, Lichao Huang, and Elliot J Crowley, ‚ÄúPlug and play active learning for object detection,‚Äù in 

Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2024, pp. 17784‚Äì17793. [9] Mengyao Lyu, Jundong Zhou, Hui Chen, and Yijie Huang et al., ‚ÄúBox-level active detection,‚Äù in Proceed-ings of the IEEE/CVF conference on computer vision and pattern recognition , 2023, pp. 23766‚Äì23775. [10] Alexander Freytag, Erik Rodner, and Joachim Denzler, ‚ÄúSelecting influential examples: Active learning with expected model output changes,‚Äù in European confer-ence on computer vision . Springer, 2014, pp. 562‚Äì577. [11] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman, ‚ÄúThe pascal visual object classes (voc) challenge,‚Äù Interna-tional journal of computer vision , vol. 88, no. 2, 2010. [12] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll¬¥ ar, and C Lawrence Zitnick, ‚ÄúMicrosoft coco: Common objects in context,‚Äù in European conference on computer vision .Springer, 2014, pp. 740‚Äì755. [13] Hamed H Aghdam, Abel Gonzalez-Garcia, Joost van de Weijer, and Antonio M L¬¥ opez, ‚ÄúActive learning for deep detection neural networks,‚Äù in Proceedings of the IEEE/CVF International Conference on Computer Vi-sion , 2019, pp. 3672‚Äì3680. [14] Fang Wan, Qixiang Ye, Tianning Yuan, Songcen Xu, Jianzhuang Liu, Xiangyang Ji, and Qingming Huang, ‚ÄúMultiple instance differentiation learning for active ob-ject detection,‚Äù IEEE Transactions on Pattern Analysis and Machine Intelligence , 2023. [15] Ming Liu, Wray Buntine, and Gholamreza Haffari, ‚ÄúLearning how to actively learn: A deep imitation learn-ing approach,‚Äù in Proceedings of the 56th Annual Meet-ing of the Association for Computational Linguistics (Volume 1: Long Papers) , 2018, pp. 1874‚Äì1883. [16] Gabriella Contardo, Ludovic Denoyer, and Thierry Arti` eres, ‚ÄúA meta-learning approach to one-step active-learning,‚Äù in International Workshop on Automatic Selection, Configuration and Composition of Machine Learning Algorithms , 2017, vol. 1998, pp. 28‚Äì40. [17] Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua, ‚ÄúDiscovering general-purpose active learning strategies,‚Äù arXiv preprint arXiv:1810.04114 , 2018. [18] Arantxa Casanova, Pedro O Pinheiro, Negar Ros-tamzadeh, and Christopher J Pal, ‚ÄúReinforced active learning for image segmentation,‚Äù in International Con-ference on Learning Representations , 2020. [19] Philip Bachman, Alessandro Sordoni, and Adam Trischler, ‚ÄúLearning algorithms for active learning,‚Äù in 

international conference on machine learning , 2017. [20] Sepp Hochreiter and J¬® urgen Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural computation , vol. 9, no. 8, 1997. [21] Jisoo Jeong, Vikas Verma, Minsung Hyun, Juho Kan-nala, and Nojun Kwak, ‚ÄúInterpolation-based semi-supervised learning for object detection,‚Äù in Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 11602‚Äì11611. [22] SS Vallender, ‚ÄúCalculation of the wasserstein distance between probability distributions on the line,‚Äù Theory of Probability & Its Applications , vol. 18, no. 4, 1974. [23] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg, ‚ÄúSsd: Single shot multibox detector,‚Äù in European conference on computer vision . Springer, 2016. [24] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll¬¥ ar, ‚ÄúFocal loss for dense object detection,‚Äù in Proceedings of the IEEE international conference on computer vision , 2017, pp. 2980‚Äì2988. [25] Ozan Sener and Silvio Savarese, ‚ÄúActive learning for convolutional neural networks: A core-set approach,‚Äù in 6th International Conference on Learning Represen-tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings . 2018, OpenReview.net.