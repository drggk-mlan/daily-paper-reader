Title: Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning

URL Source: https://arxiv.org/pdf/2601.15761v1

Published Time: Fri, 23 Jan 2026 01:43:25 GMT

Number of Pages: 9

Markdown Content:
# Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning 

## Xiefeng Wu 1 , Mingyu Hu 1 , Shu Zhang 11Wuhan University wuxiefeng@whu.edu.cn, mingyuhu@whu.edu.cn, 00033521@whu.edu.cn 

## Abstract 

Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and ro-bustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce SigEnt-SAC , an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function os-cillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demon-strate that SigEnt-SAC can learn successful poli-cies with only a small number of real-world interac-tions, suggesting a low-cost and practical pathway for real-world RL deployment. 

## 1 Introduction 

Deep Reinforcement Learning (RL) holds the promise of en-abling generalist robotic agents to perform different tasks in unstructured environments. However, the realization of this potential is severely hindered in real-world settings by expensive data collection [Luo et al. , 2025; Lei et al. ,2025], noisy observations, and an unstable RL learning pro-cess [Nakamoto et al. , 2023; Kumar et al. , 2020]. To ad-dress these challenges, existing methods typically rely on pre-collected datasets to reduce exploration cost and stabi-lize training [Lei et al. , 2025; Nair et al. , 2020a], as well as human-in-the-loop intervention [Luo et al. , 2025] or RL-based fine-tuning of pretrained vision-language-action (VLA) policies [Guo et al. , 2025; Chen et al. , 2025; Intelligence et al. , 2025; Xiao et al. , 2025] to improve task success rates. We identify two primary barriers that prevent current meth-ods from achieving efficient, learning-from-scratch execu-tion: (1) Dependency on Heavy Priors: Offline-to-online ap-proaches typically require massive, high-quality datasets to overcome distribution shifts. Acquiring such datasets for every new task is often prohibitively expensive and labor-intensive. (2) Limited Validation Under Realistic Dynamics and Noise: Existing evaluations are predominantly conducted on relatively stable robotic arm manipulation with multi-view camera setups that increase observation dimensionality and simplify state estimation. As a result, these methods are rarely stress-tested under noisy sensing or high-speed, highly dynamic motions, leaving their RL performance and robust-ness in such challenging regimes largely unverified. Thus, a practical RL framework with low cost in both data collection and real-world deployment remains elusive. To address these challenges, we introduce SigEnt-SAC ,a sample-efficient RL framework capable of learning from scratch without large-scale offline pre-training. It enables rapid convergence in sparse-reward tasks using only a sin-gle expert trajectory. SigEnt-SAC mitigates Q-value oscilla-tions in conservative Q-learning and accelerates exploration through two key designs: (1) Sigmoid-Bounded Entropy ,a novel entropy formulation for the tanh-squashed Gaussian policy that maps per-dimension surprisal through a sigmoid to produce a bounded, numerically stable entropy signal. This design prevents negative-entropy–dominated Q-learning up-dates that can amplify Q-value oscillations and drive the pol-icy to optimize toward out-of-distribution (OOD) actions; and (2) Gated Behavior Cloning (GBC) , which penalizes only those policy actions that deviate substantially from dataset (expert) samples, thereby stabilizing policy optimization un-der Q-value oscillations and preventing unnecessary explo-ration. We extensively evaluate SigEnt-SAC across two distinct settings: D4RL Adroit and Kitchen benchmarks [Fu et al. ,2020], and diverse real-world robotic tasks. Our main contri-butions are summarized as follows: • One-Shot Efficiency: In the one-shot setting, SigEnt-SAC reaches 100% evaluation task success faster than state-of-the-art offline-to-online baselines; compared to conservative methods, it substantially reduces the data 

> arXiv:2601.15761v1 [cs.AI] 22 Jan 2026

acquisition burden for sparse-reward learning and miti-gates OOD action exploration. • Single-Camera Control in Dynamic Environments: SigEnt-SAC supports control from a single egocentric camera stream and still achieves stable policy conver-gence in moving and visually dynamic environments, demonstrating robustness to partial observations and blurred visual inputs. • Cross-Embodiment Generalization: SigEnt-SAC 

generalizes across diverse physical embodiments, in-cluding humanoid robots, quadrupedal robots, and au-tonomous mobile robots, demonstrating broad applica-bility. 

## 2 Related work 

2.1 Offline-to-Online Reinforcement Learning 

Offline-to-Online RL aims to warmstart the learning pro-cess by pre-training policies on static datasets prior to on-line fine-tuning. Prominent methods such as IQL [Kostrikov 

et al. , 2021], CQL [Kumar et al. , 2020], AWAC [Nair et al. , 2020b], TD3+BC [Fujimoto and Gu, 2021], and Cal-QL [Nakamoto et al. , 2023] facilitate this transition by im-posing policy constraints or value regularization to ensure the policy remains within the support of the offline data. Addi-tionally, RLPD [Ball et al. , 2023] alleviates Q-function over-estimation by increasing the number of critics (i.e., a larger Q-ensemble) and introducing LayerNorm in the critic net-works. However, these approaches have not been extensively validated in few-shot settings or in vision-based real-world scenarios. 

2.2 Large Model-Enhanced Reinforcement Learning 

The advent of Large Language Models (LLMs) and Vision-Language Models (VLMs) has spurred interest in leverag-ing semantic priors for RL [Wang et al. , 2023; Yao et al. ,2022; Lin et al. , 2024; Jiang et al. , 2022; Shi et al. , 2024; Liu et al. , 2023; Ouyang et al. , 2024]. Typical approaches include: (1) Reward Synthesis: Using LLMs to automatically code reward functions (e.g., Eureka [Ma and others, 2023], T2R [Xie et al. , 2023]) to bypass manual engineering; (2) Preference Learning: Employing VLMs as preference scor-ers (e.g., RL-VLM-F [Wang et al. , 2024], ERL-RL [Luu et al. , 2025]) to provide feedback based on visual outcomes; and (3) Exploration Guidance: Leveraging LLMs to propose action priors or sub-goals (e.g., LLM-Explorer [Hao et al. ,2025]). However, the reliability of these methods remains a critical concern. LLM-generated rewards often suffer from hallucinations or misalignment, necessitating extensive man-ual verification. Furthermore, VLM-based preference scoring is predominantly validated in simplified environments and of-ten cannot distinguish between similar states in continuous-control tasks. 

2.3 Reinforcement Learning in Real Robot Scenarios 

Real-world RL tackles policy learning directly on physical systems under constraints of noise and sample efficiency. Early works focused on visual servoing [Levine et al. , 2016] or large-scale grasping [Kalashnikov and others, 2018]. Re-cent approaches streamline this process by integrating human guidance: SERL [Luo et al. , 2024] and HIL-SERL [Luo et al. , 2025] leverage interventions and corrections, while RL-100 [Lei et al. , 2025] focuses on iterative self-improvement. Most recently, GR-RL [Li et al. , 2025] combines offline fil-tering with online RL to fine-tune VLA policies for high-precision tasks. However, widespread deployment remains hindered by three constraints. First, heavy dependency on priors: Methods often demand dense interventions or heavy pre-trained VLA backbones (as in GR-RL), limiting usage in resource-constrained settings. Second, limited embodi-ment: Validations are mostly confined to tabletop manipu-lation, lacking cross-embodiment generalization to mobile or legged robots. Finally, complex setups: Reliance on multi-view perception or multi-stage pipelines restricts applicabil-ity in unstructured environments. In contrast, SigEnt-SAC targets a more challenging setting: learning from scratch with minimal guidance, single-view perception, and diverse robot embodiments. 

## 3 Notation 

We represent the environment in the standard form of a Markov Decision Process (MDP): 

M := ⟨S , A, R, P, γ, ρ ⟩.

where S and A denote the state space and action space, re-spectively. Let Z := S ×A . The reward function R : Z → R

maps each state–action pair to a scalar reward; the transition function P : Z → Dist (S) specifies the probability distribu-tion over next states given a state–action pair. The initial state distribution is given by ρ ∈ Dist (S).A stochastic policy π : S → ∆( A) assigns a probability distribution over actions to each state, while a deterministic policy μ : S → A selects a single action in each state. Let Q(s, a ) denote the true Q-function of the MDP. We define ˆQ(s, a ) as the Q-function estimate learned via TD updates augmented with an additional regularization term; 

ˆQsoft (s, a ) denotes the Q-function estimate learned under the soft Bellman operator [Haarnoja et al. , 2018] together with an additional bias/regularization term. Furthermore, let Q∗ denote the optimal Q-function of the local MDP D. We use ˆQ∗ to denote the converged (biased) Q-function induced by the regularized TD updates. 

## 4 Sigmoid Bounded Soft Actor Critic 

The SigEnt-SAC framework is built upon two core mecha-nisms: (1) Sigmoid-Bounded Entropy During training, we map the default (unbounded) entropy signal [Haarnoja et al. ,2018] of tanh-squashed Gaussian policies to a bounded and strictly-positive score by applying a sigmoid to the per-dimension surprisal. This bounded formulation eliminates policy over-optimization driven by negative entropy, im-proves the stability of Q-function updates, and mitigates the performance oscillation commonly observed in conservative Q-learning based methods. Figure 1: Illustration of negative-entropy effects in soft Q-learning. We compute ˆQ(s, a ) assuming a Gaussian policy with σπ = 0 .1,and assume sampled actions lie within 1.5σπ of the mean. Top row: the default entropy term (negative entropy; row 1, col 2) lowers 

ˆQ(s, a ) for a = π(s) and its neighborhood, making max-Q policy improvement more likely to move toward out-of-distribution (OOD) actions. Bottom row: a sigmoid-bounded entropy maps the entropy contribution to a strictly positive, bounded range, yielding a clearer high-Q region and a more well-defined action set for maximization. 

(2) Gated Behavior Cloning During each policy update step, we introduce an expert-based gated behavior cloning loss that is added on top of the policy loss to provide addi-tional gradients on the policy mean action. This shaping sig-nal enables the policy to remain exploring within the neigh-borhood of the expert trajectory even when the Q-function is not yet converged or is oscillating, thereby reducing ineffec-tive exploration and consequently minimizing hardware wear caused by unnecessary interactions. Collectively, these two mechanisms mitigate the oscillation issues of conservative Q-learning based methods and improve sample efficiency, enhancing training stability for real-world deployment without large-scale pretraining. 0 1 2 3 4          

> Steps
> 0.00
> 0.05
> 0.10
> 0.15
> 0.20
> 0.25
> OOD Action Ratio
> 10 5
> hammer-binary-v0
> Cal-QL( H0=1)+LN
> Cal-QL( H0=2)+LN
> SigEnt-SAC( H0=2)
> 01234
> Steps
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 10 5
> pen-binary-v0

Figure 2: Policy optimization with a negative entropy term is more likely to produce OOD actions; correspondingly, a smaller target policy entropy leads to a higher OOD action ratio. The OOD crite-rion is defined in Eq. 3 with the threshold set to 0.3. We addition-ally apply LayerNorm to Cal-QL to mitigate spurious OOD actions caused by network oscillations. 

4.1 Sigmoid-Bounded Entropy Mechanism 

We consider a tanh-squashed Gaussian policy a = tanh( x),where x is sampled via reparameterization from the pol-icy network outputs: x = μθ (s) + σθ (s) ⊙ ϵ, with 

ϵ ∼ N (0 , I ). Let log πθ,i (ai|s) denote the per-dimension 

log-density of the squashed policy (so that log πθ (a|s) =

Pdi=1 log πθ,i (ai|s), where d is the action dimension). We define the per-dimension negative log-density (surprisal) as 

si = − log πθ,i (ai|s). The bounded entropy contribution is computed via a temperature-controlled sigmoid: 

hi(si) = hmax · σ

 si − mt



, Hsig (s, a ) = 

> d

X

> i=1

hi(si),

(1) where σ(·) is the sigmoid function, hmax > 0 controls the maximum per-dimension entropy score, m is a center off-set, and t > 0 is a temperature parameter. By construction, 

Hsig (s, a ) ∈ (0 , d · hmax ).Replacing the default entropy term with Hsig , the Q-function update rule is formulated as: 

ˆQk+1 (s, a ) = (1 − α) ˆQk(s, a )+ α

h

r(s, a ) + γEa′∼π

 ˆQk(s′, a ′) + Hsig (s′, a ′)i

.

(2) As illustrated in Figure 1, under the default formulation 

ˆQ(s, a ) − α log π(a|s), the negative entropy term can dom-inate and shift the maximum-Q region toward the action-space boundary, causing out-of-distribution exploration(see Figure 2). In contrast, ˆQ(s, a ) + Hsig (s, a ) produces a “bowl-shaped” regularized landscape (with a central dip and low values near the boundary), which biases policy improve-ment toward actions within the neighborhood of the state-visit distribution p(s), thereby reducing conservative-Q-learning-induced oscillations and avoiding unnecessary, hardware-costly interactions. 

4.2 Gated Behavior Cloning 

During policy updates, we integrate the learning signals de-rived from agent-collected transitions with the guidance pro-vided by expert demonstrations. Distinct from conventional paradigms that combine policy optimization with Behavior Cloning (BC), we employ a dynamic gating mechanism based on the deviation between the policy mean action and the ex-pert action. Let amean (s) = tanh( μθ (s)) denote the deterministic mean action of the policy, and define the gating mask: 

pgate (s, a exp ) = I[∥amean (s) − aexp ∥2 > ε ] , (3) where ε > 0 is a threshold. We then optimize a unified maximum-entropy policy objective with sigmoid-bounded entropy and a gated expert shaping term: 

J (πθ ) = Es∼D , a ∼πθ (·| s)

h

Qk(s, a ) + α Hsig (s, a )

i

− λ E(s,a exp )∼D exp 

h

pgate (s, a exp ) · ∥ amean (s) − aexp ∥22

i

.

(4) where α is the entropy weight and λ > 0 controls the strength of the expert-based shaping signal. In Equation 4, the third term provides additional gradients on the policy mean action only when the deviation from the expert exceeds the threshold. This gated shaping reduces drift caused by critic oscillations and prevents unnecessary out-of-distribution exploration, while still allowing the policy to adapt beyond the demonstration. 

4.3 Algorithm Implementation 

SigEnt-SAC augments the standard off-policy actor-critic framework with two key mechanisms: Sigmoid-Bounded Entropy and Gated Behavior Cloning . The complete train-ing procedure is outlined in Algorithm 1. 

Algorithm 1 SigEnt-SAC Training Framework  

> 1:

Input: Replay buffer Dbuf , expert buffer Dexp , total training steps N , batch size B 

> 2:

Initialize: Policy parameters ϕ, critic parameters θ1, θ 2,and target parameters θ′

> 1

, θ ′ 

> 2
> 3:

// Phase 1: Expert Demonstration Collection  

> 4:

Collect expert transitions and store in Dexp  

> 5:

// Phase 2: Online Interaction & Training  

> 6:

for k = 1 to N do  

> 7:

// Environment Interaction  

> 8:

Collect transition (s, a, r, s ′) and store in Dbuf  

> 9:

if |D buf | ≥ B then  

> 10:

Sample batch Bbuf ∼ D buf and expert batch Bexp ∼Dexp  

> 11:

// Critic Update (Sigmoid-Bounded Entropy)  

> 12:

Compute Hsig (s′, a ′) for a′ ∼ πϕ(·| s′) using Eq. 1  

> 13:

Update critics θ1, θ 2 using Eq. 2  

> 14:

// Policy Update (Gated Behavior Cloning)  

> 15:

Update policy ϕ by maximizing Eq. 4  

> 16:

Target Update: θ′ 

> i

← τ θ i + (1 − τ )θ′ 

> i

for i = 1 , 2 

> 17:

end if  

> 18:

end for Critic Loss Function. The entropy-augmented Bellman target is defined as: 

y = r + (1 − d)γ



min  

> i=1 ,2

Q′

> i

(s′, a ′) + α Hsig (s′, a ′)



, (5) where a′ ∼ πϕ(·| s′) and Q′ denotes the target networks. The base TD loss is 

L(i)TD = EDbuf 

h Qi(s, a ) − y2i

, i ∈ { 1, 2}. (6) For each state s in the replay batch, we form an OOD action set by sampling multiple actions from the current policy at both s and s′ and concatenating them: 

Aood (s) = {a(j) ∼ πϕ(·| s)}nj=1 ∪ { a′(j) ∼ πϕ(·| s′)}nj=1 .

We then add a simplified CQL-style regularizer [Kumar et al. ,2020]. For each critic Qi:

L(i)CQL = E(s,a )∼D buf 

"

β log X

> ˜a∈{ a}∪A ood (s)

exp 

 1 

> β

Qi(s, ˜a)

!

− Qi(s, a )

#

,

(7) where β > 0 is the log-sum-exp temperature (set to 1 in our implementation). This term discourages spurious high Q-values on sampled OOD actions and helps reduce train-ing oscillations. Note that Qi is still lower-bounded by its Monte-Carlo discounted return that is similar in spirit to Cal-QL [Nakamoto et al. , 2023]. The final critic objective for each critic Qi is: 

LQi = L(i)TD + λood L(i)CQL , i ∈ { 1, 2}, (8) where λood controls the strength of the conservative regular-ization. 

Policy Loss with Gated Behavior Cloning. We optimize the policy by minimizing the negative of the unified objective defined in Eq. 4: 

Lπθ = − J (πθ ). (9) In implementation, the gated behavior cloning term is evaluated on the deterministic mean action amean (s) =tanh( μθ (s)) , and is activated only when the per-sample devi-ation exceeds a threshold, i.e., 1 

> d

∥amean (s) − aexp ∥22 > ε 2bc .

## 5 Experiments 

In this section, we empirically evaluate the SigEnt-SAC framework to answer the following research questions: • Q1 (Sample Efficiency): Can SigEnt-SAC achieve rapid convergence and high performance with extremely limited data (e.g., a single demonstration)? • Q2 (Robustness): Is the method resilient to a subopti-mal or noisy expert demonstration with varying quality? • Q3 (Real-World Adaptability & Generalization): 

Can the framework: (i) scale across diverse robotic mor-phologies; (ii) handle raw, unstable embodied visual observations; (iii) generalize to dynamic environmental variations; and (iv) significantly outperform the sub-optimal expert demonstration? 

• Q4 (Hyperparameter Sensitivity): How sensitive is SigEnt-SAC to the hyperparameters of the Gated Behav-ior Cloning (GBC) term? 

5.1 Experimental Setup 

Baselines. To evaluate the advantages of SigEnt-SAC in the offline-to-online setting, we compare it against several state-of-the-art algorithms: (1) Cal-QL, CQL [Nakamoto et al. ,2023; Kumar et al. , 2020]: conservative-based methods that penalize Q-values for out-of-distribution (OOD) actions to improve robustness in offline learning and stabilize offline-to-online fine-tuning; (2) RLPD [Ball et al. , 2023], an effi-cient online fine-tuning method using a Q-ensemble and lay-ernorm; (3) AWAC [Nair et al. , 2020a], a Q-value-weighted (c) Quadruped:Slalom  (d) Humanoid:Slalom   

> Tasks Overview
> (b) Wheeled:Ball-to-goal
> Policy Observation
> Obstacles random spawn region
> (d)
> Obstacle random spawn region
> Ball random spawn region
> (b)
> Obstacles random spawn region
> (c)
> Randomization Settings for Each Task
> (a) Push-Cube
> Cube random spawn region
> (a)
> (Single-view Grayscale camera)
> (a) (b)
> (c) (d)

Figure 3: Real-world task suite specification. The figure shows: (Left) an overview of the tasks; (Middle) the unified policy input, consisting of a single-view grayscale local observation; (Right) per-task randomization settings: in Push Cube, the cube is initialized at random positions; in Wheel, both the obstacle and the soccer ball are randomly placed, and the obstacle is movable/collidable; in Quadruped and Humanoid, obstacles are randomly placed within a predefined region and are also movable/collidable. 

behavior cloning method that emphasizes actions with higher estimated returns; (4) IQL [Kostrikov et al. , 2021], an implicit Q-learning method that avoids explicit behavior constraints by learning an expectile value function and deriving the pol-icy via advantage-weighted regression; (5) SAC (w/ Prior), standard SAC initialized with expert data in the replay buffer. 

Hardware Setup & Observation Design. We deploy SigEnt-SAC on a diverse set of physical robotic platforms to verify cross-morphology adaptability (refer to Figure 3). The hardware platforms include: an AgileX LIMO Pro and a TurtleBot3 Waffle (for automated ball pickup) for vision-based ball-to-goal scoring; a RealMan RM65-B manipulator for vision-based manipulation; and a Unitree Go2 quadruped as well as a Unitree G1 humanoid for vision-based locomo-tion. All platforms operate under a purely visual, local obser-vation setting. The agents receive single-view grayscale im-age streams from onboard cameras without external global state estimation. 

Task Settings. We evaluate our method across two do-mains of increasing difficulty, both utilizing sparse rewards. 1. D4RL Continuous Control: Complex manipulation tasks (Kitchen, Adroit) with only a single success trajectory pro-vided. 2. Real-World Robotics: We design four tasks target-ing the challenges in Q3: (1) Push-Cube (Manipulator), push-ing a randomly placed cube into a designated goal region; (2) Ball-to-goal (Wheeled): scoring by pushing the ball into a goal, where both the obstacle and the ball are randomly posi-tioned within a specific area and the obstacle can be pushed away; (3) Slalom (Quadruped), weaving around box-shaped obstacles, where the boxes are randomly positioned within a specific area and can be pushed away; and (4) Slalom (Hu-manoid), weaving around box-shaped obstacles with whole-body balance.                      

> METHOD #Q-FUNCTIONS PARAMS (M) TRAINING TIME (MS /UPDATE )AWAC 20.215 0.61 IQL 20.083 1.31 CAL -QL 20.090 1.51 SIG ENT -SAC 20.091 3.13 RLPD 10 0.090 0.78

Table 1: Comparison of training and deployment overhead between SigEnt-SAC and representative baselines. 

5.2 Computational Efficiency & Deployment Overhead 

A critical barrier to real-world RL is the policy inference cost during deployment. As shown in Table 1, we com-pare SigEnt-SAC with representative baselines in terms of the number of Q-functions, policy params, and training time per update. SigEnt-SAC does not increase policy inference time but increases training cost because it introduces the gated pol-icy cloning term to maintain policy stability. 

5.3 Simulation Evaluation (Q1 & Q2) 

SigEnt-SAC Is More Robust to Noisy Demonstrations. 

We study how the quality of one-shot expert demonstrations affects learning performance, aiming to reduce reliance on carefully curated expert data. We construct three degraded demonstration variants: (1) a successful trajectory with 50% of transitions dropped; (2) the same trajectory corrupted by Gaussian noise on actions with σ = 0 .2; and (3) corrupted by Gaussian noise on states with σ = 0 .2. As shown in Fig-ure 6, SigEnt-SAC exhibits stronger robustness than Cal-QL under noisy demonstration settings. In contrast, Cal-QL suf-fers a pronounced performance drop when the demonstration is corrupted by noise, which we attribute primarily to its large target-entropy setting ( H0 = 0 ), which induces higher pol-icy stochasticity, making learning more sensitive to imperfect (noisy) guidance. 

SigEnt-SAC Reaches 100% Success Faster in the One-Shot Setting. To evaluate our proposed method in the one-0 2 4 6 8 10 

> env steps (×10 5)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> success rate

door-expert      

> 0246810
> env steps (×10 5)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

door-human      

> 0246810
> env steps (×10 5)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

hammer-expert      

> 0246810
> env steps (×10 5)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

kitchen-complete      

> 0246810
> env steps (×10 5)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

pen-human 

awac iql cal-ql cql sigent-sac rlpd sac-od Figure 4: Online learning in the one-shot setting, where agents are provided with only one demonstration and must learn to succeed within 1M steps. Compared to other baselines, S IG ENT -SAC achieves a higher success rate and converges faster, allows the agent to explore with low target entropy, and exhibits no performance drop after convergence; in contrast, all other baselines either suffer performance drop after convergence or fail to learn throughout the entire training phase. 

shot setting, we conduct experiments on high-dimensional D4RL tasks and compare against state-of-the-art baselines. Figure 4 shows the learning curves. SigEnt-SAC reaches a 100% success-rate policy faster than all baselines and yields more stable Q-value estimates than conservative methods. 

5.4 Real-World Robotic Evaluation (Q3) 

We evaluate SigEnt-SAC on four physical platforms across four real-world tasks. In all scenarios, only one successful trajectory is used as the offline dataset. 

SigEnt-SAC Is Robust to Embodied Visual Observations. 

SigEnt-SAC consistently converges across all four tasks un-der a purely visual, single-view observation setting, demon-strating reliable offline-to-online learning with minimal su-pervision (one demonstration). As shown in Table 2, we com-pare SigEnt-SAC against two representative baselines: (i) be-havior cloning (BC) implemented with a Vision Transformer policy [Dosovitskiy, 2020] trained on 30 demonstrations, and (ii) a zero-shot VLM agent using ChatGPT-5.2. We find that BC struggles under limited data and lacks a recovery mecha-nism, leading to low success rates across tasks. The zero-shot VLM agent frequently fails to interpret the control command and cannot produce sufficiently accurate actions, resulting in 

TASK BC VLM SIGENT-SAC 

PUSH -C UBE (M ANIPULATOR ) 20% 10% 100% 

BALL -D RIVING (W HEELED ) 10% 10% 100% 

SLALOM (G O2) 0% 0% 100% 

SLALOM (G1) 0% 0% 100% 

Table 2: Real-world success rates (%) over 10 evaluation trials per task. We compare BC, VLM, and SIGENT-SAC. 

near-zero success. In contrast, SigEnt-SAC achieves 100% success on all tasks with only a single demonstration. 

SigEnt-SAC Surpasses the Demonstration. Through on-line data collection and policy improvement, SigEnt-SAC learns faster strategies than the provided trajectory, achieving an average 40.9% reduction in task completion time (Fig-ure 5). 

5.5 Sensitivity Analysis (Q4) 

To assess the robustness of Gated Behavior Cloning , we an-alyze the sensitivity of its key hyperparameters. 

Sensitivity to the GBC Weight λ. We vary the coefficient 

λ in Eq. 4 to control the strength of the expert-based shap-Demonstrated behavior:The agent steadily pushes the ball into the goal using its front end. 

> Learned behavior: The agent drives the ball into the goal with a single lateral sway.

(a) Ball-to-Goal qualitative comparison. 0 10 20 30 40 50 60  

> Steps to completion
> push-cube
> ball-to-goal
> quadruped:slalom
> humanoid:slalom
> Task
> 21
> 26
> 38
> 33
> 18
> 3
> 23
> 26
> Demonstration
> Learned (SigEnt-SAC)

(b) Steps to completion across four tasks. Figure 5: Demonstration and learned policy in the real world. (a) On Ball-to-Goal , the demonstration steadily pushes the ball with the uneven front end, while SigEnt-SAC uses its flatter side and a single lateral sway to drive the ball into the goal more quickly and accurately. (b) Steps required to complete each task for the demonstration and the learned policy. The learned policy reduces steps by an average of 40.9% across four tasks; on ball-to-goal , it achieves an 88.46% reduction (26 → 3 steps). 50% Drop Action Noise ( =0.2) State Noise ( =0.2)     

> hammer-binary-v0
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Success Rate
> SigEnt-SAC
> Cal-QL
> 50% Drop Action Noise ( =0.2) State Noise ( =0.2)
> door-binary-v0
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0

Figure 6: Robustness to demonstration quality degradations. We evaluate learning performance under three one-shot demonstration variants: 50% drop, action noise ( σ = 0 .2), and state noise ( σ =0.2). SigEnt-SAC is consistently robust to noisy demonstrations, while Cal-QL suffers substantial performance degradation, most no-tably on the hammer task. 

ing term (Figure 7). When λ is too small, the expert signal becomes weak and the policy relies primarily on online RL updates; in the one-shot setting this leads to more oscillatory behavior as the agent lacks a stable expert-derived correction. As λ increases to a moderate range, the gated shaping pro-vides helpful corrective gradients when the policy deviates from the expert, improving sample efficiency and accelerat-ing convergence. However, overly large λ can over-regularize the policy toward the demonstration, hindering exploration and adaptation beyond the expert; as a result, convergence can become slower than with smaller (but still effective) λ

values. 

Sensitivity to the Gating Threshold ε. We further analyze the gating threshold ε in Eq. 3, which determines when the expert shaping is activated (Figure 7). With a very small 

ε, the gate is frequently open, making the update resemble unconditional behavior cloning; this can slow convergence because the policy is forced to closely fit potentially subop-timal demonstration trajectories, limiting effective improve-ment beyond the expert. With a very large ε, the gate rarely activates, weakening the expert guidance and reducing the benefits of demonstration in the early stage; consequently, 0 1 2 3 4       

> Environment steps (×10 5)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> task success rate
> =0.1
> =0.3
> =0.8
> 01234
> Environment steps (×10 5)
> =1
> =5
> =10
> Sensitivity on BC Sensitivity on BC

Figure 7: Hyperparameter sensitivity analysis. We analyze the sen-sitivity of the boosting coefficient λ and the gating threshold ϵ. The method exhibits a wide stable region, indicating ease of tuning. 

early exploration becomes more susceptible to Q-function os-cillations, increasing the amount of ineffective exploration. Overall, SigEnt-SAC remains robust across a broad range of 

λ and ε, reducing the burden of hyperparameter tuning when transferring to new real-world tasks. 

## 6 Limitations and Conclusion 

Limitations We acknowledge certain limitations in our cur-rent study. First, our control interface is restricted to high-level velocity commands, and we do not yet demonstrate full-body, joint-level control. Second, our real-world eval-uation scenarios remain limited: we primarily focus on short-horizon, coarse tasks, and have not validated SigEnt-SAC on finer-grained manipulation or long-horizon control problems. In future work, We plan to further validate the effectiveness of SigEnt-SAC in more diverse settings and longer-horizon tasks. 

Conclusion In this paper, we presented SigEnt-SAC, a more stable conservative-based Q-learning method that en-ables policy learning and convergence in real-world settings under highly constrained supervision, using only a single demonstration and a single onboard camera view. Compared to other offline-to-online baselines, SigEnt-SAC finds a 100% success-rate policy faster, and generalizes across diverse em-bodiments: it is effective for both low-speed manipulation tasks and high-frequency locomotion tasks. References 

[Ball et al. , 2023] Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with offline data. In International Conference on Machine Learning , pages 1577–1594. PMLR, 2023. [Chen et al. , 2025] Kang Chen, Zhihao Liu, Tonghe Zhang, Zhen Guo, Si Xu, Hao Lin, Hongzhi Zang, Xiang Li, Quanlu Zhang, Zhaofei Yu, et al. πrl : Online rl fine-tuning for flow-based vision-language-action models. 

arXiv preprint arXiv:2510.25889 , 2025. [Dosovitskiy, 2020] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. 

arXiv preprint arXiv:2010.11929 , 2020. [Fu et al. , 2020] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219 , 2020. [Fujimoto and Gu, 2021] Scott Fujimoto and Shixi-ang Shane Gu. A minimalist approach to offline reinforcement learning. Advances in neural information processing systems , 34:20132–20145, 2021. [Guo et al. , 2025] Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. Improving vision-language-action model with online reinforcement learning. arXiv preprint arXiv:2501.16664 , 2025. [Haarnoja et al. , 2018] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. 

arXiv preprint arXiv:1812.05905 , 2018. [Hao et al. , 2025] Qianyue Hao, Yiwen Song, Qingmin Liao, Jian Yuan, and Yong Li. Llm-explorer: A plug-in reinforcement learning policy exploration enhance-ment driven by large language models. arXiv preprint arXiv:2505.15293 , 2025. [Intelligence et al. , 2025] Physical Intelligence, Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dha-balia, Jared DiCarlo, et al. π∗

> 0.6

: a vla that learns from experience. arXiv preprint arXiv:2511.14759 , 2025. [Jiang et al. , 2022] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. In NeurIPS 2022 Foundation Models for Deci-sion Making Workshop , 2022. [Kalashnikov and others, 2018] Dmitry Kalashnikov et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning (CoRL) , 2018. [Kostrikov et al. , 2021] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with im-plicit q-learning. arXiv preprint arXiv:2110.06169 , 2021. [Kumar et al. , 2020] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in neural infor-mation processing systems , 33:1179–1191, 2020. [Lei et al. , 2025] Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, and Huazhe Xu. Rl-100: Performant robotic ma-nipulation with real-world reinforcement learning. arXiv preprint arXiv:2510.14830 , 2025. [Levine et al. , 2016] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. In Robotics: Science and Systems (RSS) , 2016. [Li et al. , 2025] Yunfei Li, Xiao Ma, Jiafeng Xu, Yu Cui, Zhongren Cui, Zhigang Han, Liqun Huang, Tao Kong, Yuxiao Liu, Hao Niu, et al. Gr-rl: Going dexterous and precise for long-horizon robotic manipulation. arXiv preprint arXiv:2512.01801 , 2025. [Lin et al. , 2024] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Advances in Neural Infor-mation Processing Systems , 36, 2024. [Liu et al. , 2023] Huihan Liu, Alice Chen, Yuke Zhu, Adith Swaminathan, Andrey Kolobov, and Ching-An Cheng. In-teractive robot learning from verbal correction. arXiv preprint arXiv:2310.17555 , 2023. [Luo et al. , 2024] Jianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Berg, Archit Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, and Sergey Levine. Serl: A software suite for sample-efficient robotic reinforce-ment learning. In 2024 IEEE International Conference on Robotics and Automation (ICRA) , pages 16961–16969. IEEE, 2024. [Luo et al. , 2025] Jianlan Luo, Charles Xu, Jeffrey Wu, and Sergey Levine. Precise and dexterous robotic manipula-tion via human-in-the-loop reinforcement learning. Sci-ence Robotics , 10(105):eads5033, 2025. [Luu et al. , 2025] Tung Minh Luu, Younghwan Lee, Donghoon Lee, Sunho Kim, Min Jun Kim, and Chang D Yoo. Enhancing rating-based reinforcement learning to effectively leverage feedback from large vision-language models. arXiv preprint arXiv:2506.12822 , 2025. [Ma and others, 2023] Yuheng Ma et al. Eureka: Human-level reward design via large language models. In 

Advances in Neural Information Processing Systems (NeurIPS) , 2023. [Nair et al. , 2020a] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online re-inforcement learning with offline datasets. arXiv preprint arXiv:2006.09359 , 2020. [Nair et al. , 2020b] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online re-inforcement learning with offline datasets. arXiv preprint arXiv:2006.09359 , 2020. [Nakamoto et al. , 2023] Mitsuhiko Nakamoto, Simon Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. 

Advances in Neural Information Processing Systems ,36:62244–62269, 2023. [Ouyang et al. , 2024] Yutao Ouyang, Jinhan Li, Yunfei Li, Zhongyu Li, Chao Yu, Koushil Sreenath, and Yi Wu. Long-horizon locomotion and manipulation on a quadrupedal robot with large language models. arXiv preprint arXiv:2404.05291 , 2024. [Shi et al. , 2024] Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, and Chelsea Finn. Yell at your robot: Improv-ing on-the-fly from language corrections. arXiv preprint arXiv:2403.12910 , 2024. [Wang et al. , 2023] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended em-bodied agent with large language models. arXiv preprint arXiv:2305.16291 , 2023. [Wang et al. , 2024] Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David Held, and Zackory Er-ickson. Rl-vlm-f: Reinforcement learning from vision language foundation model feedback. arXiv preprint arXiv:2402.03681 , 2024. [Xiao et al. , 2025] Wenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He, Yuqi Xie, Fengyuan Hu, Jimmy Wu, Zhengyi Luo, Linxi Fan, et al. Self-improving vision-language-action models with data generation via residual rl. arXiv preprint arXiv:2511.00091 , 2025. [Xie et al. , 2023] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Reward shaping with lan-guage models for reinforcement learning. arXiv preprint arXiv:2309.11489 , 2023. [Yao et al. , 2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language mod-els. arXiv preprint arXiv:2210.03629 , 2022.