# LLM-in-Sandbox Elicits General Agentic Intelligence
# LLM-in-Sandbox 激发通用智能体智能

**Authors**: Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.16206v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span> <span class="tag-label tag-pink">keyword:ppo</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 9.0
**Evidence**: introduces reinforcement learning for LLM agents aligning with RL and PPO interests and LLM queries

---

## Abstract
We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.

## 摘要
我们引入了 LLM-in-Sandbox，使

---

## 论文详细总结（自动生成）

这篇论文由微软亚洲研究院、中国人民大学和清华大学的研究团队合作完成，探讨了如何通过为大语言模型（LLM）提供一个“代码沙盒”（虚拟计算机环境）来激发其在非代码领域的通用智能。

以下是对该论文的结构化深入总结：

### 1. 论文的核心问题与整体含义（研究动机和背景）
*   **核心问题**：如何进一步解锁 LLM 的潜力，使其从单纯的文本生成器转变为能够解决复杂现实问题的通用智能体？
*   **背景**：目前的智能体框架（如 Claude Code）主要局限于软件工程（SWE）领域。作者认为，计算机是人类最通用的工具，其具备的**外部资源访问、文件管理、代码执行**三大元能力，可以被 LLM 用来解决数学、科学、长文本理解等非代码任务。
*   **整体含义**：论文提出了 **LLM-in-Sandbox** 范式，证明了强大的模型无需额外训练即可自发利用沙盒解决复杂任务，并提出了一种利用通用数据进行强化学习（RL）的方法，以增强弱模型的智能体能力。

### 2. 论文提出的方法论
*   **核心思想**：将 LLM 置于一个轻量级的 Ubuntu Docker 容器中，赋予其终端访问权限，使其通过多轮交互（ReAct 模式）自主探索并解决任务。
*   **关键技术细节**：
    *   **轻量级通用沙盒**：不同于 SWE 智能体需要复杂的预配置环境，该沙盒仅提供基础 Python 环境，鼓励模型自主安装工具（如 `pip install`）。
    *   **三大核心工具**：`execute_bash`（执行命令）、`str_replace_editor`（文件读写编辑）、`submit`（提交结果）。
    *   **LLM-in-Sandbox-RL**：针对弱模型在沙盒中“乱撞”的问题，提出使用**基于上下文的任务（Context-based tasks）**进行强化学习。关键创新在于将背景文档存放在沙盒文件系统中而非 Prompt 中，强制模型学习“搜索-读取-分析”的探索路径。
*   **算法流程**：模型接收任务 -> 思考并调用工具 -> 沙盒返回观测结果 -> 循环交互 -> 将最终答案写入指定文件（如 `answer.txt`）并提交。

### 3. 实验设计
*   **数据集/场景**：涵盖 6 个非代码领域：
    1.  **数学**：AIME25（奥数级题目）。
    2.  **物理**：UGPhysics（大学物理）。
    3.  **化学**：ChemBench。
    4.  **生物医学**：MedXpertQA。
    5.  **长文本理解**：AA-LCR（平均 100K token 的多文档推理）。
    6.  **指令遵循**：IFBench。
    7.  **软件工程**：SWE-bench Verified（用于验证通用训练是否损害代码能力）。
*   **对比方法**：
    *   **Vanilla LLM**：直接生成答案（不使用沙盒）。
    *   **LLM-in-Sandbox**：使用沙盒进行多轮探索。
    *   **LLM-RL**：在纯文本模式下进行强化学习（对比基准）。
*   **评估模型**：包括 Claude-Sonnet-4.5、GPT-5、DeepSeek-V3.2、Kimi-K2、MiniMax-M2、Qwen3 等顶尖模型。

### 4. 资源与算力
*   **硬件环境**：实验使用了单台 **NVIDIA DGX 节点**，配备 **2TB 系统内存**。
*   **推理框架**：使用了 SGLang 和 vLLM 进行本地模型部署。
*   **训练细节**：使用了 GRPO++ 算法。Qwen3-4B 训练了 150 步，Qwen3-30B 训练了 50 步。
*   **算力开销**：文中指出沙盒本身的内存开销极低（单个容器空闲 50MB，峰值 200MB），512 个并发沙盒仅占用系统 5% 的内存。

### 5. 实验数量与充分性
*   **实验规模**：论文在 7 个不同规模和类型的模型上进行了跨 6 个领域的测试，每项任务包含数百个样本（如 AIME 重复 16 次取平均），实验组数非常多。
*   **消融实验**：针对训练数据来源（数学 vs. SWE vs. 通用数据）和上下文存放位置（Prompt vs. Sandbox）做了详细的消融对比。
*   **充分性评价**：实验设计非常全面，不仅验证了性能提升，还深入分析了模型在沙盒中的行为模式（如外部资源调用频率）、推理成本（Token 消耗）和系统效率，具有很高的客观性和说服力。

### 6. 论文的主要结论与发现
*   **性能飞跃**：强模型在沙盒模式下性能显著提升，例如 Qwen3-Coder 在数学上提升了 **24.2%**，Claude 在指令遵循上提升了 **12.7%**。
*   **自发行为**：模型展现出令人惊讶的自主性，例如在化学任务中自发安装 Java 环境并下载第三方库来解析分子结构。
*   **RL 的泛化性**：通过