Title: D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot

URL Source: https://arxiv.org/pdf/2601.15707v1

Published Time: Fri, 23 Jan 2026 01:43:44 GMT

Number of Pages: 21

Markdown Content:
# D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot 

## Qifan Hu a, Branko Celler b, Weidong Mu a,âˆ— and Steven W. Su c,âˆ—

> a

Affiliated Provincial Hospital, Shandong First Medical University, 324 Jing Wu Road, Jinan, 250021, Shandong, China 

> b

Faculty of Engineering, University of New South Wales, NSW 2052, Australia 

> c

Faculty of Engineering and IT, University of Technology Sydney, NSW 2007, Australia 

A R T I C L E I N F O 

Keywords :System calibration Parameter estimation D-optimal experimental design Tri-axial rehabilitation robot Reinforcement learning Proximal Policy Optimization Reward shaping Posture selection Open-loop calibration 

A B S T R A C T 

Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the inputâ€“output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher (>100 Ã—) with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots. 

## 1. Introduction 

Rehabilitation therapy plays a critical role in restoring lower-limb function. Previous studies have demonstrated that repetitive, high-intensity, and task-oriented rehabilitation is essential for promoting neural plasticity and functional recovery [3, 13]. Conventional gait training (CGT) typically relies on therapist assistance: patients who are able to walk use walkers for supported gait training, whereas patients with limited mobility perform pre-walking exercises such as trunk balance training, sitting and standing balance exercises, and isolated lower-limb movements [25]. However, traditional rehabilitation training methods suffer from two major limitations: the lack of feedback on humanâ€“machine interaction and the absence of intelligent data-driven training feedback [26]. Robotic-assisted rehabilitation has emerged as a promising solution to overcome these limitations by automating training processes and providing repetitive, intensive, and task-oriented exercises [16, 18, 20, 15]. In particular, repetitive ankle movements have been shown to effectively improve joint range of motion and neuromuscular coordination. Robotic devices further enable therapists to supervise multiple patients simultaneously and support more complex rehabilitation tasks, thereby enhancing rehabilitation efficiency and promoting neural plasticity [24, 5]. In our preliminary work, we developed and prototyped a compact 3-DOF ankle rehabilitation robot that supports both single-DOF and coupled multi-DOF motions. Beyond mechanical design, effective control remains a central issue in rehabilitation robotics [23, 4]. Existing approaches typically guide the ankleâ€“foot complex along reference trajectories and can be broadly categorised into trajectory-tracking control and assist-as-needed (AAN) strategies [11, 1]. For multi-DOF devices, accurate posture alignment is a prerequisite for both categories, because misalignment directly propagates into tracking errors, inappropriate assistance, and potentially unsafe interaction forces. 

> ORCID

(s): 

1

> arXiv:2601.15707v1 [cs.RO] 22 Jan 2026

For the proposed 3-DOF ankle rehabilitation robot, posture alignment constitutes a multi-input multi-output (MIMO) problem [6]. In practice, discrepancies between commanded motor inputs and the actual end-effector postures are inevitable due to mechanical tolerances, transmission backlash, and assembly errors. Such misalignment degrades control accuracy, compromises rehabilitation performance, and may introduce safety risks for patients. A straightforward approach is to construct a closed-loop system by mounting posture sensors on the end-effector to provide real-time feedback for calibration and control [29]. Common sensing technologies include vision-based systems [10], Global Positioning System (GPS) receivers [14], ultrasonic sensors [21], and inertial measurement units (IMUs) integrating accelerometers, gyroscopes, and magnetometers [30, 17, 12]. Although IMUs can estimate full attitude, their performance is often limited by gyroscope drift and magnetometer sensitivity to electromagnetic interference [9], particularly in indoor environments [22, 8, 28]. GPS-assisted heading correction similarly suffers from reduced accuracy in indoor or signal-degraded scenarios [2, 27]. More importantly for rehabilitation robots, the strict timing constraints of the motionâ€“measurement loop make reliable real-time attitude sensing, synchronisation, and filtering challenging under dynamic conditions. To mitigate the impact of sensing limitations on control accuracy, an open-loop calibration strategy can be adopted. Rather than relying on continuous high-dynamic posture feedback during operation, a one-time calibration experiment is performed using steady-state high-precision sensing to collect paired inputâ€“output posture data. Open-loop calibration exploits steady-state measurements at selected postures, thereby bypassing high-dynamic sensing requirements and improving measurement accuracy and repeatability. An input-output mapping model is identified from the calibration dataset and embedded into the controller, enabling accurate open-loop posture control without real-time feedback during subsequent use [19, 7]. However, calibration accuracy must be balanced against calibration time, experimental burden, and safety. In our system, collecting a candidate set of 50 postures per session is feasible, but utilizing all of them for identification is inefficient. The key challenge is to select a much smaller subset of postures that remains sufficiently informative for accurate and robust estimation of the calibration parameters. In this paper, we propose a Kronecker-product-based open-loop posture calibration method for a custom 3-DOF ankle rehabilitation robot. The resulting calibration model requires the estimation of twelve parameters . The Kronecker-based formulation implies that, under ideal rank conditions, only a small number of distinct postures is theoretically sufficient for identifiability. This motivates a principled experimental design problem: how to minimise the number of calibration postures while preserving maximal information for parameter estimation. To address this, we built a simulation model based on the robotâ€™s SolidWorks design to generate calibration data and support offline optimisation. We then developed a simulation-guided framework that integrates reinforcement learning with a ğ· -optimality-inspired experimental design objective, where the informativeness of a selected posture subset is quantified by the log-determinant of the corresponding information matrix. Specifically, a Proximal Policy Optimisation (PPO) agent is trained to sequentially select a small yet informative subset of posturesâ€”as few as four, consistent with the minimum identifiability requirementâ€”from each candidate set of 50. Extensive simulation studies and practical experiments demonstrate that the learned policy consistently identifies informative posture combinations, significantly reducing calibration effort while maintaining high-quality parameter identification. More broadly, the proposed framework provides a general solution for simulation-guided experimental point selection in multi-DOF robotic systems under strict measurement budgets. The main contributions of this work are as follows: 1. A Kronecker-product-based open-loop posture calibration method for a self-designed 3-DOF ankle rehabilitation robot, enabling estimation of a 12-parameter calibration model from one-time high-precision calibration data without relying on real-time feedback during operation. 2. A discrete posture-subset selection formulation induced by the proposed calibration model, together with a learning-based solver that combines a ğ· -optimality information criterion (log-determinant) with a PPO policy to obtain approximate information-maximising posture sets under strict calibration budgets. 3. Comprehensive simulation and real-world validation on a compact 3-DOF ankle rehabilitation robot, demon-strating that simulation-guided, information-maximising posture selection can substantially reduce calibration effort while maintaining robust parameter estimation. To the best of our knowledge, while reinforcement-learning-based sequential experimental design has been explored in general settings, its use as a practical tool for ğ· -optimality-inspired posture subset selection tailored to 

open-loop calibration of multi-DOF rehabilitation robots has been rarely studied in the literature. 22. Methodology 

In this section, a Kronecker-product-based open-loop posture calibration method is introduced for system parameter identification. Furthermore, to design informative experiments that maximize parameter identifiability and estimation accuracy while minimizing experimental cost and effort, an optimized experimental posture selection method is proposed, which integrates D-optimality-guided experimental design with reinforcement learning. The proposed PPO-based policy optimization framework provides a general, simulation-guided solution for ex-perimental point optimization, enabling efficient and robust experimental design using only system input information, without reliance on explicit analytical model structures. 

2.1. System Modeling and Parameter Identification 

The self-designed 3-DOF ankle rehabilitation robot is modeled as a multiple-input multiple-output (MIMO) system. Based on this model, an open-loop calibration-based parameter identification approach is developed to improve alignment accuracy. Subsequently, the proposed calibration algorithm is implemented for robot alignment. 

2.1.1. MIMO System Formulation 

This system has three inputs: the pitch angle control input ğ‘¢ ğ‘¥ , yaw angle control input ğ‘¢ ğ‘¦ , and roll angle control input ğ‘¢ ğ‘§ , which together form the input state vector ğ‘ˆ = [ ğ‘¢ ğ‘¥ ğ‘¢ ğ‘¦ ğ‘¢ ğ‘§ ]ğ‘‡ . The corresponding output state vector is 

ğ‘Œ = [ ğ‘¦ ğ‘¥ ğ‘¦ ğ‘¦ ğ‘¦ ğ‘§ ]ğ‘‡ . After conducting ğ‘ alignment experiments, we obtained ğ‘ sets of input state vectors and ğ‘ 

corresponding output state vectors: 

ğ‘ˆ 1 = [ ğ‘¢ ğ‘¥ 1 ğ‘¢ ğ‘¦ 1 ğ‘¢ ğ‘§ 1 ]ğ‘‡ , ğ‘ˆ 2 = [ ğ‘¢ ğ‘¥ 2 ğ‘¢ ğ‘¦ 2 ğ‘¢ ğ‘§ 2 ]ğ‘‡ , . . . , ğ‘ˆ ğ‘ = [ ğ‘¢ ğ‘¥ ğ‘ ğ‘¢ ğ‘¦ ğ‘ ğ‘¢ ğ‘§ ğ‘ ]ğ‘‡ ,and 

ğ‘Œ 1 = [ ğ‘¦ ğ‘¥ 1 ğ‘¦ ğ‘¦ 1 ğ‘¦ ğ‘§ 1 ]ğ‘‡ , ğ‘Œ 2 = [ ğ‘¦ ğ‘¥ 2 ğ‘¦ ğ‘¦ 2 ğ‘¦ ğ‘§ 2 ]ğ‘‡ , . . . , ğ‘Œ ğ‘ = [ ğ‘¦ ğ‘¥ ğ‘ ğ‘¦ ğ‘¦ ğ‘ ğ‘¦ ğ‘§ ğ‘ ]ğ‘‡ .The linear relationship between the input posture and the output posture can be expressed as follows: 

ğ‘Œ =

â¡â¢â¢â£

ğ‘ 11 ğ‘ 12 ğ‘ 13 

ğ‘ 21 ğ‘ 22 ğ‘ 23 

ğ‘ 31 ğ‘ 32 ğ‘ 33 

â¤â¥â¥â¦

ğ‘ˆ +

â¡â¢â¢â£

ğ‘ ğ‘¥ 

ğ‘ ğ‘¦ 

ğ‘ ğ‘§ 

â¤â¥â¥â¦

, (1) 

ğ‘Œ = ğ‘‹ ğ´ ğ‘ˆ + ğ‘‹ ğµ , (2) where the parameter matrices are defined as follows: 

ğ‘‹ ğ´ =

â¡â¢â¢â£

ğ‘ 11 ğ‘ 12 ğ‘ 13 

ğ‘ 21 ğ‘ 22 ğ‘ 23 

ğ‘ 31 ğ‘ 32 ğ‘ 33 

â¤â¥â¥â¦

, (3) 

ğ‘‹ ğµ = [ğ‘ ğ‘¥ ğ‘ ğ‘¦ ğ‘ ğ‘§ 

]ğ‘‡ . (4) It should be noted that in Equation (3), the element ğ‘ 11 represents the scaling factor of the pitch angle for the end-effector of a three-axis robot, the element ğ‘ 22 represents the scaling factor of the yaw angle and ğ‘ 33 represents the scaling factor of the roll angle. In Equation (4), the elements ğ‘ ğ‘¥ , ğ‘ ğ‘¦ , and ğ‘ ğ‘§ represent the biases of the three-axis robotâ€™s final pitch, yaw, and roll axes, respectively. 32.1.2. Kronecker-Based Open-Loop Parameter Identification 

Based on the above equations, it is clear that in the proposed algorithm, we need to set 12 calibration parameters. In order to identify the parameters using the well-developed standard Least Square approach, we have to transfer the parameters into a single vector. In this section, we apply the Kronecker product approach for the first item of the right side of Equation (2), using a single vector ğ‘£ğ‘’ğ‘ (ğ‘‹ ğ´ ) = [ ğ‘ 11 ğ‘ 12 ğ‘ 13 ğ‘ 21 ğ‘ 22 ğ‘ 23 ğ‘ 31 ğ‘ 32 ğ‘ 33 ]ğ‘‡ in the following format: 

ğ‘£ğ‘’ğ‘ (ğ‘Œ ) = ( ğ¼ 3 âŠ— ğ‘ˆ ğ‘‡ )ğ‘£ğ‘’ğ‘ (ğ‘‹ ğ´ ) + ğ‘£ğ‘’ğ‘ (ğ‘‹ ğµ ). (5) That is, 

ğ‘Œ =

â¡â¢â¢â£

ğ‘ˆ ğ‘‡ 0 00 ğ‘ˆ ğ‘‡ 00 0 ğ‘ˆ ğ‘‡ 

â¤â¥â¥â¦

ğ‘£ğ‘’ğ‘ (ğ‘‹ ğ´ ) + ğ‘‹ ğµ . (6) Denote ğ‘‹ = [ ğ‘£ğ‘’ğ‘ (ğ‘‹ ğ´ )ğ‘‡ | ğ‘‹ ğ‘‡ ğµ ]ğ‘‡ , we have 

ğ‘Œ =

â¡â¢â¢â£

ğ‘ˆ ğ‘‡ 0 0 | 1 0 00 ğ‘ˆ ğ‘‡ 0 | 0 1 00 0 ğ‘ˆ ğ‘‡ | 0 0 1

â¤â¥â¥â¦

ğ‘‹. (7) When considering the total ğ‘ sets of experimental data, Equation (7) can be augmented in the following form: 

â¡â¢â¢â¢â¢â¢â¢â¢â¢â£

ğ‘¦ ğ‘¥ 1

ğ‘¦ ğ‘¦ 1

ğ‘¦ ğ‘§ 1

â‹®

ğ‘¦ ğ‘¥ ğ‘ 

ğ‘¦ ğ‘¦ ğ‘ 

ğ‘¦ ğ‘§ ğ‘ 

â¤â¥â¥â¥â¥â¥â¥â¥â¥â¦

=

â¡â¢â¢â¢â¢â¢â¢â¢â¢â£

ğ‘ˆ ğ‘‡  

> 1

0 0 1 0 00 ğ‘ˆ ğ‘‡  

> 1

0 0 1 00 0 ğ‘ˆ ğ‘‡  

> 1

0 0 1

â‹® â‹® â‹® â‹® â‹® â‹®

ğ‘ˆ ğ‘‡ ğ‘ 0 0 1 0 00 ğ‘ˆ ğ‘‡ ğ‘ 0 0 1 00 0 ğ‘ˆ ğ‘‡ ğ‘ 0 0 1

â¤â¥â¥â¥â¥â¥â¥â¥â¥â¦[ğ‘ 11 ğ‘ 12 ğ‘ 13 ğ‘ 21 ğ‘ 22 ğ‘ 23 ğ‘ 31 ğ‘ 32 ğ‘ 33 ğ‘ ğ‘¥ ğ‘ ğ‘¦ ğ‘ ğ‘§ 

]ğ‘‡ . (8) The compact form of the above equation can be expressed as follows: Ì„ğ‘Œ = ğ´ğ‘‹ + ğœ–, (9) where Ì„ ğ‘Œ = [ ğ‘¦ ğ‘¥ 1 ğ‘¦ ğ‘¦ 1 ğ‘¦ ğ‘§ 1 â‹¯ ğ‘¦ ğ‘¥ ğ‘ ğ‘¦ ğ‘¦ ğ‘ ğ‘¦ ğ‘§ ğ‘ ]ğ‘‡ .In Equation (9), when considering measurement noise, a vector ğœ– is added, whose elements are white noises with zero mean. According to the classical least-square approach, the parameter estimate is given by: 

ğ‘‹ = ( ğ´ ğ‘‡ ğ´ )âˆ’1 ğ´ ğ‘‡ Ì„ ğ‘Œ . (10) It should be noted that although Equation (3) and (4) indicate that a total of 12 parameters need to be identified, Equation (8), obtained through the Kronecker productâ€“based reformulation, reveals that only four distinct experimental points are theoretically sufficient to identify all parameters. The accuracy and robustness of system parameter identification are strongly dependent on the quality of exper-imental data. Although the proposed open-loop parameter identification method enables effective estimation of the unknown model parameters, the identifiability and estimation accuracy are inherently influenced by the selection of experimental operating points. Inappropriate or poorly distributed experimental points may lead to ill-conditioned regression matrices and degraded identification performance. In addition to estimation accuracy considerations, practical constraints often limit the number of feasible experi-mental points in robotic systems. Excessive data collection may increase mechanical wear, risk safety violations, and prolong experimental time. Consequently, an optimized experimental point selection strategy is required to maximize identification performance using a limited set of experimental data. 4Figure 1: Formulation of a D-Optimality-Guided Posture Selection Problem. 

2.2. Experimental Point Optimization via Design of Experiments 

In this part, a general and effective experimental point optimization strategy is presented. The proposed approach leverages simulation to guide the selection of experimental points in real-world settings and performs optimization based solely on the system inputs, without requiring additional system output feedback during the optimization process. Moreover, the explicit system model is required only during the training stage, but not during the application stage, which enables a general and flexible experimental point optimization framework. In summary, the experimental point optimization problem can be interpreted as a design of experiments (DoE) problem aimed at maximizing parameter identifiability. This design facilitates practical experimental implementation under constrained conditions and improves the accuracy and robustness of subsequent parameter estimation. 

2.2.1. Problem Formulation and Design Objectives 

To reformulate the experimental point selection problem as a DoE problem aimed at improving parameter identifiability, we consider selecting a subset of experimental postures from a finite candidate set. Let îˆ¼ denote the set of all feasible postures. At the ğ‘– -th episode, a candidate posture set îˆ¼(ğ‘– )cand âŠ† îˆ¼ with cardinality 

ğ‘€ is given. From this set, a subset îˆ¼(ğ‘– )opt âŠ† îˆ¼(ğ‘– )cand containing ğ¾ postures is selected, where ğ‘€ and ğ¾ are positive integers satisfying 4 â‰¤ ğ¾ < ğ‘€ .For a given posture subset îˆ¼(ğ‘– )opt , the corresponding information matrix is constructed as 

ğ’ (îˆ¼(ğ‘– )opt 

) = ğ´ âŠ¤(îˆ¼(ğ‘– )opt 

)ğ´ (îˆ¼(ğ‘– )opt 

), (11) The experimental design problem is then formulated under the D-optimality criterion as 

max  

> îˆ¼(ğ‘– )opt âŠ†îˆ¼(ğ‘– )cand

det 

(

ğ’ (îˆ¼(ğ‘– )opt 

))

s.t. |||îˆ¼(ğ‘– )opt 

||| = ğ¾. 

(12) 5From a practical perspective, acquiring a moderate number of experimental postures in a single calibration session helps reduce mechanical wear, ensure operational safety, and limit the overall experimental duration. Accordingly, the number of experimental postures collected in each acquisition is fixed at ğ‘€ = 50 .From a theoretical standpoint, at least four experimental postures are required to identify the twelve unknown calibration parameters. Therefore, the optimization objective is to select a subset of ğ¾ = 4 postures from each candidate set of 50 postures that maximizes the information content of the collected data while maintaining practical feasibility. In summary, under the considered problem setting, the experimental point selection problem aims to identify four informative postures from each candidate set of 50 available postures by maximizing the determinant of the corresponding information matrix. 

2.2.2. D-Optimality-Guided Optimization Criterion 

To quantitatively evaluate the informativeness of experimental points for parameter identification, an optimization criterion guided by the D-optimality principle is adopted. From a parameter estimation perspective, the accuracy of parameter identification is directly related to the conditioning and information content of the regression matrix. Under standard assumptions, this relationship can be characterized by the information matrix, whose inverse provides a lower bound on the parameter covariance. Maximizing the determinant of the information matrix, or equivalently its logarithmic determinant, enhances parameter identifiability and reduces estimation uncertainty, which forms the theoretical basis of the D-optimal design criterion. In this study, the D-optimality criterion is employed as a design objective to guide experimental point selection. Notably, the simulation data are generated by constraining the spatial motion angles of the three rotational axes of the ankle rehabilitation robot within predefined ranges, which can be specified according to different mechanical structures and the required range of ankle motion. This input-driven formulation enables the evaluation of information content during offline training using a structured regression model, while no explicit system model is required during the deployment stage, thereby providing a general and flexible optimization criterion suitable for simulation-guided experimental point optimization. Based on the above criterion, the logarithmic determinant of the information matrix is employed as the optimization objective and subsequently incorporated as the reward signal in a reinforcement learning framework. The detailed reinforcement learning formulation and training procedure are presented in the following subsection. It should be noted that the structured system model is only utilized to compute the reward signal during the offline reinforcement learning training process. Once training is completed, the learned policy directly outputs optimized experimental points based solely on system input constraints, without requiring access to the system model or real-time output feedback during practical deployment. 

2.2.3. Simulation-Guided Reinforcement Learning Framework 

To efficiently optimize experimental points under practical constraints, a simulation-guided reinforcement learning (RL) framework is developed. The key idea is to decouple experimental point optimization from real-time data collec-tion by performing policy learning entirely in a simulation environment, while deploying the optimized experimental points in real-world experiments for parameter identification. This simulation-to-real separation significantly reduces experimental cost and avoids unnecessary hardware wear. Within the proposed reinforcement learning framework, the reward function is constructed based on the information matrix evaluated using a structured system model. Specifically, for a given set of selected experimental points, the corresponding regression matrix is constructed according to the predefined model structure, and the information matrix is subsequently computed. The logarithmic determinant of the information matrix is then used to quantitatively evaluate the overall informativeness of the selected experimental points. To reflect practical experimental constraints and encourage the selection of complementary experimental points, the reward is designed to be sparsely evaluated. In particular, the reinforcement learning agent receives a reward only after every four experimental points are selected, which corresponds to the minimum batch size required to form a full-rank regression matrix under the adopted parameterization, rather than after each individual action. This delayed reward mechanism allows the information contribution of a group of experimental points to be jointly assessed, which is consistent with the batch construction of the regression matrix based on the Kronecker product formulation. Under this formulation, the agent performs a sequence of actions to generate experimental points, while the environment accumulates the corresponding input configurations. Once a predefined batch size is reached, the information matrix is computed based on the accumulated experimental points, and the resulting log-determinant value 6is provided as the reward signal. This design facilitates stable policy learning and encourages consistency between the reinforcement learning objective and the D-optimality criterion employed for experimental point selection. 

2.2.4. Policy Optimization Using Proximal Policy Optimization  

> Figure 2: PPO-based RL Frame Workflow.

The policy is parameterized using an actorâ€“critic architecture, in which a policy network (actor) selects experi-mental points and a value network (critic) estimates the state value to reduce the variance of policy gradient updates. An attention-based policy network is employed to handle the combinatorial nature of experimental point selection: candidate experimental points are individually encoded, while the current optimization contextâ€”including statistics of previously selected points, optimization progress, and matrix-related informationâ€”is processed by a shared context encoder. An attention mechanism then computes selection scores for all feasible experimental points by evaluating their relevance to the encoded context. The policy outputs a categorical distribution over the available experimental points, from which actions are sampled during training, while the value network produces a scalar state-value estimate. Policy updates are performed using Proximal Policy Optimization with a clipped surrogate objective, combined with a mean-squared error loss for value function approximation and an entropy regularization term to encourage exploration. The entire optimization process is conducted in simulation using only system input information, and the trained policy can be directly deployed for real-world experimental point selection without requiring additional output feedback or explicit analytical model structures. 

## 3. Simulation and Analysis 

In this section, two simulation studies are conducted for different purposes. The first simulation is designed to preliminarily verify the effectiveness of the proposed Kronecker-product-based parameter identification model. The second simulation focuses on the training and analysis of the reinforcement-learning-based experimental design method. Specifically, the PPO policy network is trained in a simulation environment using a reward function primarily designed to promote higher information content, as quantified by the determinant of the information matrix. Standard PPO exploration strategies are adopted during training. Subsequently, the trained PPO policy and a random selection baseline are evaluated on newly generated simulation datasets. The determinant of the information matrix is recorded as the primary evaluation metric, and both the mean and standard deviation are computed to assess performance and stability. 

3.1. Kronecker-Based Parameter Identification Validation 

In this part, a simulation environment for the 3-DOF ankle rehabilitation robot is first developed using the SimMechanics toolbox in MATLAB/Simulink, based on the mechanical structure designed in SolidWorks. The 7Table 1 

> Set values

# Pitch Roll Yaw Scaling factor 0.43 0.87 0.71 

# Bias 3.1 2.41 âˆ’5 .8

simulation environment is then employed to set system parameters and use simulation ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 0 (see Table 2) for algorithm validation. 

3.1.1. Simulation Setup and Data collection  

> Figure 3: End-effector attitude change relative to the world coordinate system.

Before conducting simulations, it is essential to establish a world coordinate system and set up a reference coordinate system at the end-effector of the three-axis ankle rehabilitation robot. As shown in Fig. 3, the world coordinate system (ğ‘‹ ğ‘¤ , ğ‘Œ ğ‘¤ , ğ‘ ğ‘¤ ) remains fixed, while the reference coordinate system (ğ¹ ğ‘Ÿğ‘ğ‘šğ‘’ 1) initially coincides with the world coordinate system at the base position (ğµ ) and changes synchronously with the orientation of the end-effector. The relative attitude change of the reference coordinate system (ğ¹ ğ‘Ÿğ‘ğ‘šğ‘’ 1) with respect to the base coordinate system (ğµ ) accurately reflects the motion of the end-effector from its initial position. For validation purposes, artificial scaling factors and bias parameters are imposed on each system axis in the simulation environment. The detailed parameter settings are provided in Table 1. Data collection is conducted in the simulation environment under open-loop control. Provide the system with three independent random input signals: one for controlling the pitch angle, one for controlling the roll angle, and one for controlling the yaw angle. Each signal should vary randomly over time to test the systemâ€™s response to dynamic, unpredictable inputs in all three rotational axes. The robot model is driven through a sequence of predefined postures, and the system input signals ğ‘ˆ together with the corresponding output responses ğ‘Œ are sampled at a fixed frequency of 1 Hz. The simulation duration for each group is 50 s, resulting in a total of 200 simulation groups, which is shown in Table 2 . The resulting ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 0 are used for subsequent parameter identification and performance evaluation. 

3.1.2. Simulation Results and Analysis 

Figure 4 illustrates the boxplot comparison of inputâ€“output errors for each axis before and after calibration based on the simulation ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 0 collected before. Prior to calibration, significant systematic biases and large error dispersion can be observed, particularly along the yaw axis. After applying the proposed parameter identification algorithm according to Equation (10), the median errors of all axes are driven close to zero, and the interquartile ranges are substantially reduced. These results indicate that the proposed method effectively compensates for axis-wise scaling and bias errors, leading to improved accuracy and robustness. 8Figure 4: Distribution of input-output errors before and after open-loop calibration in simulation experiments. 

3.2. Experimental Point Optimization validation 

This part presents the training and evaluation of a PPO-based experimental point optimization model and demonstrates the practical performance of the proposed experimental design approach on both simulated and real-time data. Specifically, this study proposes a RLâ€“based posture selection method that incorporates the D-optimality criterion into a PPO framework to reduce the complexity of the DoE procedure. The experimental evaluation aims to systematically assess the optimization performance of the proposed method in comparison with a random selection baseline, examine its generalization capability and stability in a simulated environments. To ensure fairness and reliability of the experimental evaluation, the training and testing phases were performed on separate datasets, and the random-selection baseline was executed multiple times, with its results averaged to obtain stable performance estimate. In addition, all strategies performed the same number of pose selections ( 4 out of 50 

postures) to ensure a consistent evaluation setting.  

> Figure 5: Experimental Procedure.

3.2.1. Simulation Setup 

All experiments were conducted using Python 3.8 and PyTorch framework, which served as the platform for implementing and training the PPO-based reinforcement learning network. 9Table 2 

> Datasets

## No. Dataset Type Episode (N) Dim Collect-num Select-num 

## 0 Simulation 200 6 50 Ã— ğ‘ None 1 Simulation 200000 3 50 Ã— ğ‘ 4 Ã— ğ‘ 

## 2 Simulation 1000 3 50 Ã— ğ‘ 4 Ã— ğ‘ 

## 3 Simulation 100 6 50 Ã— ğ‘ 4 Ã— ğ‘ 

## 4 Real-time 100 6 50 Ã— ğ‘ 4 Ã— ğ‘ 

## 5 Real-time 1 6 50 4

3.2.2. Simulation Datasets 

Simulation datasets are generated within fixed angular ranges for each axis to ensure that all inputs remain within the admissible input space. All input variables are normalized to the range [0 , 1] to eliminate scale differences across dimensions. Input sequences are randomly sampled, and the corresponding system outputs are obtained by simulating the system structure described in Equation (8), with additive Gaussian noise introduced to emulate measurement uncertainty. Both the input and output spaces are three-dimensional. Notably, the simulated system outputs are used exclusively during the evaluation phase and are not accessed during the policy training process. As summarized in Table (2). For policy learning, ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 1 is used to train the PPO agent, while the ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 2 is reserved for evaluating parameter identification performance. This split is adopted to ensure sufficient data diversity for policy learning while maintaining an unbiased evaluation set. Furthermore, ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 3 is used as an independent dataset for evaluating the performance of methods. 

3.2.3. Baseline and Metrics 

PPO (Proposed Method) The proposed PPO-based method employs a reinforcement learning model to au-tonomously select an informative subset of postures based on the current state vector. 

Random Strategy: In each episode, 4 postures are randomly selected from the 50 available postures. The process is repeated across multiple episodes, and the resulting performance metrics are averaged to establish a baseline. 

Determinant of Information Matrix According to Equation (8), each episode select ğ¾ from 50 input postures constitute matrix A, the determinant of the selected input posture matrix reflects the volume of the corresponding subspace, which serves as an indirect measure of the diversity of the selected posture subset. Under different strategies, a larger determinant indicates a richer and more geometrically well-distributed set of postures, providing better coverage of the input space and improving the parameter estimation quality in the system identification task. 

Reward (PPO only) For the proposed PPO-based method, the reward is used to assess the convergence quality and overall effectiveness of the RL policy. Higher rewards indicate that the agent has successfully learned to select postures that maximize information gain or other task-specific objectives. It is worth noting that rewards function is designed based on the determinant of the information matrix of each episode. The reward, which depends on det( ğ’ ) and is computed once after all ğ¾ postures have been selected. 

Variance and Stability Indicators The standard deviation of repeated experiments is used to evaluate the stability and robustness of each algorithm. By running multiple independent trials, both the mean and variance of the determinant and reward metrics are calculated. A lower variance indicates more consistent performance across different runs and ensures the reliability of the proposed posture selection method under varying conditions. 

3.2.4. PPO Training and Policy Learning 

Following the simulation-guided reinforcement learning framework introduced in Section X, Proximal Policy Optimization (PPO) is instantiated to learn an experimental point selection policy under a discrete action space and sparse terminal rewards. 10 The state representation is a 210-dimensional vector that aggregates multiple sources of information relevant to experimental point optimization. Specifically, it consists of pose-related features (150 dimensions), a binary availability indicator for candidate poses (50 dimensions), statistical summaries of observed system responses (6 dimensions), a scalar progress indicator encoding the current step within an episode (1 dimension), and auxiliary matrix-derived features (3 dimensions). This state formulation enables the policy to jointly reason about geometric coverage, feasibility constraints, and accumulated identification information. The action space is discrete, defined over a finite set of 50 candidate poses. At each decision step, the policy selects a single pose from the currently available candidates. An episode corresponds to the sequential selection of four poses, after which the episode terminates automatically.  

> Figure 6: Architecture of PPO-based RL Framework for 4-Step Sequential Pose Selection.

A sparse reward scheme is adopted to align policy learning with the objective of experimental informativeness. No intermediate rewards are provided during pose selection. Upon episode termination, a scalar reward proportional to the log-determinant of the resulting information matrix is assigned. To stabilize policy optimization under sparse terminal rewards, scaled by a factor of 1/10 to stabilize policy updates. Under this formulation, the observed episode-level rewards typically fall within the range of 3.0 to 4.2. The policy and value functions are parameterized by a neural network architecture with a shared encoder and separate actor and critic heads. Approximately 71% of the network parameters are shared, promoting feature reuse between policy evaluation and value estimation. The hidden representation dimension is set to 768, resulting in a total parameter count of approximately 3.5 million. The actor outputs a categorical action distribution via a softmax layer, from which actions are sampled during training. Training is conducted for a fixed budget of 200,000 episodes, corresponding to 800,000 decision steps in total. Policy updates are performed every 16 episodes, with each update consisting of 10 optimization epochs over the collected rollout data. A fixed training horizon is used to ensure consistency and fair comparison across different baselines under sparse-reward conditions. 

3.2.5. Simulation Results Analysis 

Convergence and Learning Behavior Figure 7 illustrates the convergence and learning behavior of the proposed PPO-based policy. As shown in Fig. 7a, the episode reward exhibits a clear upward trend with decreasing variance as training progresses. The smoothed learning curve in Fig. 7c further indicates that the policy gradually converges to a stable performance plateau. To assess stability in the converged regime, Fig. 7b reports the reward distribution over the final 1000 episodes, which demonstrates a concentrated distribution with low variance, confirming stable policy behavior. 11 Table 3 

PPO Training Configuration Summary 

Module Item Specification 

State Space Dimension 210 Composition Poses (150) + Availability (50) + Statistics (6) + Progress (1) + Matrix features (3) Action Space Type Discrete Action set {0 , â€¦ , 49} 

Semantics Select one available pose per step Reward Design Reward type Sparse Intermediate reward 0Terminal reward logdetâˆ•10 

Reward scale [3 .0, 4.2] 

Episode Definition Episode length 4 steps Episode meaning Sequential selection of 4 poses Termination condition len(selected) = 4 

Policy Network Architecture Shared encoder + separate actor/critic heads Parameter sharing âˆ¼71% Hidden dimension 768 Total parameters âˆ¼3.5M Output Parameterization Action distribution Categorical Implementation Softmax + sampling Training Protocol Training budget 200k episodes Total decisions 800k steps Update frequency Every 16 episodes Optimization epochs 10 per update Convergence Criterion Automatic convergence Not used Training termination Fixed 200k episodes 

Figure 7: a .Training reward and moving average over episodes (left), b.Reward distribution over the final 1000 episodes (middle), c.Smoothed learning curve of average episode reward (right). 

12 Performance Comparison with Baselines Figure 8 compares the proposed PPO-based method with the baseline strategy, a random selection, in terms of the determinant of the information matrix. As shown in Fig. 8a, the PPO policy consistently achieves higher det( ğ’ ) values with reduced variance, indicating both superior performance and improved stability. The mean det( ğ’ ) values reported in Fig. 8b further demonstrate that PPO significantly outperforms the random strategy by more than two orders of magnitude. To provide a distribution-level comparison, Fig. 8c illustrates the log-scaled det( ğ’ ) distributions over multiple trials. The PPO distribution is clearly right-shifted and more concentrated, confirming that the observed performance gains are consistent rather than arising from occasional high-reward samples.         

> Figure 8: a .Compare det( ğ’ )with baseline, b.Compare mean det( ğ’ )with baseline, c.Compare det( ğ’ )distribution with baseline in log scale.

Generalization under Independent Simulation Runs To further assess the robustness of the learned policy, we evaluate PPO on an independent simulation dataset ( ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 3, see Table 2) generated from the same underlying distribution as the training environment . Importantly, ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 3 is only used for evaluation and does not contribute to policy updates. As shown in Fig. 9, PPO maintains a consistently high det( ğ’ ) value across episodes, significantly outperforming the random baseline in terms of both average performance and stability. The best det( ğ’ ) found by PPO rapidly approaches the upper range of the distribution, whereas the random strategy exhibits slow improvement and high variance. These results indicate that the learned policy generalizes well beyond the specific simulation trajectories encountered during training. 

Parameter Estimation Stability and Output Diversity Analysis Using ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 3, we evaluate the cross-episode stability of parameter estimation ( ğ— ) with different point selection strategies. The variance of the estimated parameters across episodes directly reflects the stability and consistency of the identification results. Figure 10 shows that PPO achieves the lowest cross-episode parameter variance among all compared methods, demonstrating its ability to select informative experimental points that lead to robust parameter estimation. 

## 4. Experimental Validation on the Physical Robot 

In this section, real-world experiments are divided into two stages. The first part validates the effectiveness of the Kronecker-product-based parameter identification method in a real-world environment. The second part analyzes the performance of the proposed PPO-based strategy when applied to experimental point selection. 

4.1. Real-World Experimental Setup 

Hardware platform We utilize a 3-DOF ankle rehabilitation device (Fig. 11), controlled by three motors that facilitate both independent and coupled movements. 13 Figure 9: Independent Simulation Validation (performance comparison on an ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 3 generated from the same underlying distribution as the training environment, the evaluated data were not used during PPO training). 

Figure 10: Cross-episode variance of the estimated parameters evaluated on the ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 3 under different experimental point selection strategies. 

Sensors and sampling A smartphone which is equipped with an IMU solution application is mounted on the end-effector, providing real-time attitude feedback for pitch, roll, and yaw axes. The IMU solution application communicates with the computer via the TCP/IP protocol, while the deviceâ€™s motors communicate using the 485/Modbus-RTU protocol (Fig. 11). 14 Input Random Signal    

> Output poses Collection Figure 11: Experimental Platform.

4.2. Real-time Data collection 

When the IMU mounted on the end-effector measures the attitude as ( 0.00 , 0.00 , 0.00 ), this is defined as the initial posture of the ankle rehabilitation device and serves as the zero reference point for all motor movements. From this initial posture, random values within the pulse count range for the ankleâ€™s motion are generated. The motors then guide the rehabilitation deviceâ€™s end-effector to achieve the specified posture, recording the motor pulse counts ğ‘‚ (ğ‘œ ğ‘¥ ,ğ‘œ ğ‘¦ ,ğ‘œ ğ‘§ ) for pitch, yaw, and roll, along with the corresponding IMU feedback posture angles ğ‘Œ (ğ‘¦ ğ‘¥ , ğ‘¦ ğ‘¦ , ğ‘¦ ğ‘§ ). To ensure the accuracy of the collected postures, data acquisition is performed only after the device has fully settled at each new posture. This stabilization requirement significantly increases the time cost of the experimental procedure. The relationship between motor pulse count and rotation angle is: 

ğœƒ = 360 ğ‘ ğœ” ğœ… . (13) In Equation (13), the parameter ğ‘ represents the motor pulse count, where the sign of ğ‘ indicates the direction of motor rotation. The parameter ğœ” stands for pulses per revolution, and ğœ… is the reduction ratio. Based on Equation (13), the pulse counts ğ¼ (ğ‘– ğ‘¥ ,ğ‘– ğ‘¦ ,ğ‘– ğ‘§ ) are converted to the posture angles ğ‘ˆ (ğ‘¢ ğ‘¥ ,ğ‘¢ ğ‘¦ ,ğ‘¢ ğ‘§ ). According to Table (2), real-time totally contains 550 pairs of system input postures ğ‘ˆ (ğ‘¢ ğ‘¥ ,ğ‘¢ ğ‘¦ ,ğ‘¢ ğ‘§ ) and the corresponding system output postures ğ‘Œ (ğ‘¦ ğ‘¥ ,ğ‘¦ ğ‘¦ ,ğ‘¦ ğ‘§ ). 

4.3. Real-World Experiment Results and Analysis 

Four real-world experiments are conducted in this part to comprehensively evaluate the proposed methods. First, the effectiveness of the Kronecker-product-based parameter identification method is validated under real experimental conditions. Second, the PPO model with frozen weights is tested on an independent real-world dataset, and its performance is compared with a random selection baseline to assess the effectiveness of the proposed experimental point selection strategy. Third, using the same dataset, the system parameter vector ğ‘‹ , consisting of 12 unknown parameters, is estimated to evaluate identification accuracy. Finally, the variance of the output posture vector ğ‘Œ , which includes three dimensions, is analyzed to assess estimation stability. All experiments are conducted using real-time experimental data, thereby validating the practical applicability and effectiveness of the proposed framework in real-world operational settings. 

4.3.1. Real-World Experimental Evaluation of the Kronecker-Product-Based Calibration Method 

Figure 12 presents the statistical distributions of the posture errors before and after calibration obtained from the real-world experimental dataset. Specifically, (ğ‘, ğ‘Ÿ, ğ‘¦ ) denote the posture discrepancies between the motor input and the end-effector output before compensation, while (ğ‘ ğ‘ , ğ‘Ÿ ğ‘ , ğ‘¦ ğ‘ ) represent the corresponding posture discrepancies after applying the proposed calibration method. As shown in the Fig. 12, the uncompensated posture errors exhibit large dispersion, particularly along the yaw axis, indicating significant misalignment and coupling effects in the raw system. After compensation, the posture 15 Figure 12: Distribution of input-output errors before and after open-loop calibration in real-world experiments. 

errors in all three axes are substantially reduced and tightly clustered around zero, demonstrating that the proposed Kronecker-product-based open-loop calibration effectively corrects the inputâ€“output misalignment. Moreover, the error reduction trends observed in the real-world experiments are consistent with those obtained in the simulation results (Fig. 4) presented in Section 3.1.2, confirming the robustness and practical applicability of the proposed calibration method under real operating conditions. 

4.3.2. RL-based DoE Performance compared with Baseline on Real-time Dataset  

> Figure 13: Test on independent real-time dataset.

In section 3.2.5, the simulation results shows that the PPO strategy compared with a random baseline at experimental points selection has a better performance. Furthermore, in order to exam different strategiesâ€™ performance on the real-time dataset, we use an independent real-time dataset content 100 episode ( ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 4, shown in Table 2). 16 Figure 13 presents the comparison of det( ğ’ ) achieved by PPO, and Random strategies. The determinant of the state-related matrix ğ’ is used as an indicator of the informativeness of the selected experimental configurations. PPO achieves the highest mean det( ğ’ ), with values consistently concentrated in the upper range, indicating that the selected experimental points provide highly informative system observations. It is worth noting that under fixed PPO configs and dataset, the random strategy still exhibits significantly lower median values and a wide dispersion with sporadic outliers, suggesting unstable and inconsistent information acquisition. 

4.3.3. Efficiency and Robustness of RL-based DoE 

According to Fig. 14, the performance of different methods in maximizing det( ğ’ ) is evaluated on the same real-time dataset ( ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 4). PPO maintains consistently high det( ğ’ ) values across episodes, significantly outperforming the random baseline in terms of both average performance and stability. The maximum det( ğ’ ) achieved by PPO rapidly approaches the upper range of the observed distribution, whereas the random strategy exhibits slower improvement and substantially higher variance. These results demonstrate the robustness and effectiveness of PPO for experimental point selection on real-time data.  

> Figure 14: Real-world verification.

Figure 15 evaluates the variance of the estimated system parameters across episodes on ğ·ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ 4 (shown in Table 2), which directly reflects the stability and consistency of parameter identification results. A total of 12 parameters are analyzed, and significant differences in estimation stability are observed among the two strategies. PPO consistently achieves the lowest parameter variance across most parameters, indicating robust and repeatable identification results, whereas the Random strategy results in extremely large parameter variance, spanning several orders of magnitude for certain parameters. These results demonstrate that random experimental selection fails to provide reliable information for stable parameter estimation. 

4.3.4. Parameter Identification Performance with and without RL-based DoE 

A clear performance difference can be observed by comparing Fig. 16 and Fig. 17 corresponding to the parameter vectors ğ— ğŸ and ğ— âˆ—, respectively. ğ— ğŸ is estimated from four informative postures selected by the proposed PPO-based 17 Figure 15: Identify parameter vector X variance across 100 episodes from real-time datasets. 

method from an earlier episode of the real-world experimental dataset, whereas ğ— âˆ— is estimated from a posture combination that maximizes det( ğ’ ) based on a later episode. Figure 16 illustrates the predicted outputs obtained using ğ— ğŸ (predict eight points from each episode), when applied across episodes, noticeable discrepancies between the predicted outputs ğ˜ ğğ¬ğ­ and the true outputs ğ˜ can be observed, particularly in the Yaw dimension, the predicted signal exhibits a pronounced systematic bias, indicating limited generalization capability of ğ— ğŸ when evaluated beyond the episode from which it was identified. In contrast, Fig. 17 presents the prediction results obtained using ğ— âˆ—, although this parameter vector is identified using only four carefully selected postures within a single episode, the resulting predictions show a substantially higher level of agreement with the true outputs across all three output dimensions. The predicted curves closely follow the true signals in both trend and magnitude, demonstrating improved consistency and reduced estimation error across episodes. This comparison clearly indicates that the parameter vector ğ— âˆ—, derived from a later episode, achieves superior prediction accuracy and stronger cross-episode generalization than ğ— ğŸ , which is estimated from an earlier episode, when evaluated on the same real-time dataset. These results highlight the effectiveness of maximizing det( ğ’ ) as a criterion for experimental point selection, as well as the necessity of periodic calibration, enabling reliable parameter identification with fewer measurements. 

## 5. Conclusion 

This paper first proposed a Kronecker-product-based open-loop calibration method that formulates the input-output alignment as a linear parameter identification task. It then investigates the problem of experimental point selection guided by information maximization for system parameter identification under limited experimental conditions. Based on the established Kronecker-product-based open-loop parameter identification formulation, a simulation-guided reinforcement learning framework using Proximal Policy Optimization (PPO) is proposed to automatically select informative postures that maximize the information content of the collected data, quantified by the determinant of the information matrix, det( ğ’ ).Extensive simulation and real-world experiments demonstrate that the proposed experimental selection method consistently outperforms random baseline strategies. The PPO-based policy successfully identifies posture combi-nations that yield significantly higher det( ğ’ ) values, leading to more robust and stable parameter estimation across episodes. In particular, the learned policy achieves low cross-episode variance in the estimated parameters while avoiding excessive or unstructured variability in the identification results, indicating an effective balance between exploration and estimation stability. Furthermore, experimental results on independent real-world datasets demonstrate that parameter vectors estimated from PPO-selected posture combinations exhibit strong generalization capability beyond the specific episodes used for identification. Despite relying on only a limited number of selected postures, the resulting parameter estimates 18 Figure 16: The estimated X from the random combination of previous real datasets is applied to estimate eight points of Y across other different episodes of real datasets. 

yield accurate output predictions across different episodes and datasets, outperforming estimates obtained from larger but unstructured measurement sets. These findings confirm that maximizing det( ğ’ ) is a meaningful and effective criterion for experimental design, and that reinforcement learning provides a powerful mechanism for discovering highly informative experimental configurations. In summary, this study presents a practical and general framework for improving system parameter identification through learning-based experimental point optimization. By integrating a Kronecker-based open-loop identification method with a D-optimality-guided reinforcement learning strategy, the proposed approach enhances parameter observability while substantially reducing experimental burden. Importantly, the optimization process relies solely on system input information and does not require explicit analytical system models, making it particularly suitable for real-world systems with limited sensing capabilities. Overall, the proposed framework provides a general and scalable solution for experimental design and parameter identification in complex robotic and mechatronic systems. 19 Figure 17: The estimated X from the best deterministic combination of real datasets is applied to estimate eight points of Y across other different episodes. 

## References 

[1] Abarca, V.E., Elias, D.A., 2023. A review of parallel robots: Rehabilitation, assistance, and humanoid applications for neck, shoulder, wrist, hip, and ankle joints. Robotics 12, 131. [2] Chang, Y., Cheng, Y., Manzoor, U., Murray, J., 2023. A review of uav autonomous navigation in gps-denied environments. Robotics and Autonomous Systems , 104533. [3] Cramer, S.C., Sur, M., Dobkin, B.H., Oâ€™Brien, C., Sanger, T.D., Trojanowski, J.Q., Rumsey, J.M., Hicks, R., Cameron, J., Chen, D., et al., 2011. Harnessing neuroplasticity for clinical applications. Brain 134, 1591â€“1609. [4] Diaz, M.A., Voss, M., Dillen, A., Tassignon, B., Flynn, L., Geeroms, J., Meeusen, R., Verstraten, T., BabiÄ, J., Beckerle, P., et al., 2022. Human-in-the-loop optimization of wearable robotic devices to improve humanâ€“robot interaction: A systematic review. IEEE Transactions on Cybernetics 53, 7483â€“7496. [5] Diego, P., Herrero, S., Macho, E., Corral, J., Diez, M., Campa, F.J., Pinto, C., 2024. Devices for gait and balance rehabilitation: General classification and a narrative review of end effector-based manipulators. Applied Sciences 14, 4147. [6] Gao, Z.F., Jiang, B., Shi, P., Cheng, Y.H., 2010. Sensor fault estimation and compensation for microsatellite attitude control systems. International Journal of Control, Automation and Systems 8, 228â€“237. [7] Gheorghe, M., Neal, J., 2023. Disentangling triaxial sensor nonorthogonalities and installation errors. IEEE Sensors Letters . [8] Guo, P., Qiu, H., Yang, Y., Ren, Z., 2008. The soft iron and hard iron calibration method using extended kalman filter for attitude and heading reference system, in: 2008 IEEE/ION Position, Location and Navigation Symposium, IEEE. pp. 1167â€“1174. 

20 [9] Han, J., Hou, Z.W., Zhang, T., Wu, S.X., Xu, D.Y., 2025a. Pure imu localization for intelligent platforms with cnn adaptive invariant extended kalman filter noise fusion. Journal of Vibroengineering 27, 709â€“726. URL: https://doi.org/10.21595/jve.2025.24765 ,doi: 10.21595/jve.2025.24765 .[10] Han, X., Guffanti, D., Brunete, A., 2025b. A comprehensive review of vision-based sensor systems for human gait analysis. Sensors 25. URL: 

https://www.mdpi.com/1424-8220/25/2/498 , doi: 10.3390/s25020498 .[11] Hussain, S., Jamwal, P.K., Vliet, P.V., Brown, N.A., 2021. Robot assisted ankle neuro-rehabilitation: state of the art and future challenges. Expert Review of Neurotherapeutics 21, 111â€“121. [12] Khor, C.W., Ahmad, N.S., 2025. Ble-based indoor localization with temporal convolutional network. Journal of Engineering Research URL: 

https://www.sciencedirect.com/science/article/pii/S2307187725000549 , doi: https://doi.org/10.1016/j.jer.2025. 05.003 .[13] Kim, S.J., Swanson, V.A., Collier, G.H., Rabinowitz, A.R., Zondervan, D.K., Reinkensmeyer, D.J., 2024. Using large-scale sensor data to test factors predictive of perseverance in home movement rehabilitation: Early exercise frequency and schedule consistency. IEEE Transactions on Neural Systems and Rehabilitation Engineering . [14] Kumar, S., Moore, K.B., 2002. The evolution of global positioning system (gps) technology. Journal of science Education and Technology 11, 59â€“80. [15] Laszlo, C., Munari, D., Maggioni, S., Knechtle, D., Wolf, P., De Bon, D., 2023. Feasibility of an intelligent algorithm based on an assist-as-needed controller for a robot-aided gait trainer (lokomat) in neurological disorders: a longitudinal pilot study. Brain Sciences 13, 612. [16] Morone, G., Paolucci, S., Cherubini, A., De Angelis, D., Venturiero, V., Coiro, P., Iosa, M., 2017. Robot-assisted gait training for stroke patients: current state of the art and perspectives of robotics. Neuropsychiatric disease and treatment , 1303â€“1311. [17] Nazarahari, M., Rouhani, H., 2021. 40 years of sensor fusion for orientation tracking via magnetic and inertial measurement units: Methods, lessons learned, and future challenges. Information Fusion 68, 67â€“84. URL: https://www.sciencedirect.com/science/article/ pii/S1566253520303997 , doi: https://doi.org/10.1016/j.inffus.2020.10.018 .[18] Nizamis, K., Athanasiou, A., Almpani, S., Dimitrousis, C., Astaras, A., 2021. Converging robotic technologies in targeted neural rehabilitation: a review of emerging solutions and challenges. Sensors 21, 2084. [19] Papafotis, K., Sotiriadis, P.P., 2020. Multiple accelerometers and magnetometers joint calibration and alignment. IEEE Sensors Letters 4, 1â€“4. [20] Poli, P., Morone, G., Rosati, G., Masiero, S., 2013. Robotic technologies and rehabilitation: new tools for stroke patientsâ€™ therapy. BioMed research international 2013, 153872. [21] Qiu, Z., Lu, Y., Qiu, Z., 2022. Review of ultrasonic ranging methods and their current challenges. Micromachines 13, 520. [22] Renaudin, V., Afzal, M.H., Lachapelle, G., 2010. Complete triaxis magnetometer calibration in the magnetic domain. Journal of sensors 2010, 967245. [23] RodrÃ­guez-FernÃ¡ndez, A., Lobo-Prat, J., Font-Llagunes, J.M., 2021. Systematic review on wearable lower-limb exoskeletons for gait training in neuromuscular impairments. Journal of neuroengineering and rehabilitation 18, 22. [24] Stanton, R., Ada, L., Dean, C.M., Preston, E., 2011. Biofeedback improves activities of the lower limb after stroke: a systematic review. Journal of physiotherapy 57, 145â€“155. [25] Talaty, M., Esquenazi, A., 2023. Feasibility and outcomes of supplemental gait training by robotic and conventional means in acute stroke rehabilitation. Journal of NeuroEngineering and Rehabilitation 20, 134. [26] Wang, D., Li, J., Jian, Z., Su, H., Wang, H., Fang, F., 2023. Modeling and control of a bedside cable-driven lower-limb rehabilitation robot for bedridden individuals. Frontiers in Bioengineering and Biotechnology 11, 1321905. [27] Wang, S., Ahmad, N.S., 2025. Ai-based approaches for improving autonomous mobile robot localization in indoor environments: A comprehensive review. Engineering Science and Technology, an International Journal 63, 101977. URL: https://www.sciencedirect. com/science/article/pii/S2215098625000321 , doi: https://doi.org/10.1016/j.jestch.2025.101977 .[28] Yu, H., Guo, Y., Ye, L., Su, S.W., 2022. Statistical analysis of in-field magnetometer calibration for two representative methods. IEEE Transactions on Instrumentation and Measurement 71, 1â€“8. [29] Zhang, J., Shen, W., Chen, L., Song, A., 2025. Calibration and closed-loop control improve performance of a force feedback device. IEEE Transactions on Haptics 18, 255â€“268. doi: 10.1109/TOH.2025.3531471 .[30] Zhang, Z., Wang, L., Lee, C., 2023. Recent advances in artificial intelligence sensors. Advanced Sensor Research 2, 2200072. 

21