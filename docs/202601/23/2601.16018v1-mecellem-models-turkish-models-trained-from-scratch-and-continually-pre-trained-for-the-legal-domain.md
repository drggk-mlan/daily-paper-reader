# Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain
# Mecellem 模型：针对法律领域从头训练及持续预训练的土耳其语模型

**Authors**: Özgür Uğur, Mahmut Göksu, Mahmut Çimen, Musa Yılmaz, Esra Şavirdi, Alp Talha Demir, Rumeysa Güllüce, İclal Çetin, Ömer Can Sağbaş
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.16018v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 8.0
**Evidence**: Technical report on training large language models from scratch

---

## Abstract
This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.

## 摘要
本文介绍了 Mecellem 模型

---

## 论文详细总结（自动生成）

这篇论文介绍了 **Mecellem** 模型家族，这是专门为土耳其语法律领域设计的语言模型框架。该研究由土耳其 AI 团队 NewmindAI 完成，涵盖了从头训练的编码器（Encoder）和持续预训练的解码器（Decoder）。

以下是对该论文的深度结构化总结：

### 1. 核心问题与研究动机
*   **核心问题**：通用大语言模型（LLM）在处理特定领域（如法律）和特定语言（如土耳其语）时表现不佳。
*   **研究动机**：
    *   **语言特性**：土耳其语是一种形态丰富的黏着语，复杂的后缀和语法结构对分词和语义建模提出了极高要求。
    *   **领域挑战**：法律文本包含大量专业术语、长难句和严格的逻辑约束，通用模型难以精准捕捉其细微差别。
    *   **资源匮乏**：缺乏高质量、大规模的土耳其语法律专用 NLP 资源和基准测试。

### 2. 方法论与关键技术
论文采用了两种互补的策略：

#### A. 编码器模型（从头训练）
*   **核心思想**：基于 **ModernBERT** 架构，在 112.7B Token 的土耳其语主导语料库上从头开始训练双向编码器。
*   **关键技术**：
    *   **下游任务导向的 Checkpoint 选择**：研究发现，预训练损失（MLM Loss）达到最低点时，下游检索性能未必最优。因此，团队在训练过程中实时评估检索指标来选择最佳模型。
    *   **定制分词器**：针对土耳其语形态特征设计，提高“词元纯度”（Token Purity），确保分词符合语言学逻辑。
    *   **对比学习后训练**：使用 InfoNCE 和 GISTEmbed（带引导模型过滤负样本）技术，将预训练编码器转化为高效的向量嵌入（Embedding）模型。

#### B. 解码器模型（持续预训练 - CPT）
*   **核心思想**：对 **Qwen3-1.7B** 和 **Qwen3-4B** 进行持续预训练，注入法律领域知识。
*   **关键技术**：
    *   **四阶段课程学习（Curriculum Learning）**：从通用文本 -> 法律术语 -> 长文本/规范性文档 -> 领域精炼，逐步增加难度。
    *   **灾难性遗忘缓解**：通过数据回放（Replay Buffer）和保守的学习率策略，在获取法律知识的同时保留通用语言能力。
    *   **解码器转编码器**：尝试将自回归模型转换为双向嵌入模型，以支持检索增强生成（RAG）。

### 3. 实验设计
*   **数据集**：
    *   **预训练**：整合了最高法院判决、学术论文（YÖKTEZ）、政府公报及通用网页数据（FineWeb2）。
    *   **评估**：开发了 **MTEB-Turkish** 基准（17个任务，涵盖分类、检索、聚类等）和 **EuroHPC-Legal** 法律专用评估集。
*   **对比方法**：
    *   **编码器**：对比了 BAAI/bge-m3、Google/embeddinggemma-300m、BERTurk-Legal 等。
    *   **解码器**：对比了原始 Qwen3 基座模型。
*   **评估指标**：nDCG@10（检索）、困惑度 PPL（生成）、以及专门开发的 **Muhakim** 奖励模型（从法律准确性、引用质量等维度评分）。

### 4. 资源与算力
*   **硬件平台**：使用西班牙巴塞罗那超级计算中心的 **MareNostrum 5** 超级计算机。
*   **算力细节**：
    *   **节点配置**：每个节点配备 4 张 NVIDIA **H100 64GB** GPU。
    *   **训练规模**：
        *   编码器训练：使用 16-32 个节点（64-128 张 H100）。
        *   解码器 CPT：使用 50-100 个节点（200-400 张 H100）。
    *   **吞吐量**：在 400 张 GPU 上达到了 7.35M tokens/sec 的峰值速度。

### 5. 实验数量与充分性
*   **实验规模**：论文进行了大量的消融实验