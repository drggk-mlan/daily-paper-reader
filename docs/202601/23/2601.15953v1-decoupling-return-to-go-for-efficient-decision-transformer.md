# Decoupling Return-to-Go for Efficient Decision Transformer
# 解耦待获得回报以实现高效的决策 Transformer

**Authors**: Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li, Qirui Zheng, Xionghui Yang, Wenxin Li
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15953v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:RL</span>
**Score**: 8.0
**Evidence**: Proposes an efficient architecture for offline reinforcement learning by decoupling return-to-go

---

## Abstract
The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

## 摘要
决策 Transformer (DT) 为离线强化学习建立了一种强大的序列建模方法。它将动作预测以待获得回报 (RTG) 为条件，在训练期间利用其区分轨迹质量，并在推理时指导动作生成。在这项工作中，我们发现了该设计中的一个关键冗余：从理论上讲，将整个 RTG 序列输入 Transformer 是不必要的，因为只有最近的 RTG 会影响动作预测

---

## 论文详细总结（自动生成）

这篇论文由北京大学团队提出，旨在解决决策 Transformer（Decision Transformer, DT）在离线强化学习中的冗余设计问题。以下是对该论文的深度结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **核心问题**：传统的 DT 将“待获得回报”（Return-to-Go, RTG）、观测（Observation）和动作（Action）作为等地位的 Token 全部输入 Transformer。作者指出，**输入完整的 RTG 历史序列在理论上是冗余的**。
*   **研究动机**：
    *   **理论层面**：在部分可观测马尔可夫决策过程（POMDP）中，决策仅依赖于当前信念状态（由历史观测和动作构建）和未来的目标（当前的 RTG）。历史 RTG 之间的差值仅代表已获得的奖励，对预测未来动作没有额外帮助。
    *   **性能层面**：这种冗余不仅增加了计算开销（Transformer 的计算量随序列长度平方增长），还可能干扰注意力机制的分配，导致性能下降。

### 2. 论文提出的方法论：DDT (Decoupled DT)
*   **核心思想**：将 RTG 从 Transformer 的输入序列中**解耦**出来。Transformer 仅处理观测和动作序列，而当前的 RTG 则作为一种“调制信号”直接作用于输出层。
*   **关键技术细节**：
    *   **输入简化**：输入序列从 $(R_1, o_1, a_1, \dots, R_t, o_t)$ 缩减为 $(o_1, a_1, \dots, o_t)$，序列长度从 $3k$ 减少到 $2k$。
    *   **自适应层归一化 (adaLN)**：借鉴了 Diffusion Transformer (DiT) 的设计。使用一个简单的 MLP 将当前的 RTG $\hat{R}_t$ 映射为缩放参数 $\gamma$ 和偏移参数 $\beta$。
    *   **动作预测流程**：Transformer 输出观测 $o_t$ 对应的隐藏状态后，通过 adaLN 注入 RTG 信息，再经过线性解码器预测动作 $a_t$。
*   **算法流程**：
    1. 将观测和动作序列进行线性嵌入并加入位置编码。
    2. 通过 GPT 骨干网络提取特征。
    3. 提取最后一个观测的隐藏状态。
    4. **注入条件**：使用当前 RTG 通过 adaLN 对该隐藏状态进行调制。
    5. 输出预测动作。

### 3. 实验设计
*   **数据集/场景**：
    *   **D4RL 标准基准**：包含 Hopper, Walker2d, HalfCheetah 三个环境，涵盖 Medium, Medium-Replay, Medium-Expert 三种数据质量。
    *   **离散/随机环境**：2048 游戏（具有高随机性和稀疏奖励特点）。
*   **对比方法 (Baselines)**：
    *   **经典离线 RL**：CQL, IQL, TD3+BC, BRAC-v。
    *   **DT 及其变体**：原始 DT, VDT (Value-guided DT), LSDT (Long-short DT)。

### 4. 资源与算力
*   **算力说明**：论文**未明确列出**具体的 GPU 型号、数量或总训练时长。
*   **效率提升**：文中强调了由于输入序列长度减少了 1/3，基于 Transformer 的二次方复杂度特性，DDT 在推理和训练时的计算开销和内存占用均显著低于原始 DT。

### 5. 实验数量与充分性
*   **实验规模**：
    *   在 9 个 D4RL 任务上进行了对比实验，每个任务运行了 4 个不同随机种子，每个种子进行 100 次评估。
    *   进行了 **2048 游戏** 的扩展实验，验证了在离散和稀疏奖励场景下的泛化性。
*   **消融实验**：
    *   **Blocked-DT**：仅通过注意力掩码屏蔽历史 RTG（不使用 adaLN），证明了 adaLN 注入信息的有效性。
    *   **