# ZENITH: Automated Gradient Norm Informed Stochastic Optimization
# ZENITH：自动化梯度范数告知的随机优化

**Authors**: Dhrubo Saha \\
**Date**: 2026-01-21 \\
**PDF**: https://arxiv.org/pdf/2601.15212v1 \\
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 8.0 \\
**Evidence**: automated evolution of learning rate using training history aligns with evolution of heuristics and automatic algorithms \\

---

## Abstract
Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.

## 摘要
训练深度计算机视觉模型需要对学习率（LR）调度进行人工监督或超参数调优。虽然现有的自

---

## 论文详细总结（自动生成）

这篇论文介绍了一种名为 **ZENITH**（Zero-overhead Evolution using Norm-Informed Training History）的新型自动学习率（LR）调度优化器。以下是对该论文的深度结构化总结：

### 1. 核心问题与整体含义（研究动机）
在深度学习中，学习率（LR）的设置和调度对模型性能至关重要，但通常需要繁琐的手动调优或预设调度器（如余弦退火）。现有的自动优化器（如 D-Adaptation, COCOB, Polyak-style 等）虽然试图解决这一问题，但普遍存在以下痛点：
*   **计算与内存开销大**：需要存储多个辅助变量（如梯度的平方和、最大梯度等），导致内存占用是 SGD 的数倍。
*   **墙钟时间（Wall-clock time）长**：复杂的计算增加了每轮迭代的时间。
*   **对损失缩放敏感**：不同任务的损失函数量级不同，导致算法失效。
*   **与正则化不兼容**：加入 L2 正则化后，某些算法会误判收敛状态。
*   **泛化性差**：容易陷入尖锐的局部最小值。

**ZENITH 的目标**是提供一个**零额外内存开销、极低计算开销、且对正则化友好**的自动学习率调度方案。

### 2. 核心方法论
ZENITH 的核心思想是利用**梯度范数（Gradient Norm）的历史演化**来动态调整学习率。

*   **核心逻辑**：在训练初期，梯度范数较大，表示处于损失平面的陡峭区域，此时应保持高学习率以快速下降并逃离局部最小值；随着接近最小值，梯度范数减小，学习率应相应衰减以稳定收敛。
*   **关键技术细节**：
    1.  **滑动窗口均值 ($\mu_t$)**：维护一个容量为 $W$ 的先进先出队列，存储最近 $W$ 个迭代的梯度 L2 范数，计算其滚动平均值，以平滑噪声。
    2.  **历史最高点 ($Z_t$)**：记录训练过程中 $\mu_t$ 达到的历史最大值（即“Zenith”）。
    3.  **更新规则**：
        $$\eta_t = \eta_0 \cdot \frac{\mu_t}{Z_t}$$
        其中 $\eta_0$ 是初始学习率。该公式确保了学习率始终在 $[0, \eta_0]$ 之间，且随梯度衰减自动实现类似指数或余弦退火的效果。
*   **算法优势**：由于只处理标量（梯度范数），它不需要为每个权重存储状态，因此**内存开销为零**（相对于模型参数量）。

### 3. 实验设计
论文进行了广泛的实验，涵盖了多种计算机视觉任务：
*   **图像分类**：
    *   **数据集**：MNIST, CIFAR-10, CIFAR-100, Food-101, Tiny ImageNet, ImageNet-100。
    *   **模型**：EfficientNet, VGG19, ResNet50, DLA, Inception, DenseNet121。
*   **检测与分割**（MS COCO 数据集）：
    *   使用 Keypoint R-CNN 进行关键点检测。
    *   使用 Mask R-CNN 进行实例分割。
*   **对比方法（Baselines）**：对比了 11 种主流自动优化器，包括 PAL, LQA, GeN, COCOB, DoG, DoWG, D-Adaptation, Prodigy, L4, ALIG, SPS。

### 4. 资源与算力
*   **硬件**：使用了 **NVIDIA A100 GPU**（40GB 或 80GB 版本）。
*   **训练时长**：
    *   分类任务：根据数据集不同，设定为 1 小时到 10 小时不等。
    *   COCO 任务：总时长 24 小时（含训练与定期测试）。
*   **效率指标**：专门测量了“墙钟时间”（Wall-clock time），结果显示 ZENITH 的时间开销仅比原生 SGD 高出 **1.3%**，而其他方法（如 LQA）高出 372%。

### 5. 实验数量与充分性
*   **实验规模**：涵盖了 6 种架构、6 个分类基准以及 2 个复杂的检测/分割任务，实验设置较为全面。
*   **消融与分析**：
    *   **超参数敏感性**：测试了不同初始学习率 $\eta_0$ 和窗口大小 $W$ 的影响，证明了 ZENITH 对这些参数不敏感。
    *   **正则化测试**：专门对比了在不同 L2 正则化强度下的表现。
    *   **景观分析**：通过 Hessian 矩阵的最大特征值分析了收敛点的“平坦度”（Sharpness），解释了其泛化性好的原因。
*   **客观性**：所有对比方法均使用了其论文推荐的配置，且 ZENITH 在所有实验中固定 $W=500$，未进行特定任务的调优，体现了公平性。

### 6. 主要结论与发现
*   **性能领先**：ZENITH 在几乎所有分类任务中达到了最高的测试准确率，且收敛速度（按墙钟时间计）最快。
*   **泛化性强**：ZENITH 倾向于收敛到更平坦的最小值（Flatter Minima），这直接提升了测试集的表现。
*   **尺度不变性