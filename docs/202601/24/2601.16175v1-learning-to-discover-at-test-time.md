# Learning to Discover at Test Time
# 在测试时学习发现

**Authors**: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun \\
**Date**: 2026-01-22 \\
**PDF**: https://arxiv.org/pdf/2601.16175v1 \\
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 8.0 \\
**Evidence**: Focuses on automated discovery of state-of-the-art solutions using LLM-based search and evolution \\

---

## Abstract
How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

## 摘要
如何利用人工智能为科学问题发现新的最先进（SOTA）成果？以往关于测试时

---

## 论文详细总结（自动生成）

这篇论文由斯坦福大学、NVIDIA、UC圣地亚哥等机构的研究者共同完成，提出了一种名为 **TTT-Discover (Test-Time Training to Discover)** 的新方法。该方法通过在测试阶段对大语言模型（LLM）进行强化学习，成功在数学、算子工程、算法竞赛和生物学等多个领域刷新了世界纪录（SOTA）。

以下是对该论文的结构化深入总结：

### 1. 论文的核心问题与整体含义
*   **研究动机**：科学发现本质上要求产生超出模型训练数据、甚至超出人类现有知识的新思想。
*   **核心问题**：现有的测试时缩放（Test-time Scaling）方法（如 AlphaEvolve）通常使用“冻结”的 LLM 进行启发式搜索。这种方式下，LLM 无法从失败或局部的成功中“内化”新知识。
*   **整体含义**：论文提出，LLM 应该像人类一样，在解决难题的过程中通过尝试和失败进行学习。TTT-Discover 将“搜索”与“在线学习”结合，使模型在针对特定测试问题进行推理的同时，实时更新自身权重。

### 2. 方法论：核心思想与关键技术
TTT-Discover 的核心是将测试过程视为一个单问题的强化学习（RL）环境。其关键技术细节包括：

*   **核心思想**：不追求策略的泛化性，也不追求平均奖励，而是通过在线训练产生**一个**极优的解。
*   **关键技术组件**：
    1.  **熵目标函数 (Entropic Objective, $J_\beta$)**：不同于标准 RL 最大化平均奖励，该目标函数通过指数加权（$\beta$ 参数）使模型专注于追逐最大奖励（Max Reward）。
    2.  **自适应 $\beta$ (Adaptive $\beta$)**：为了平衡训练稳定性，系统会根据当前状态的 KL 散度约束自动调整 $\beta$，防止在训练初期因权重过分集中而崩溃。
    3.  **PUCT 状态重用 (PUCT-based Reuse)**：借鉴 AlphaZero 的启发式搜索，维护一个优秀解的缓存池（Buffer）。使用基于最大奖励（Q）和排名先验（P）的规则来决定从哪个历史解开始下一步的探索，平衡“利用”与“探索”。
    4.  **在线 LoRA 微调**：在测试时，使用 LoRA（秩为 32）对模型进行参数更新，每步训练包含 512 个采样（Rollouts）。

### 3. 实验设计：场景、Benchmark 与对比
论文在四个极具挑战性的领域进行了实验：
*   **数学 (Mathematics)**：
    *   **问题**：埃尔德什最小重叠问题（Erdős’ minimum overlap）、自相关不等式、圆堆积问题。
    *   **对比**：AlphaEvolve (V1/V2)、ThetaEvolve、人类数学家给出的已知界限。
*   **GPU 算子工程 (Kernel Engineering)**：
    *   **场景**：GPUMode 竞赛中的 TriMul（三角矩阵乘法）和 DeepSeek MLA 算子优化。
    *   **对比**：人类专家提交的最优代码、Best-of-N 采样。
*   **算法设计 (Algorithm Design)**：
    *   **场景**：AtCoder 启发式竞赛（AHC039 几何问题、AHC058 调度问题）。
    *   **对比**：ALE-Agent、ShinkaEvolve、AtCoder 历史排名。
*   **生物学 (Biology)**：
    *   **场景**：单细胞 RNA 测序数据去噪（OpenProblems 挑战赛）。
    *   **对比**：MAGIC、ALRA 等专业生物信息学算法。

### 4. 资源与算力
*   **模型**：主要使用开源模型 **OpenAI gpt-oss-120b**，部分对比实验使用了 Qwen3-8B。
*   **算力平台**：通过 Thinking Machines 的 **Tinker API** 进行训练。
*   **成本与规模**：
    *   每个问题执行 50 步训练，每步 512 个采样，总计约 25,600 次采样。
    *   **经济成本**：每个问题的训练费用仅需 **几百美元**（约 $500）。
    *   **硬件**：评估过程使用了 H100、A100、H200 等 GPU（取决于具体任务）。

### 5. 实验数量与充分性
*   **实验规模**：论文对每一个尝试过的问题都报告了结果，没有“樱桃拾取”（Cherry-picking）。
*   **消融实验**：非常充分。作者对比了“无 TTT（仅搜索）”、“无熵