{
  "mode": "standard",
  "generated_at": "2026-01-23T04:38:08.075278+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 6,
    "deep_divecandidates": 7,
    "deep_cap": 11,
    "deep_selected": 7,
    "quick_candidates": 23,
    "quick_skim_target": 16,
    "quick_selected": 16
  },
  "deep_dive": [
    {
      "id": "2601.15812v1",
      "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models",
      "abstract": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.",
      "authors": [
        "Shir Ashury-Tahan",
        "Yifan Mai",
        "Elron Bandel",
        "Michal Shmueli-Scheuer",
        "Leshem Choshen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22 09:52:39+00:00",
      "link": "https://arxiv.org/pdf/2601.15812v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Evaluation framework for failure analysis in LLM benchmarks",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.16206v1",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "authors": [
        "Daixuan Cheng",
        "Shaohan Huang",
        "Yuxian Gu",
        "Huatong Song",
        "Guoxin Chen",
        "Li Dong",
        "Wayne Xin Zhao",
        "Ji-Rong Wen",
        "Furu Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-22 18:57:09+00:00",
      "link": "https://arxiv.org/pdf/2601.16206v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "introduces reinforcement learning for LLM agents aligning with RL and PPO interests and LLM queries",
      "llm_tags": [
        "keyword:RL",
        "keyword:ppo",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.15690v1",
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
      "authors": [
        "Jiaxin Zhang",
        "Wendi Cui",
        "Zhuohang Li",
        "Lifu Huang",
        "Bradley Malin",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "published": "2026-01-22 06:21:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15690v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "survey on LLM uncertainty in reinforcement learning and autonomous agents",
      "llm_tags": [
        "query:大厂llm",
        "keyword:RL"
      ]
    },
    {
      "id": "2601.15706v1",
      "title": "Improving Methodologies for LLM Evaluations Across Global Languages",
      "abstract": "As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.   The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.",
      "authors": [
        "Akriti Vij",
        "Benjamin Chua",
        "Darshini Ramiah",
        "En Qi Ng",
        "Mahran Morsidi",
        "Naga Nikshith Gangarapu",
        "Sharmini Johnson",
        "Vanessa Wilfred",
        "Vikneswaran Kumaran",
        "Wan Sie Lee",
        "Wenzhuo Yang",
        "Yongsen Zheng",
        "Bill Black",
        "Boming Xia",
        "Frank Sun",
        "Hao Zhang",
        "Qinghua Lu",
        "Suyu Ma",
        "Yue Liu",
        "Chi-kiu Lo",
        "Fatemeh Azadi",
        "Isar Nejadgholi",
        "Sowmya Vajjala",
        "Agnes Delaborde",
        "Nicolas Rolin",
        "Tom Seimandi",
        "Akiko Murakami",
        "Haruto Ishi",
        "Satoshi Sekine",
        "Takayuki Semitsu",
        "Tasuku Sasaki",
        "Angela Kinuthia",
        "Jean Wangari",
        "Michael Michie",
        "Stephanie Kasaon",
        "Hankyul Baek",
        "Jaewon Noh",
        "Kihyuk Nam",
        "Sang Seo",
        "Sungpil Shin",
        "Taewhi Lee",
        "Yongsu Kim",
        "Daisy Newbold-Harrop",
        "Jessica Wang",
        "Mahmoud Ghanem",
        "Vy Hong"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 07:18:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15706v1",
      "tags": [
        "keyword:ppo",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Describes multilingual evaluation benchmarks for frontier AI models",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.15953v1",
      "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
      "abstract": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.",
      "authors": [
        "Yongyi Wang",
        "Hanyu Liu",
        "Lingfeng Li",
        "Bozhou Chen",
        "Ang Li",
        "Qirui Zheng",
        "Xionghui Yang",
        "Wenxin Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 13:42:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15953v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Proposes an efficient architecture for offline reinforcement learning by decoupling return-to-go",
      "llm_tags": [
        "keyword:RL"
      ]
    },
    {
      "id": "2601.16018v1",
      "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
      "abstract": "This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.",
      "authors": [
        "Özgür Uğur",
        "Mahmut Göksu",
        "Mahmut Çimen",
        "Musa Yılmaz",
        "Esra Şavirdi",
        "Alp Talha Demir",
        "Rumeysa Güllüce",
        "İclal Çetin",
        "Ömer Can Sağbaş"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 14:41:32+00:00",
      "link": "https://arxiv.org/pdf/2601.16018v1",
      "tags": [
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Technical report on training large language models from scratch",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.16172v1",
      "title": "Structured Hints for Sample-Efficient Lean Theorem Proving",
      "abstract": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.",
      "authors": [
        "Zachary Burton"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 18:16:46+00:00",
      "link": "https://arxiv.org/pdf/2601.16172v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Discusses DeepSeek-Prover-V1.5 which is an industry LLM using RL and PPO-style training",
      "llm_tags": [
        "keyword:RL",
        "keyword:ppo",
        "query:大厂llm"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2601.15645v1",
      "title": "Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation",
      "abstract": "Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.",
      "authors": [
        "Zhiyao Ren",
        "Yibing Zhan",
        "Siyuan Liang",
        "Guozheng Ma",
        "Baosheng Yu",
        "Dacheng Tao"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 04:51:39+00:00",
      "link": "https://arxiv.org/pdf/2601.15645v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Benchmarking large language models in specialized medical domains",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15688v1",
      "title": "Performance-guided Reinforced Active Learning for Object Detection",
      "abstract": "Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.",
      "authors": [
        "Zhixuan Liang",
        "Xingyu Zeng",
        "Rui Zhao",
        "Ping Luo"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2026-01-22 06:17:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15688v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet"
      ],
      "llm_score": 6.0,
      "llm_evidence": "proposes a reinforced active learning approach for object detection tasks",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15668v1",
      "title": "EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning",
      "abstract": "Emotional information in speech plays a unique role in multimodal perception. However, current Speech Large Language Models (SpeechLLMs), similar to conventional speech emotion recognition (SER) systems, still treat emotion understanding as a simple classification problem. This provides limited interpretability of predictions, while leaving the LLMs' expressive and reasoning capabilities underutilized. In this work, we take the first step to reformulate SER as a deep reasoning problem through reinforcement learning (RL). We propose EmotionThinker, which is designed to generate accurate emotion predictions with interpretable explanations grounded in fine-grained acoustic cues. To achieve this, we first construct EmotionCoT-35K, an emotional reasoning dataset with Chain-of-Thought annotations and detailed captions. Second, we observe that current SpeechLLMs exhibit weak prosody perception, whereas prosodic cues constitute fundamental signals for interpreting emotions. To address this, we develop the prosody-enhanced foundation model EmotionThinker-Base, and demonstrate that prosody enhancement improves emotion understanding. Third, we introduce Group-Relative-Policy-Optimization with Progressive-Trust-aware-Reasoning-Reward (GRPO-PTR) for RL. Different from standard GRPO, which relies only on rule-based outcome rewards, GRPO-PTR progressively introduces reasoning reward, dynamically adjusts it with a trustworthiness weight reflecting the alignment between reasoning and outcome, and evaluates the overall reasoning quality with a reward model based on multi-dimensional criteria. EmotionThinker outperforms previous state-of-the-art evaluation models both in emotion accuracy and explanation quality, advancing SER toward interpretable multimodal reasoning. Project page: https://github.com/dingdongwang/EmotionThinker",
      "authors": [
        "Dingdong Wang",
        "Shujie Liu",
        "Tianhua Zhang",
        "Youjun Chen",
        "Jinyu Li",
        "Helen Meng"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD"
      ],
      "published": "2026-01-22 05:51:53+00:00",
      "link": "https://arxiv.org/pdf/2601.15668v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Reinforcement learning for reasoning in speech LLMs",
      "llm_tags": [
        "keyword:RL",
        "keyword:ppo",
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15707v1",
      "title": "D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot",
      "abstract": "Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.",
      "authors": [
        "Qifan Hu",
        "Branko Celler",
        "Weidong Mu",
        "Steven W. Su"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-01-22 07:20:55+00:00",
      "link": "https://arxiv.org/pdf/2601.15707v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Reinforcement learning applied to robot calibration problems",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15717v1",
      "title": "Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling",
      "abstract": "Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.",
      "authors": [
        "Luyao Zhu",
        "Fangfang Zhang",
        "Yi Mei",
        "Mengjie Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 07:38:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15717v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Genetic Programming for scheduling is a core method in symbolic regression research",
      "llm_tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15708v1",
      "title": "Persona Switch: Mixing Distinct Perspectives in Decoding Time",
      "abstract": "Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.",
      "authors": [
        "Junseok Kim",
        "Nakyeong Yang",
        "Kyomin Jung"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 07:30:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15708v1",
      "tags": [
        "keyword:ppo"
      ],
      "llm_score": 6.0,
      "llm_evidence": "proposes a novel decoding method for LLMs to improve zero-shot reasoning capabilities",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15761v1",
      "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
      "abstract": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.",
      "authors": [
        "Xiefeng Wu",
        "Mingyu Hu",
        "Shu Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 08:51:16+00:00",
      "link": "https://arxiv.org/pdf/2601.15761v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "off-policy actor-critic reinforcement learning for real-world robotics",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15724v1",
      "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
      "abstract": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.",
      "authors": [
        "Chenglin Li",
        "Qianglong Chen",
        "Feng Han",
        "Yikun Wang",
        "Xingxi Yin",
        "Yan Gong",
        "Ruilin Li",
        "Yin Zhang",
        "Jiaqi Wang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-22 07:47:29+00:00",
      "link": "https://arxiv.org/pdf/2601.15724v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "describes building agentic VideoLLMs using LLM-guided reasoning and synthetic trajectories",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15879v1",
      "title": "Evaluating and Achieving Controllable Code Completion in Code LLM",
      "abstract": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.",
      "authors": [
        "Jiajun Zhang",
        "Zeyu Cui",
        "Lei Zhang",
        "Jian Yang",
        "Jiaxi Yang",
        "Qiang Liu",
        "Zilei Wang",
        "Binyuan Hui",
        "Liang Wang",
        "Junyang Lin"
      ],
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "published": "2026-01-22 11:40:04+00:00",
      "link": "https://arxiv.org/pdf/2601.15879v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "comprehensive evaluation of 40 mainstream LLMs on a new code benchmark",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15773v1",
      "title": "Next Generation Active Learning: Mixture of LLMs in the Loop",
      "abstract": "With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.",
      "authors": [
        "Yuanyuan Qi",
        "Xiaohao Yang",
        "Jueqing Lu",
        "Guoxiang Guo",
        "Joanne Enticott",
        "Gang Liu",
        "Lan Du"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-22 09:01:42+00:00",
      "link": "https://arxiv.org/pdf/2601.15773v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "framework utilizing a mixture of large language models for annotation",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16061v1",
      "title": "Dynamic Tactile Sensing System and Soft Actor Critic Reinforcement Learning for Inclusion Characterization",
      "abstract": "This paper presents the Dynamic Tactile Sensing System that utilizes robotic tactile sensing in conjunction with reinforcement learning to locate and characterize embedded inclusions. A dual arm robot is integrated with an optical Tactile Imaging Sensor that utilizes the Soft Actor Critic Algorithm to acquire tactile data based on a pixel intensity reward. A Dynamic Interrogation procedure for tactile exploration is developed that enables the robot to first localize inclusion and refine their positions for precise imaging. Experimental validation conducted on Polydimethylsiloxane phantoms demonstrates that the robot using the Tactile Soft Actor Critic Model was able to achieve size estimation errors of 2.61% and 5.29% for soft and hard inclusions compared to 7.84% and 6.87% for expert human operators. Results also show that Dynamic Tactile Sensing System was able to locate embedded inclusions and autonomously determine their mechanical properties, useful in applications such as breast tumor characterization.",
      "authors": [
        "John Bannan",
        "Nazia Rahman",
        "Chang-Hee Won"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-01-22 15:57:15+00:00",
      "link": "https://arxiv.org/pdf/2601.16061v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 7.0,
      "llm_evidence": "utilizes reinforcement learning for robotic tactile sensing and inclusion characterization",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15793v1",
      "title": "HumanLLM: Towards Personalized Understanding and Simulation of Human Nature",
      "abstract": "Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.",
      "authors": [
        "Yuxuan Lei",
        "Tianfu Wang",
        "Jianxun Lian",
        "Zhengyu Hu",
        "Defu Lian",
        "Xing Xie"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 09:27:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15793v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Discusses LLM pretraining and simulation capabilities relevant to large language model technical reports",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16200v1",
      "title": "Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing",
      "abstract": "Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\\% to about 1\\%.",
      "authors": [
        "Song Xia",
        "Meiwen Ding",
        "Chenqi Kong",
        "Wenhan Yang",
        "Xudong Jiang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2026-01-22 18:52:21+00:00",
      "link": "https://arxiv.org/pdf/2601.16200v1",
      "tags": [
        "keyword:ppo",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Robustness and evaluation of multimodal large language models",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15892v1",
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "abstract": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
      "authors": [
        "Chenghao Fan",
        "Wen Heng",
        "Bo Li",
        "Sichen Liu",
        "Yuxuan Song",
        "Jing Su",
        "Xiaoye Qu",
        "Kai Shen",
        "Wei Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 12:13:17+00:00",
      "link": "https://arxiv.org/pdf/2601.15892v1",
      "tags": [
        "keyword:resnet",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "diffusion based code large language model architecture and benchmark performance",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15912v1",
      "title": "TeNet: Text-to-Network for Compact Policy Synthesis",
      "abstract": "Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.",
      "authors": [
        "Ariyan Bighashdel",
        "Kevin Sebastian Luck"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2026-01-22 12:42:30+00:00",
      "link": "https://arxiv.org/pdf/2601.15912v1",
      "tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Uses pretrained LLMs to generate robot policies, bridging large language models and control",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15995v1",
      "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour",
      "abstract": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.",
      "authors": [
        "Liang Wang",
        "Kanzhong Yao",
        "Yang Liu",
        "Weikai Qin",
        "Jun Wu",
        "Zhe Sun",
        "Qiuguo Zhu"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22 14:16:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15995v1",
      "tags": [
        "keyword:RL",
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "end-to-end reinforcement learning framework for quadruped locomotion",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "6"
    }
  ]
}