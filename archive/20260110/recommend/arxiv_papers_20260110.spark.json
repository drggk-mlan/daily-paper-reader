{
  "mode": "spark",
  "generated_at": "2026-01-10T22:36:38.861106+00:00",
  "stats": {
    "mode": "spark",
    "tag_count": 5,
    "deep_divecandidates": 52,
    "deep_cap": 10,
    "deep_selected": 10,
    "quick_candidates": 133,
    "quick_skim_target": 15,
    "quick_selected": 15
  },
  "deep_dive": [
    {
      "id": "2601.03723v1",
      "title": "ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffers from a structural limitation: it imposes a uniform, static trust region constraint across all samples. This design implicitly assumes signal homogeneity, a premise misaligned with the heterogeneous nature of outcome-driven learning, where advantage magnitudes and variances fluctuate significantly. Consequently, static constraints fail to fully exploit high-quality signals while insufficiently suppressing noise, often precipitating rapid entropy collapse. To address this, we propose \\textbf{E}lastic \\textbf{T}rust \\textbf{R}egions (\\textbf{ETR}), a dynamic mechanism that aligns optimization constraints with signal quality. ETR constructs a signal-aware landscape through dual-level elasticity: at the micro level, it scales clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths; at the macro level, it leverages group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Extensive experiments on AIME and MATH benchmarks demonstrate that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.",
      "authors": [
        "Shijie Zhang",
        "Kevin Zhang",
        "Zheyuan Gu",
        "Xiang Guo",
        "Rujun Guo",
        "Shaoyu Liu",
        "Guanjun Jiang",
        "Xiaozhao Wang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-07 09:19:53+00:00",
      "link": "https://arxiv.org/pdf/2601.03723v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 10.0,
      "llm_evidence": "Discusses RLVR and GRPO algorithms used in OpenAI o1 and DeepSeek-R1",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.03590v1",
      "title": "Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions",
      "abstract": "Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant \"spatial gap\" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .",
      "authors": [
        "Zhongbin Guo",
        "Zhen Yang",
        "Yushan Li",
        "Xinyue Zhang",
        "Wenyu Gao",
        "Jiacheng Wang",
        "Chengzhi Li",
        "Xiangrui Liu",
        "Ping Jian"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-07 05:13:52+00:00",
      "link": "https://arxiv.org/pdf/2601.03590v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Introduces a benchmark for LLM spatial intelligence and symbolic reasoning",
      "llm_tags": [
        "query:sr-bench",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.04051v1",
      "title": "Symbolic Regression for Shared Expressions: Introducing Partial Parameter Sharing",
      "abstract": "Symbolic Regression aims to find symbolic expressions that describe datasets. Due to better interpretability, it is a machine learning paradigm particularly powerful for scientific discovery. In recent years, several works have expanded the concept to allow the description of similar phenomena using a single expression with varying sets of parameters, thereby introducing categorical variables. Some previous works allow only \"non-shared\" (category-value-specific) parameters, and others also incorporate \"shared\" (category-value-agnostic) parameters. We expand upon those efforts by considering multiple categorical variables, and introducing intermediate levels of parameter sharing. With two categorical variables, an intermediate level of parameter sharing emerges, i.e., parameters which are shared across either category but change across the other. The new approach potentially decreases the number of parameters, while revealing additional information about the problem. Using a synthetic, fitting-only example, we test the limits of this setup in terms of data requirement reduction and transfer learning. As a real-world symbolic regression example, we demonstrate the benefits of the proposed approach on an astrophysics dataset used in a previous study, which considered only one categorical variable. We achieve a similar fit quality but require significantly fewer individual parameters, and extract additional information about the problem.",
      "authors": [
        "Viktor Martinek",
        "Roland Herzog"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-07 16:12:14+00:00",
      "link": "https://arxiv.org/pdf/2601.04051v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Advances in symbolic regression methodology for scientific discovery",
      "llm_tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ]
    },
    {
      "id": "2601.04365v1",
      "title": "Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning",
      "abstract": "In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife.",
      "authors": [
        "Anton Roupassov-Ruiz",
        "Yiyang Zuo"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-07 20:09:28+00:00",
      "link": "https://arxiv.org/pdf/2601.04365v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Compares neural and programmatic policies in evolutionary reinforcement learning",
      "llm_tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ]
    },
    {
      "id": "2601.03661v1",
      "title": "AMIR-GRPO: Inducing Implicit Preference Signals into GRPO",
      "abstract": "Reinforcement learning has become the primary paradigm for aligning large language models (LLMs) on complex reasoning tasks, with group relative policy optimization (GRPO) widely used in large-scale post-training. However, GRPO faces structural limitations in reasoning-heavy settings: sequence-level advantage normalization introduces systematic length bias, penalties for low-quality trajectories are diluted, and the scalar objective discards rich pairwise preference information embedded in within-group reward rankings. As a result, valuable supervision from costly rollouts remains underutilized.   We propose AMIR-GRPO, which augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no additional annotations. This mechanism amplifies suppression of low-reward trajectories, attenuates response-level length bias, and transforms each rollout group into a denser set of supervision constraints. Across multiple mathematical reasoning benchmarks, AMIR-GRPO consistently outperforms strong GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and delivers broader coverage gains beyond the subset of instances solved by standard GRPO.",
      "authors": [
        "Amir Hossein Yari",
        "Fajri Koto"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-07 07:22:58+00:00",
      "link": "https://arxiv.org/pdf/2601.03661v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Reinforcement learning alignment for large language models using GRPO",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.03895v1",
      "title": "Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training",
      "abstract": "Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.",
      "authors": [
        "Chi Liu",
        "Xin Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-07 13:04:52+00:00",
      "link": "https://arxiv.org/pdf/2601.03895v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Refines GRPO reinforcement learning for LLM training stability",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.03699v1",
      "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
      "abstract": "As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval",
      "authors": [
        "Quy-Anh Dang",
        "Chris Ngo",
        "Truong-Son Hy"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 08:34:17+00:00",
      "link": "https://arxiv.org/pdf/2601.03699v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Universal dataset for comprehensive red teaming and evaluation of LLMs",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.04876v1",
      "title": "ChronosAudio: A Comprehensive Long-Audio Benchmark for Evaluating Audio-Large Language Models",
      "abstract": "Although Audio Large Language Models (ALLMs) have witnessed substantial advancements, their long audio understanding capabilities remain unexplored. A plethora of benchmarks have been proposed for general audio tasks, they predominantly focus on short-form clips, leaving without a consensus on evaluating ALLMs over extended durations. This paper proposes ChronosAudio, the first multi-task benchmark tailored for long-audio understanding in ALLMs. It encompasses six major task categories and comprises 36,000 test instances totaling over 200 hours audio, stratified into short, middle, and long-form categories to comprehensively evaluate length generalization. Extensive experiments on 16 state-of-the-art models using ChronosAudio yield three critical findings: 1.Precipitous Long-Context Collapse: ALLMs exhibit a severe inability to sustain performance, with the transition from short to long contexts triggering a staggering performance degradation of over 90% in specific tasks. 2.Structural Attention Dilution: Performance degradation stems from a fundamental failure in maintaining temporal locality; attention mechanisms suffer from significant diffusion in later sequences. 3.Restorative Ceiling of Mitigation: Current strategies only offer 50% recovery. These findings reveal significant challenges in long-audio, underscoring the urgent need for approaches to achieve robust, document-level audio reasoning.",
      "authors": [
        "Kaiwen Luo",
        "Liang Lin",
        "Yibo Zhang",
        "Moayad Aloqaily",
        "Dexian Wang",
        "Zhenhong Zhou",
        "Junwei Zhang",
        "Kun Wang",
        "Li Sun",
        "Qingsong Wen"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD"
      ],
      "published": "2026-01-08 12:21:09+00:00",
      "link": "https://arxiv.org/pdf/2601.04876v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Describes a comprehensive evaluation benchmark for Audio-Large Language Models",
      "llm_tags": [
        "query:sr-bench",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.04411v1",
      "title": "Rate or Fate? RLV$^\\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?   To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time (\"rate, not fate\"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.",
      "authors": [
        "Ali Rad",
        "Khashayar Filom",
        "Darioush Keivan",
        "Peyman Mohajerin Esfahani",
        "Ehsan Kamalinejad"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-07 21:31:26+00:00",
      "link": "https://arxiv.org/pdf/2601.04411v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Reinforcement learning with verifiable rewards for training LLMs",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.03986v1",
      "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
      "abstract": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
      "authors": [
        "Qi Qian",
        "Chengsong Huang",
        "Jingwen Xu",
        "Changze Lv",
        "Muling Wu",
        "Wenhao Liu",
        "Xiaohua Wang",
        "Zhenghua Wang",
        "Zisu Huang",
        "Muzhao Tian",
        "Jianhan Xu",
        "Kun Hu",
        "He-Da Wang",
        "Yao Hu",
        "Xuanjing Huang",
        "Xiaoqing Zheng"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 14:59:03+00:00",
      "link": "https://arxiv.org/pdf/2601.03986v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "presents a framework for systematic evaluation of LLM benchmarks across multiple domains",
      "llm_tags": [
        "query:大厂llm"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2601.04260v1",
      "title": "Towards a Mechanistic Understanding of Propositional Logical Reasoning in Large Language Models",
      "abstract": "Understanding how Large Language Models (LLMs) perform logical reasoning internally remains a fundamental challenge. While prior mechanistic studies focus on identifying taskspecific circuits, they leave open the question of what computational strategies LLMs employ for propositional reasoning. We address this gap through comprehensive analysis of Qwen3 (8B and 14B) on PropLogic-MI, a controlled dataset spanning 11 propositional logic rule categories across one-hop and two-hop reasoning. Rather than asking ''which components are necessary,'' we ask ''how does the model organize computation?'' Our analysis reveals a coherent computational architecture comprising four interlocking mechanisms: Staged Computation (layer-wise processing phases), Information Transmission (information flow aggregation at boundary tokens), Fact Retrospection (persistent re-access of source facts), and Specialized Attention Heads (functionally distinct head types). These mechanisms generalize across model scales, rule types, and reasoning depths, providing mechanistic evidence that LLMs employ structured computational strategies for logical reasoning.",
      "authors": [
        "Danchun Chen",
        "Qiyao Yan",
        "Liangming Pan"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-07 04:20:30+00:00",
      "link": "https://arxiv.org/pdf/2601.04260v1",
      "tags": [
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Mechanistic analysis of Qwen3 architecture and reasoning strategies, directly relevant to LLM technical reports",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "8plus"
    },
    {
      "id": "2601.03486v1",
      "title": "Adaptive Model-Based Reinforcement Learning for Orbit Feedback Control in NSLS-II Storage Ring",
      "abstract": "The National Synchrotron Light Source II (NSLS-II) uses highly stable electron beam to produce high-quality X-ray beams with high brightness and low-emittance synchrotron radiation. The traditional algorithm to stabilize the beam applies singular value decomposition (SVD) on the orbit response matrix to remove noise and extract actions. Supervised learning has been studied on NSLS-II storage ring stabilization and other accelerator facilities recently. Several problems, for example, machine status drifting, environment noise, and non-linear accelerator dynamics, remain unresolved in the SVD-based and supervised learning algorithms. To address these problems, we propose an adaptive training framework based on model-based reinforcement learning. This framework consists of two types of optimizations: trajectory optimization attempts to minimize the expected total reward in a differentiable environment, and online model optimization learns non-linear machine dynamics through the agent-environment interaction. Through online training, this framework tracks the internal status drifting in the electron beam ring. Simulation and real in-facility experiments on NSLS-II reveal that our method stabilizes the beam position and minimizes the alignment error, defined as the root mean square (RMS) error between adjusted beam positions and the reference position, down to ~1$μ$m.",
      "authors": [
        "Zeyu Dong",
        "Yuke Tian",
        "Yu Sun"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-01-07 00:49:57+00:00",
      "link": "https://arxiv.org/pdf/2601.03486v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Applies model-based reinforcement learning to physical control systems",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.03500v1",
      "title": "SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models",
      "abstract": "Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.",
      "authors": [
        "Yuxuan Xia",
        "Siheng Wang",
        "Peng Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-07 01:27:58+00:00",
      "link": "https://arxiv.org/pdf/2601.03500v1",
      "tags": [
        "keyword:resnet"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Addresses internal complexities and hallucinations in Large Vision-Language Models",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.04424v1",
      "title": "Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization",
      "abstract": "Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.",
      "authors": [
        "Yao Dou",
        "Wei Xu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 22:08:17+00:00",
      "link": "https://arxiv.org/pdf/2601.04424v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Evaluates frontier LLMs like Gemini on long-context benchmarks",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "8plus"
    },
    {
      "id": "2601.03505v1",
      "title": "Beyond Perplexity: A Lightweight Benchmark for Knowledge Retention in Supervised Fine-Tuning",
      "abstract": "Supervised Fine-Tuning (SFT) is a standard approach for injecting domain knowledge into Large Language Models (LLMs). However, relying on validation perplexity to monitor training is often insufficient, as it confounds stylistic mimicry with genuine factual internalization. To address this, we introduce the Knowledge Retention (KR) Test , a lightweight, corpus-grounded evaluation framework designed to distinguish factual learning from linguistics. KR-Test utilizes automatically generated contrastive examples to measure likelihood preferences for correct versus incorrect continuations, requiring no instruction tuning or generative decoding. We validate the framework's integrity through a \"blind vs. oracle\" baseline analysis. Furthermore, we demonstrate the diagnostic capabilities of KR-Test by analyzing the training dynamics of Low-Rank Adaptation (LoRA). By exposing the fine-grained dissociation between linguistic convergence and knowledge retention, KR-Test enhances the interpretability of fine-tuning dynamics.",
      "authors": [
        "Soheil Zibakhsh Shabgahi",
        "Pedram Aghazadeh",
        "Farinaz Koushanfar"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-07 01:34:28+00:00",
      "link": "https://arxiv.org/pdf/2601.03505v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Evaluation framework for knowledge retention in LLMs",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.03520v1",
      "title": "A Reinforcement Learning-Based Model for Mapping and Goal-Directed Navigation Using Multiscale Place Fields",
      "abstract": "Autonomous navigation in complex and partially observable environments remains a central challenge in robotics. Several bio-inspired models of mapping and navigation based on place cells in the mammalian hippocampus have been proposed. This paper introduces a new robust model that employs parallel layers of place fields at multiple spatial scales, a replay-based reward mechanism, and dynamic scale fusion. Simulations show that the model improves path efficiency and accelerates learning compared to single-scale baselines, highlighting the value of multiscale spatial representations for adaptive robot navigation.",
      "authors": [
        "Bekarys Dukenbaev",
        "Andrew Gerstenslager",
        "Alexander Johnson",
        "Ali A. Minai"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.RO"
      ],
      "published": "2026-01-07 02:10:52+00:00",
      "link": "https://arxiv.org/pdf/2601.03520v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Focuses on Reinforcement Learning for autonomous navigation and mapping",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.04537v1",
      "title": "Not All Steps are Informative: On the Linearity of LLMs' RLVR Training",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.",
      "authors": [
        "Tianle Wang",
        "Zhongyuan Wu",
        "Shenghao Jin",
        "Hao Xu",
        "Wei Chen",
        "Ning Miao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-08 03:06:18+00:00",
      "link": "https://arxiv.org/pdf/2601.04537v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 9.0,
      "llm_evidence": "Analyzes reinforcement learning training methodologies for LLM post-training",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "quick_tier": "8plus"
    },
    {
      "id": "2601.03543v1",
      "title": "EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory",
      "abstract": "Despite recent advances in understanding and leveraging long-range conversational memory, existing benchmarks still lack systematic evaluation of large language models(LLMs) across diverse memory dimensions, particularly in multi-session settings. In this work, we propose EvolMem, a new benchmark for assessing multi-session memory capabilities of LLMs and agent systems. EvolMem is grounded in cognitive psychology and encompasses both declarative and non-declarative memory, further decomposed into multiple fine-grained abilities. To construct the benchmark, we introduce a hybrid data synthesis framework that consists of topic-initiated generation and narrative-inspired transformations. This framework enables scalable generation of multi-session conversations with controllable complexity, accompanied by sample-specific evaluation guidelines. Extensive evaluation reveals that no LLM consistently outperforms others across all memory dimensions. Moreover, agent memory mechanisms do not necessarily enhance LLMs' capabilities and often exhibit notable efficiency limitations. Data and code will be released at https://github.com/shenye7436/EvolMem.",
      "authors": [
        "Ye Shen",
        "Dun Pei",
        "Yiqiu Guo",
        "Junying Wang",
        "Yijin Guo",
        "Zicheng Zhang",
        "Qi Jia",
        "Jun Zhou",
        "Guangtao Zhai"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 03:14:42+00:00",
      "link": "https://arxiv.org/pdf/2601.03543v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Benchmark for evaluating LLM memory capabilities across diverse dimensions",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.03597v1",
      "title": "From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs",
      "abstract": "Large Language Models (LLMs) show strong reasoning ability in open-domain question answering, yet their reasoning processes are typically linear and often logically inconsistent. In contrast, real-world reasoning requires integrating multiple premises and solving subproblems in parallel. Existing methods, such as Chain-of-Thought (CoT), express reasoning in a linear textual form, which may appear coherent but frequently leads to inconsistent conclusions. Recent approaches rely on externally provided graphs and do not explore how LLMs can construct and use their own graph-structured reasoning, particularly in open-domain QA. To fill this gap, we novelly explore graph-structured reasoning of LLMs in general-domain question answering. We propose Self-Graph Reasoning (SGR), a framework that enables LLMs to explicitly represent their reasoning process as a structured graph before producing the final answer. We further construct a graph-structured reasoning dataset that merges multiple candidate reasoning graphs into refined graph structures for model training. Experiments on five QA benchmarks across both general and specialized domains show that SGR consistently improves reasoning consistency and yields a 17.74% gain over the base model. The LLaMA-3.3-70B model fine-tuned with SGR performs comparably to GPT-4o and surpasses Claude-3.5-Haiku, demonstrating the effectiveness of graph-structured reasoning.",
      "authors": [
        "Yingjian Chen",
        "Haoran Liu",
        "Yinhong Liu",
        "Sherry T. Tong",
        "Aosong Feng",
        "Jinghui Lu",
        "Juntao Zhang",
        "Yusuke Iwasawa",
        "Yutaka Matsuo",
        "Irene Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-07 05:27:41+00:00",
      "link": "https://arxiv.org/pdf/2601.03597v1",
      "tags": [
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Explores reasoning architectures and graph-structured methodologies for LLMs",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.03604v1",
      "title": "Interleaved Tool-Call Reasoning for Protein Function Understanding",
      "abstract": "Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.",
      "authors": [
        "Chuanliu Fan",
        "Zicheng Ma",
        "Huanran Meng",
        "Aijia Zhang",
        "Wenjie Du",
        "Jun Zhang",
        "Yi Qin Gao",
        "Ziqiang Cao",
        "Guohong Fu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-07 05:34:38+00:00",
      "link": "https://arxiv.org/pdf/2601.03604v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 6.0,
      "llm_evidence": "explores reinforcement learning and reasoning paradigms in large language models",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.03670v1",
      "title": "DisastQA: A Comprehensive Benchmark for Evaluating Question Answering in Disaster Management",
      "abstract": "Accurate question answering (QA) in disaster management requires reasoning over uncertain and conflicting information, a setting poorly captured by existing benchmarks built on clean evidence. We introduce DisastQA, a large-scale benchmark of 3,000 rigorously verified questions (2,000 multiple-choice and 1,000 open-ended) spanning eight disaster types. The benchmark is constructed via a human-LLM collaboration pipeline with stratified sampling to ensure balanced coverage. Models are evaluated under varying evidence conditions, from closed-book to noisy evidence integration, enabling separation of internal knowledge from reasoning under imperfect information. For open-ended QA, we propose a human-verified keypoint-based evaluation protocol emphasizing factual completeness over verbosity. Experiments with 20 models reveal substantial divergences from general-purpose leaderboards such as MMLU-Pro. While recent open-weight models approach proprietary systems in clean settings, performance degrades sharply under realistic noise, exposing critical reliability gaps for disaster response. All code, data, and evaluation resources are available at https://github.com/TamuChen18/DisastQA_open.",
      "authors": [
        "Zhitong Chen",
        "Kai Yin",
        "Xiangjue Dong",
        "Chengkai Liu",
        "Xiangpeng Li",
        "Yiming Xiao",
        "Bo Li",
        "Junwei Ma",
        "Ali Mostafavi",
        "James Caverlee"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 07:46:42+00:00",
      "link": "https://arxiv.org/pdf/2601.03670v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "A comprehensive benchmark for evaluating LLM reasoning under uncertainty, matching evaluation interests",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.03676v1",
      "title": "Towards Compositional Generalization of LLMs via Skill Taxonomy Guided Data Synthesis",
      "abstract": "Large Language Models (LLMs) and agent-based systems often struggle with compositional generalization due to a data bottleneck in which complex skill combinations follow a long-tailed, power-law distribution, limiting both instruction-following performance and generalization in agent-centric tasks. To address this challenge, we propose STEPS, a Skill Taxonomy guided Entropy-based Post-training data Synthesis framework for generating compositionally challenging data. STEPS explicitly targets compositional generalization by uncovering latent relationships among skills and organizing them into an interpretable, hierarchical skill taxonomy using structural information theory. Building on this taxonomy, we formulate data synthesis as a constrained information maximization problem, selecting skill combinations that maximize marginal structural information within the hierarchy while preserving semantic coherence. Experiments on challenging instruction-following benchmarks show that STEPS outperforms existing data synthesis baselines, while also yielding improved compositional generalization in downstream agent-based evaluations.",
      "authors": [
        "Yifan Wei",
        "Li Du",
        "Xiaoyan Yu",
        "Yang Feng",
        "Angsheng Li"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-07 07:58:51+00:00",
      "link": "https://arxiv.org/pdf/2601.03676v1",
      "tags": [
        "keyword:resnet",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Discusses LLM post-training data synthesis and generalization methodologies",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.03682v1",
      "title": "From Implicit to Explicit: Token-Efficient Logical Supervision for Mathematical Reasoning in LLMs",
      "abstract": "Recent studies reveal that large language models (LLMs) exhibit limited logical reasoning abilities in mathematical problem-solving, instead often relying on pattern-matching and memorization. We systematically analyze this limitation, focusing on logical relationship understanding, which is a core capability underlying genuine logical reasoning, and reveal that errors related to this capability account for over 90\\% of incorrect predictions, with Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) failing to substantially reduce these errors. To address this bottleneck, we propose First-Step Logical Reasoning (FSLR), a lightweight training framework targeting logical relationship understanding. Our key insight is that the first planning step-identifying which variables to use and which operation to apply-encourages the model to derive logical relationships directly from the problem statement. By training models on this isolated step, FSLR provides explicit supervision for logical relationship understanding, unlike CoT-SFT which implicitly embeds such relationships within complete solution trajectories. Extensive experiments across multiple models and datasets demonstrate that FSLR consistently outperforms CoT-SFT under both in-distribution and out-of-distribution settings, with average improvements of 3.2\\% and 4.6\\%, respectively. Moreover, FSLR achieves 4-6x faster training and reduces training token consumption by over 80\\%.",
      "authors": [
        "Shaojie Wang",
        "Liang Zhang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-07 08:15:01+00:00",
      "link": "https://arxiv.org/pdf/2601.03682v1",
      "tags": [
        "keyword:resnet",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Analyzes and improves mathematical reasoning in LLMs through logical supervision",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.03725v1",
      "title": "EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning",
      "abstract": "Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model's evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.",
      "authors": [
        "Jing-Cheng Pang",
        "Liu Sun",
        "Chang Zhou",
        "Xian Tang",
        "Haichuan Ma",
        "Kun Jiang",
        "Jianlong Wang",
        "Kai Zhang",
        "Sijie Wu",
        "Haoran Cai",
        "Chenwei Wu",
        "Xubin Li",
        "Xin Chen"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-01-07 09:20:05+00:00",
      "link": "https://arxiv.org/pdf/2601.03725v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Proposes dynamic curriculum orchestration for domain-specific LLM fine-tuning",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.03783v1",
      "title": "HearSay Benchmark: Do Audio LLMs Leak What They Hear?",
      "abstract": "While Audio Large Language Models (ALLMs) have achieved remarkable progress in understanding and generation, their potential privacy implications remain largely unexplored. This paper takes the first step to investigate whether ALLMs inadvertently leak user privacy solely through acoustic voiceprints and introduces $\\textit{HearSay}$, a comprehensive benchmark constructed from over 22,000 real-world audio clips. To ensure data quality, the benchmark is meticulously curated through a rigorous pipeline involving automated profiling and human verification, guaranteeing that all privacy labels are grounded in factual records. Extensive experiments on $\\textit{HearSay}$ yield three critical findings: $\\textbf{Significant Privacy Leakage}$: ALLMs inherently extract private attributes from voiceprints, reaching 92.89% accuracy on gender and effectively profiling social attributes. $\\textbf{Insufficient Safety Mechanisms}$: Alarmingly, existing safeguards are severely inadequate; most models fail to refuse privacy-intruding requests, exhibiting near-zero refusal rates for physiological traits. $\\textbf{Reasoning Amplifies Risk}$: Chain-of-Thought (CoT) reasoning exacerbates privacy risks in capable models by uncovering deeper acoustic correlations. These findings expose critical vulnerabilities in ALLMs, underscoring the urgent need for targeted privacy alignment. The codes and dataset are available at https://github.com/JinWang79/HearSay_Benchmark",
      "authors": [
        "Jin Wang",
        "Liang Lin",
        "Kaiwen Luo",
        "Weiliu Wang",
        "Yitian Chen",
        "Moayad Aloqaily",
        "Xuehai Tang",
        "Zhenhong Zhou",
        "Kun Wang",
        "Li Sun",
        "Qingsong Wen"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-07 10:33:44+00:00",
      "link": "https://arxiv.org/pdf/2601.03783v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Privacy benchmark for audio-based Large Language Models",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    }
  ]
}