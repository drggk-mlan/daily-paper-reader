import arxiv
import json
import os
import time
from datetime import datetime, timedelta, timezone

# é¡¹ç›®æ ¹ç›®å½•ï¼ˆå½“å‰è„šæœ¬ä½äº src/ ä¸‹ï¼‰
SCRIPT_DIR = os.path.dirname(__file__)
ROOT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, ".."))
CONFIG_FILE = os.path.join(ROOT_DIR, "config.yaml")
CRAWL_STATE_FILE = os.path.join(ROOT_DIR, "archive", "crawl_state.json")
SEEN_IDS_FILE = os.path.join(ROOT_DIR, "archive", "arxiv_seen.json")

# ArXiv çš„ä¸»è¦ä¸€çº§åˆ†ç±»åˆ—è¡¨
# æ³¨æ„ï¼šç‰©ç†å­¦æ¯”è¾ƒç‰¹æ®Šï¼ŒArXiv å†å²ä¸Šæœ‰å¾ˆå¤šç‹¬ç«‹çš„ç‰©ç†å­˜æ¡£ï¼Œä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬åˆ—å‡ºä¸»è¦çš„
CATEGORIES_TO_FETCH = [
    "cs", "math", "stat", "q-bio", "q-fin", "eess", "econ",
    "physics", "cond-mat", "hep-ph", "hep-th", "gr-qc", "astro-ph",
]


def load_config() -> dict:
    if not os.path.exists(CONFIG_FILE):
        return {}
    try:
        import yaml  # type: ignore
    except Exception:
        log("[WARN] æœªå®‰è£… PyYAMLï¼Œæ— æ³•è§£æ config.yamlã€‚")
        return {}
    try:
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
            return data if isinstance(data, dict) else {}
    except Exception as e:
        log(f"[WARN] è¯»å– config.yaml å¤±è´¥ï¼š{e}")
        return {}


def resolve_days_window(default_days: int) -> int:
    config = load_config()
    paper_setting = (config or {}).get("arxiv_paper_setting") or {}
    crawler_setting = (config or {}).get("crawler") or {}

    value = paper_setting.get("days_window")
    if value is None:
        value = crawler_setting.get("days_window")
    try:
        days = int(value)
        return max(days, 1)
    except Exception:
        return max(default_days, 1)


def load_last_crawl_at() -> datetime | None:
    if not os.path.exists(CRAWL_STATE_FILE):
        return None
    try:
        with open(CRAWL_STATE_FILE, "r", encoding="utf-8") as f:
            payload = json.load(f) or {}
    except Exception:
        return None
    raw = str(payload.get("last_crawl_at") or "").strip()
    if not raw:
        return None
    try:
        dt = datetime.fromisoformat(raw.replace("Z", "+00:00"))
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None


def save_last_crawl_at(at_time: datetime) -> None:
    os.makedirs(os.path.dirname(CRAWL_STATE_FILE), exist_ok=True)
    payload = {"last_crawl_at": at_time.astimezone(timezone.utc).isoformat()}
    with open(CRAWL_STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def load_seen_state() -> tuple[set[str], datetime | None]:
    if not os.path.exists(SEEN_IDS_FILE):
        return set(), None
    try:
        with open(SEEN_IDS_FILE, "r", encoding="utf-8") as f:
            payload = json.load(f) or {}
    except Exception:
        return set(), None

    raw_ids = payload.get("ids") or []
    if not isinstance(raw_ids, list):
        raw_ids = []
    seen_ids = {str(i).strip() for i in raw_ids if str(i).strip()}

    raw_latest = str(payload.get("latest_published_at") or "").strip()
    latest_dt = None
    if raw_latest:
        try:
            latest_dt = datetime.fromisoformat(raw_latest.replace("Z", "+00:00"))
            if latest_dt.tzinfo is None:
                latest_dt = latest_dt.replace(tzinfo=timezone.utc)
            latest_dt = latest_dt.astimezone(timezone.utc)
        except Exception:
            latest_dt = None

    return seen_ids, latest_dt


def save_seen_state(seen_ids: set[str], latest_published_at: datetime | None) -> None:
    os.makedirs(os.path.dirname(SEEN_IDS_FILE), exist_ok=True)
    payload = {
        "updated_at": datetime.now(timezone.utc).isoformat(),
        "latest_published_at": latest_published_at.astimezone(timezone.utc).isoformat()
        if latest_published_at
        else "",
        "ids": sorted(seen_ids),
    }
    with open(SEEN_IDS_FILE, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

def log(message: str) -> None:
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] {message}", flush=True)


def group_start(title: str) -> None:
    print(f"::group::{title}", flush=True)


def group_end() -> None:
    print("::endgroup::", flush=True)


def fetch_all_domains_metadata_robust(
    days: int | None = None,
    output_file: str | None = None,
) -> None:
    # 1. è®¡ç®—æ—¶é—´çª—å£ï¼ˆä¼˜å…ˆä½¿ç”¨ä¸Šæ¬¡æŠ“å–æ—¶é—´ï¼‰
    end_date = datetime.now(timezone.utc)
    seen_ids, latest_published_at = load_seen_state()
    if days is None:
        days = resolve_days_window(1)
    if latest_published_at:
        start_date = latest_published_at
        source_desc = "latest_published_at"
    else:
        last_crawl_at = load_last_crawl_at()
        if last_crawl_at:
            start_date = last_crawl_at
            source_desc = "last_crawl_at"
        else:
            start_date = end_date - timedelta(days=days)
            source_desc = f"days_window={days}"

    # å…œåº•ï¼šæ— è®ºæ¥æºå¦‚ä½•ï¼Œéƒ½ä¸æ—©äº (now - days_window)
    start_date = max(start_date, end_date - timedelta(days=days))

    if start_date >= end_date:
        start_date = end_date - timedelta(minutes=1)

    start_str = start_date.strftime("%Y%m%d%H%M")
    end_str = end_date.strftime("%Y%m%d%H%M")
    
    group_start("Step 1 - fetch arXiv")
    log(f"ğŸŒ [Global Ingest] Window: {start_str} TO {end_str} ({source_desc})")
    
    # ç»“æœé›†ä½¿ç”¨å­—å…¸å»é‡ (å› ä¸ºæœ‰äº›è®ºæ–‡è·¨é¢†åŸŸï¼Œæ¯”å¦‚åŒæ—¶åœ¨ cs å’Œ stat)
    unique_papers = {}
    max_published_new: datetime | None = None
    
    client = arxiv.Client(
        page_size=200,    # é™çº§ï¼šä» 1000 é™åˆ° 200ï¼Œé¿å…å•æ¬¡å“åº”è¿‡å¤§å¯¼è‡´ 500
        delay_seconds=3.0,
        num_retries=5
    )

    # 2. éå†åˆ†ç±»è¿›è¡ŒæŠ“å–
    for category in CATEGORIES_TO_FETCH:
        group_start(f"Fetch category: {category}")
        log(f"ğŸš€ Fetching category: {category} ...")
        
        # æ„é€ æŸ¥è¯¢ï¼šcat:cs* AND submittedDate[...]
        # ä½¿ç”¨é€šé…ç¬¦ category* ä»¥è¦†ç›–å­é¢†åŸŸ (å¦‚ cs.AI, cs.LG)
        query = f"cat:{category}* AND submittedDate:[{start_str} TO {end_str}]"
        
        search = arxiv.Search(
            query=query,
            max_results=None,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        
        count = 0
        try:
            for r in client.results(search):
                pid = r.get_short_id()

                if pid in seen_ids:
                    continue
                
                # å¦‚æœè¿™ç¯‡è®ºæ–‡å·²ç»å­˜åœ¨ï¼ˆè¢«å…¶ä»–åˆ†ç±»æŠ“è¿‡äº†ï¼‰ï¼Œè·³è¿‡
                if pid in unique_papers:
                    continue
                    
                # ä½¿ç”¨ PDF é“¾æ¥è€Œä¸æ˜¯æ‘˜è¦é¡µé“¾æ¥ï¼Œæ–¹ä¾¿åç»­ç›´æ¥ä¸‹è½½æˆ–ä¼ ç»™ä¸‹æ¸¸å¤„ç†
                pdf_link = getattr(r, "pdf_url", None) or r.entry_id
                paper_dict = {
                    "id": pid,
                    "source": "arxiv",
                    "title": r.title.replace("\n", " "),
                    "abstract": r.summary.replace("\n", " "),
                    "authors": [a.name for a in r.authors],
                    "primary_category": r.primary_category,
                    "categories": r.categories,
                    "published": str(r.published),
                    "link": pdf_link,
                }
                unique_papers[pid] = paper_dict
                count += 1

                seen_ids.add(pid)
                published_dt = r.published
                if isinstance(published_dt, datetime):
                    if published_dt.tzinfo is None:
                        published_dt = published_dt.replace(tzinfo=timezone.utc)
                    published_dt = published_dt.astimezone(timezone.utc)
                    if max_published_new is None or published_dt > max_published_new:
                        max_published_new = published_dt
                
                if count % 100 == 0:
                    log(f"   Category {category}: {count} papers fetched...")
            
            log(f"   âœ… Finished {category}: Got {count} new papers.")
            
        except Exception as e:
            # å•ä¸ªåˆ†ç±»å¤±è´¥ä¸å½±å“å¤§å±€ï¼Œæ‰“å°é”™è¯¯ç»§ç»­ä¸‹ä¸€ä¸ª
            log(f"   âŒ Error fetching category {category}: {e}")
            time.sleep(5) # å‡ºé”™åå¤šæ­‡ä¸€ä¼š
        finally:
            group_end()

    # 3. ä¿å­˜æ±‡æ€»ç»“æœ
    total_count = len(unique_papers)
    log(f"âœ… All Done. Total unique papers fetched: {total_count}")
    
    if total_count > 0:
        # è‹¥æœªæ˜¾å¼æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œåˆ™æŒ‰æ—¥æœŸå‘½ååˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ archive/YYYYMMDD/raw ç›®å½•ï¼š
        # <ROOT_DIR>/archive/YYYYMMDD/raw/arxiv_papers_YYYYMMDD.json
        if not output_file:
            today_str = end_date.strftime("%Y%m%d")
            archive_dir = os.path.join(ROOT_DIR, "archive", today_str)
            raw_dir = os.path.join(archive_dir, "raw")
            output_file = os.path.join(
                raw_dir,
                f"arxiv_papers_{today_str}.json",
            )

        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(list(unique_papers.values()), f, ensure_ascii=False, indent=2)
        log(f"ğŸ’¾ File saved to: {output_file}")
    else:
        log("âš ï¸ No papers found. Check your date range or network.")
    if max_published_new:
        save_seen_state(seen_ids, max_published_new)
    else:
        save_seen_state(seen_ids, latest_published_at)
    save_last_crawl_at(end_date)
    group_end()

if __name__ == "__main__":
    # å»ºè®®å…ˆç”¨ days=1 æµ‹è¯•ä¸€ä¸‹ï¼Œæ²¡é—®é¢˜å†è·‘æ›´é•¿æ—¶é—´çª—å£
    fetch_all_domains_metadata_robust()
